{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Data available, in different format so we need to repreprocess everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    padding: 15px;\n",
    "    border: 2px solid #bee5eb;\n",
    "    border-radius: 5px;\n",
    "    background-color: #d1ecf1;\n",
    "    color: #0c5460;\n",
    "    font-size: 14px;\n",
    "    margin-bottom: 10px;\n",
    "    line-height: 1.5;\n",
    "    max-width: 1125px;\">\n",
    "    <strong>ℹ️ Data Processing:</strong>\n",
    "    <p>\n",
    "    Although the initial dataset is of sufficient quality, augmenting the dataset did lead to some <b>type inconsistencies</b>, hence the .astype() functions.\n",
    "    Additionally, I erased the 'blanks' row, since I did not want to train a model to predict nothing, which would only decrease accuracy.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Finalment  Atena le recibe en l'acropoli d'Ate...</td>\n",
       "      <td>arg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jane Laffort fille de Joseph Laffort et d' Ang...</td>\n",
       "      <td>lat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Label\n",
       "0  Finalment  Atena le recibe en l'acropoli d'Ate...   arg\n",
       "1  Jane Laffort fille de Joseph Laffort et d' Ang...   lat"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train = pd.read_csv('Lexical_juggling_train.csv')\n",
    "# df_train = pd.read_csv('train_submission.csv')\n",
    "df_train = pd.read_excel('train_augmented.xlsx', sheet_name='Data')\n",
    "df_train.dropna(subset=['Label'], inplace=True)\n",
    "labels_with_multiple_rows = df_train['Label'].value_counts()\n",
    "labels_to_keep = labels_with_multiple_rows[labels_with_multiple_rows > 1].index\n",
    "df_train = df_train[df_train['Label'].isin(labels_to_keep)]\n",
    "df_train['Text'] = df_train['Text'].astype(str)\n",
    "df_train['Label'] = df_train['Label'].astype(str)\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Usage</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55</td>\n",
       "      <td>Private</td>\n",
       "      <td>Ponovo dobija riječni oblik do Drežnice.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71</td>\n",
       "      <td>Private</td>\n",
       "      <td>Se formaron aproximadamente hace apenas unos 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    Usage                                               Text\n",
       "0  55  Private           Ponovo dobija riječni oblik do Drežnice.\n",
       "1  71  Private  Se formaron aproximadamente hace apenas unos 1..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('test_without_labels.csv')\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape =  (41149, 2)\n",
      "Test shape =  (38827, 3)\n",
      "List labels length =  385\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Shape = \",df_train.shape)\n",
    "print(\"Test shape = \",df_test.shape)\n",
    "print(\"List labels length = \", len(df_train['Label'].unique()))\n",
    "# print(df_train['Label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 5px solid green; padding-left: 10px;\">As such, we have <b>38K sentences</b> in different languages, to classify in 390 categories. We assume that each label refers to a language, though the exact semantic is not relevant for the classification. The classes were initially unbalanced. To solve the class imbalance, we decided to proceed with <b>data augmentation</b> (oversampling to be accurate) and generate \"train_augmented.xlsx\" through a mix of gpt2 (initially), and then queries with chatgpt-o1-mini for quality's sake.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Label  Count\n",
      "0     tgk    300\n",
      "1     teo    205\n",
      "2     wbm    203\n",
      "3     hin    200\n",
      "4     tat    200\n",
      "..    ...    ...\n",
      "380   xho    100\n",
      "381   yao    100\n",
      "382   hus    100\n",
      "383   kau    100\n",
      "384   ceb     99\n",
      "\n",
      "[385 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_lab = pd.DataFrame(df_train)\n",
    "\n",
    "label_counts = df_lab[\"Label\"].value_counts().reset_index()\n",
    "label_counts.columns = [\"Label\", \"Count\"]\n",
    "# Simple print to make sure that each label possesses at least 100 observations (ceb was missed by one but don't mind it, I forgot one example)\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class TqdmTfidfVectorizer(TfidfVectorizer):\n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        raw_documents = list(tqdm(raw_documents, desc=\"TFIDF Fit Transform\"))\n",
    "        return super().fit_transform(raw_documents, y)\n",
    "    def transform(self, raw_documents):\n",
    "        raw_documents = list(tqdm(raw_documents, desc=\"TFIDF Transform\"))\n",
    "        return super().transform(raw_documents)\n",
    "\n",
    "# Defining'Text' as X since this will be the data available and 'Label' as Y as we need to classify, hence it is our 'Prediction'\n",
    "X = df_train['Text']\n",
    "y = df_train['Label']\n",
    "\n",
    "# Split in 80/20, sampled randomly while keeping class balance in both train and test sets, using the argument 'stratify' based on labels.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    padding: 15px;\n",
    "    border: 2px solid #bee5eb;\n",
    "    border-radius: 5px;\n",
    "    background-color: #d1ecf1;\n",
    "    color: #0c5460;\n",
    "    font-size: 14px;\n",
    "    margin-bottom: 10px;\n",
    "    line-height: 1.5;\n",
    "    max-width: 1125px;\">\n",
    "    <strong>ℹ️ Pipeline Description:</strong>\n",
    "    <p>\n",
    "    The baseline pipeline consists in enconding the text in latent space using the Tfidf Vectorizer with 20k features, and then\n",
    "    classify the examples with a logistic regression with 'saga' solver.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TFIDF Fit Transform: 100%|██████████| 32919/32919 [00:00<00:00, 2129430.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 22 epochs took 22 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TFIDF Transform: 100%|██████████| 8230/8230 [00:00<00:00, 595804.44it/s]\n",
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         abk       1.00      0.45      0.62        20\n",
      "         ace       1.00      0.90      0.95        20\n",
      "         ach       0.86      0.95      0.90        38\n",
      "         acm       0.00      0.00      0.00        20\n",
      "         acr       1.00      0.80      0.89        20\n",
      "         ada       1.00      0.90      0.95        21\n",
      "         afb       0.21      0.15      0.18        20\n",
      "         afr       0.71      0.85      0.77        20\n",
      "         ahk       0.95      1.00      0.98        20\n",
      "         ajp       0.16      0.20      0.18        20\n",
      "         aka       0.64      0.70      0.67        20\n",
      "         aln       0.55      0.55      0.55        20\n",
      "         als       0.47      0.35      0.40        20\n",
      "         alt       0.93      0.70      0.80        20\n",
      "         amh       1.00      0.60      0.75        20\n",
      "         aoj       1.00      1.00      1.00        20\n",
      "         apc       0.21      0.20      0.21        20\n",
      "         ara       0.32      0.35      0.33        20\n",
      "         arb       0.68      0.85      0.76        20\n",
      "         arg       0.35      0.60      0.44        20\n",
      "         arn       1.00      0.95      0.97        20\n",
      "         ary       0.25      0.25      0.25        20\n",
      "         arz       0.71      0.85      0.77        20\n",
      "         asm       1.00      0.85      0.92        20\n",
      "         ast       0.69      0.55      0.61        20\n",
      "         aym       0.75      0.30      0.43        20\n",
      "         ayr       0.71      0.60      0.65        20\n",
      "         azb       0.95      0.90      0.92        20\n",
      "         aze       0.24      0.33      0.28        40\n",
      "         azj       0.57      0.20      0.30        20\n",
      "         bak       0.40      0.40      0.40        40\n",
      "         bam       0.84      0.80      0.82        20\n",
      "         ban       0.43      0.45      0.44        20\n",
      "         bar       0.58      0.55      0.56        20\n",
      "         bcl       0.50      0.35      0.41        20\n",
      "         bel       0.55      0.30      0.39        20\n",
      "         bem       1.00      0.90      0.95        20\n",
      "         ber       0.52      0.55      0.54        20\n",
      "         bew       0.45      0.50      0.48        20\n",
      "         bih       0.56      0.45      0.50        20\n",
      "         bik       0.56      0.70      0.62        20\n",
      "         bis       0.82      0.90      0.86        20\n",
      "         bjn       0.70      0.80      0.74        20\n",
      "         bod       0.86      0.90      0.88        20\n",
      "         bos       0.40      0.20      0.27        20\n",
      "         bpy       1.00      1.00      1.00        20\n",
      "         bqc       1.00      0.90      0.95        20\n",
      "         bre       0.76      0.80      0.78        20\n",
      "         bsb       0.81      0.65      0.72        20\n",
      "         bul       0.42      0.75      0.54        20\n",
      "         bzj       1.00      1.00      1.00        20\n",
      "         cab       1.00      0.85      0.92        20\n",
      "         cak       0.71      0.75      0.73        20\n",
      "         cat       0.85      0.55      0.67        20\n",
      "         cbk       0.69      0.45      0.55        20\n",
      "         ceb       0.76      0.65      0.70        20\n",
      "         ces       0.45      0.25      0.32        20\n",
      "         che       0.95      1.00      0.98        20\n",
      "         chk       0.95      1.00      0.98        20\n",
      "         chv       0.56      0.50      0.53        20\n",
      "         cjk       1.00      0.80      0.89        20\n",
      "         ckb       0.58      0.35      0.44        20\n",
      "         cmn       0.00      0.00      0.00        20\n",
      "         cos       1.00      0.05      0.10        20\n",
      "         crh       0.60      0.80      0.69        40\n",
      "         crs       0.91      1.00      0.95        20\n",
      "         csb       0.86      0.60      0.71        20\n",
      "         csy       0.83      1.00      0.91        20\n",
      "         ctu       0.95      1.00      0.98        20\n",
      "         cuk       1.00      0.95      0.97        20\n",
      "         cym       1.00      0.85      0.92        20\n",
      "         dan       0.74      0.70      0.72        20\n",
      "         deu       0.76      0.95      0.84        20\n",
      "         diq       0.80      0.60      0.69        20\n",
      "         div       1.00      0.35      0.52        20\n",
      "         djk       0.95      1.00      0.98        20\n",
      "         dtp       0.56      0.90      0.69        20\n",
      "         dyu       0.60      0.75      0.67        20\n",
      "         dzo       0.93      0.70      0.80        20\n",
      "         ekk       0.46      0.30      0.36        20\n",
      "         ell       1.00      0.90      0.95        20\n",
      "         eml       0.80      0.80      0.80        20\n",
      "         eng       0.26      0.35      0.30        20\n",
      "         enm       0.45      0.91      0.60        23\n",
      "         epo       0.64      0.70      0.67        20\n",
      "         est       0.46      0.65      0.54        20\n",
      "         eus       0.91      0.50      0.65        20\n",
      "         ewe       0.79      0.95      0.86        20\n",
      "         ext       0.47      0.45      0.46        20\n",
      "         fao       0.85      0.85      0.85        20\n",
      "         fas       0.24      0.20      0.22        20\n",
      "         fij       0.95      1.00      0.98        20\n",
      "         fil       0.62      0.25      0.36        20\n",
      "         fin       0.80      0.40      0.53        20\n",
      "         fon       1.00      0.75      0.86        20\n",
      "         fra       0.88      0.70      0.78        20\n",
      "         frr       1.00      0.55      0.71        20\n",
      "         fry       0.83      0.75      0.79        20\n",
      "         ful       0.62      0.75      0.68        20\n",
      "         fur       0.77      0.85      0.81        20\n",
      "         gcf       0.71      0.85      0.77        20\n",
      "         gil       0.95      0.90      0.92        20\n",
      "         gla       0.78      0.70      0.74        20\n",
      "         gle       0.94      0.80      0.86        20\n",
      "         glg       0.35      0.35      0.35        20\n",
      "         glk       0.44      0.40      0.42        20\n",
      "         glv       0.82      0.90      0.86        20\n",
      "         gom       0.63      0.65      0.64        40\n",
      "         gor       1.00      0.55      0.71        20\n",
      "         grc       0.90      0.95      0.93        20\n",
      "         grn       0.60      0.60      0.60        20\n",
      "         gsw       0.82      0.70      0.76        20\n",
      "         guc       0.95      1.00      0.98        20\n",
      "         gug       0.75      0.90      0.82        20\n",
      "         guj       0.74      0.50      0.60        40\n",
      "         gym       1.00      1.00      1.00        20\n",
      "         hat       0.78      0.70      0.74        20\n",
      "         hau       0.33      0.33      0.33        40\n",
      "         haw       0.86      0.30      0.44        20\n",
      "         hbo       0.81      0.78      0.79        27\n",
      "         hbs       0.36      0.53      0.43        40\n",
      "         heb       0.75      0.45      0.56        20\n",
      "         hif       0.67      0.70      0.68        20\n",
      "         hil       0.92      1.00      0.96        22\n",
      "         hin       0.23      0.57      0.33        40\n",
      "         hmn       1.00      0.60      0.75        20\n",
      "         hmo       0.94      0.85      0.89        20\n",
      "         hne       0.55      0.60      0.57        20\n",
      "         hnj       1.00      1.00      1.00        26\n",
      "         hrv       0.17      0.15      0.16        20\n",
      "         hrx       0.74      0.85      0.79        20\n",
      "         hsb       0.78      0.35      0.48        20\n",
      "         hui       1.00      0.95      0.97        20\n",
      "         hun       0.95      0.90      0.92        20\n",
      "         hus       1.00      1.00      1.00        20\n",
      "         hye       0.88      0.75      0.81        20\n",
      "         hyw       0.92      0.88      0.90        25\n",
      "         iba       0.95      0.95      0.95        22\n",
      "         ibo       0.93      0.70      0.80        20\n",
      "         ido       0.65      0.75      0.70        20\n",
      "         ikk       1.00      1.00      1.00        20\n",
      "         iku       1.00      0.75      0.86        20\n",
      "         ile       0.72      0.90      0.80        20\n",
      "         ilo       0.83      0.95      0.88        20\n",
      "         ina       0.86      0.60      0.71        20\n",
      "         ind       0.43      0.50      0.47        20\n",
      "         isl       0.89      0.85      0.87        20\n",
      "         ita       0.47      0.45      0.46        20\n",
      "         ixl       1.00      1.00      1.00        20\n",
      "         jam       0.94      0.85      0.89        20\n",
      "         jav       0.79      0.75      0.77        20\n",
      "         jbo       0.81      0.85      0.83        20\n",
      "         jpn       0.00      0.00      0.00        20\n",
      "         kaa       0.69      0.62      0.66        40\n",
      "         kab       0.71      0.60      0.65        20\n",
      "         kac       0.91      1.00      0.95        20\n",
      "         kal       1.00      0.55      0.71        20\n",
      "         kam       0.69      0.90      0.78        20\n",
      "         kan       1.00      0.95      0.97        20\n",
      "         kat       0.47      0.53      0.49        40\n",
      "         kau       1.00      0.50      0.67        20\n",
      "         kaz       0.92      0.60      0.73        20\n",
      "         kbd       1.00      0.20      0.33        20\n",
      "         kbp       0.83      0.95      0.88        20\n",
      "         kea       0.64      0.90      0.75        20\n",
      "         kek       1.00      1.00      1.00        20\n",
      "         khm       1.00      0.90      0.95        20\n",
      "         kik       0.94      0.75      0.83        20\n",
      "         kin       0.37      0.35      0.36        20\n",
      "         kir       0.91      0.50      0.65        20\n",
      "         kjb       0.95      1.00      0.98        20\n",
      "         kjh       0.85      0.85      0.85        20\n",
      "         kmb       0.86      0.90      0.88        20\n",
      "         kmr       0.64      0.35      0.45        20\n",
      "         knv       0.87      1.00      0.93        20\n",
      "         kom       0.74      0.70      0.72        20\n",
      "         kon       0.79      0.95      0.86        20\n",
      "         kor       0.25      0.05      0.08        20\n",
      "         kos       1.00      1.00      1.00        20\n",
      "         kpg       1.00      1.00      1.00        20\n",
      "         krc       1.00      0.80      0.89        20\n",
      "         ksd       0.91      1.00      0.95        20\n",
      "         ksh       1.00      0.65      0.79        20\n",
      "         ksw       1.00      0.65      0.79        23\n",
      "         kur       0.44      0.53      0.48        40\n",
      "         lao       1.00      0.90      0.95        20\n",
      "         lat       0.81      0.85      0.83        20\n",
      "         lfn       0.74      0.85      0.79        20\n",
      "         lhu       1.00      0.95      0.97        20\n",
      "         lij       0.65      0.65      0.65        20\n",
      "         lim       0.69      0.55      0.61        20\n",
      "         lin       0.94      0.75      0.83        20\n",
      "         lit       0.78      0.70      0.74        20\n",
      "         lmo       0.52      0.55      0.54        20\n",
      "         ltz       0.38      0.15      0.21        20\n",
      "         lua       0.90      0.90      0.90        20\n",
      "         lue       0.91      0.97      0.94        30\n",
      "         lug       0.89      0.80      0.84        20\n",
      "         luo       1.00      0.90      0.95        20\n",
      "         lus       1.00      0.90      0.95        20\n",
      "         lvs       0.41      0.65      0.50        20\n",
      "         lzh       0.00      0.00      0.00        20\n",
      "         mad       0.39      0.45      0.42        20\n",
      "         mah       1.00      0.96      0.98        23\n",
      "         mai       0.93      0.65      0.76        20\n",
      "         mal       1.00      1.00      1.00        20\n",
      "         mam       0.90      0.95      0.93        20\n",
      "         mar       0.87      0.65      0.74        20\n",
      "         mau       1.00      1.00      1.00        20\n",
      "         mco       0.95      1.00      0.98        20\n",
      "         meu       0.69      0.90      0.78        20\n",
      "         mgh       0.72      0.90      0.80        20\n",
      "         mhr       0.89      0.85      0.87        20\n",
      "         min       1.00      0.90      0.95        20\n",
      "         mkd       0.79      0.75      0.77        20\n",
      "         mlg       0.56      0.45      0.50        20\n",
      "         mlt       0.75      0.60      0.67        20\n",
      "         mon       0.57      0.72      0.64        40\n",
      "         mos       0.90      0.95      0.93        20\n",
      "         mps       1.00      1.00      1.00        20\n",
      "         mri       0.53      0.45      0.49        20\n",
      "         msa       0.31      0.20      0.24        20\n",
      "         mwl       0.63      0.85      0.72        20\n",
      "         mya       1.00      0.95      0.97        20\n",
      "         myv       0.88      0.35      0.50        20\n",
      "         mzh       0.95      1.00      0.98        20\n",
      "         mzn       0.95      0.95      0.95        20\n",
      "         nap       0.62      0.40      0.48        20\n",
      "         naq       1.00      0.95      0.97        20\n",
      "         nav       1.00      1.00      1.00        20\n",
      "         nbl       0.50      0.40      0.44        20\n",
      "         nch       1.00      0.95      0.97        20\n",
      "         ncj       0.91      1.00      0.95        20\n",
      "         nde       0.75      0.60      0.67        20\n",
      "         ndo       1.00      0.90      0.95        20\n",
      "         nds       0.67      0.70      0.68        20\n",
      "         nep       0.75      0.30      0.43        20\n",
      "         new       0.83      0.75      0.79        20\n",
      "         ngl       0.41      0.59      0.48        22\n",
      "         ngu       1.00      1.00      1.00        20\n",
      "         niu       0.74      0.81      0.77        21\n",
      "         nld       0.65      0.65      0.65        20\n",
      "         nnb       0.82      0.45      0.58        20\n",
      "         nno       0.64      0.70      0.67        20\n",
      "         nob       0.24      0.25      0.24        20\n",
      "         nor       0.40      0.40      0.40        20\n",
      "         npi       0.65      0.85      0.74        20\n",
      "         nso       0.74      0.70      0.72        20\n",
      "         nya       0.92      0.55      0.69        20\n",
      "         nyu       0.90      0.95      0.93        20\n",
      "         oci       0.50      0.40      0.44        20\n",
      "         ori       0.44      0.35      0.39        20\n",
      "         orm       0.94      0.75      0.83        20\n",
      "         ory       0.50      0.55      0.52        20\n",
      "         oss       1.00      0.75      0.86        20\n",
      "         ote       0.83      1.00      0.91        20\n",
      "         pag       0.83      0.95      0.88        20\n",
      "         pam       0.57      0.20      0.30        20\n",
      "         pan       1.00      0.90      0.95        20\n",
      "         pap       1.00      0.90      0.95        20\n",
      "         pau       1.00      1.00      1.00        20\n",
      "         pcd       0.69      0.55      0.61        20\n",
      "         pcm       0.73      0.80      0.76        20\n",
      "         pes       0.47      0.45      0.46        20\n",
      "         pfl       0.68      0.65      0.67        20\n",
      "         pis       0.94      0.85      0.89        20\n",
      "         pls       0.87      1.00      0.93        20\n",
      "         plt       0.92      0.55      0.69        20\n",
      "         pms       1.00      0.95      0.97        20\n",
      "         pnb       0.95      0.90      0.92        20\n",
      "         poh       0.87      1.00      0.93        20\n",
      "         pol       0.77      0.50      0.61        20\n",
      "         pon       0.90      0.90      0.90        20\n",
      "         por       0.52      0.65      0.58        20\n",
      "         prs       0.50      0.65      0.57        20\n",
      "         pus       0.89      0.85      0.87        20\n",
      "         qub       0.82      0.84      0.83        32\n",
      "         quc       0.83      0.75      0.79        20\n",
      "         que       0.80      0.40      0.53        20\n",
      "         quh       0.73      0.80      0.76        20\n",
      "         quw       0.95      0.95      0.95        20\n",
      "         quy       0.94      0.75      0.83        20\n",
      "         quz       0.93      0.67      0.78        21\n",
      "         qvi       0.95      0.95      0.95        20\n",
      "         rap       0.73      0.40      0.52        20\n",
      "         rmy       0.71      0.50      0.59        20\n",
      "         roh       0.67      0.80      0.73        20\n",
      "         ron       0.75      0.60      0.67        20\n",
      "         rop       0.95      0.95      0.95        20\n",
      "         rue       0.88      0.35      0.50        20\n",
      "         rug       0.87      1.00      0.93        20\n",
      "         run       0.35      0.45      0.39        20\n",
      "         sag       0.91      1.00      0.95        20\n",
      "         sah       0.94      0.75      0.83        20\n",
      "         san       0.53      0.59      0.56        32\n",
      "         sat       1.00      0.95      0.97        20\n",
      "         scn       0.60      0.60      0.60        20\n",
      "         sco       0.59      0.65      0.62        20\n",
      "         seh       0.95      0.90      0.92        20\n",
      "         sgs       0.92      0.55      0.69        20\n",
      "         sin       1.00      0.85      0.92        20\n",
      "         slk       0.75      0.45      0.56        20\n",
      "         slv       0.35      0.35      0.35        20\n",
      "         sme       0.82      0.70      0.76        20\n",
      "         smo       0.67      0.60      0.63        20\n",
      "         sna       0.75      0.15      0.25        20\n",
      "         snd       0.95      0.95      0.95        20\n",
      "         som       0.69      0.45      0.55        40\n",
      "         sot       0.63      0.60      0.62        20\n",
      "         spa       0.12      0.15      0.13        20\n",
      "         sqi       0.50      0.50      0.50        20\n",
      "         srd       0.65      0.75      0.70        20\n",
      "         srm       1.00      1.00      1.00        20\n",
      "         srn       0.90      0.95      0.93        20\n",
      "         srp       0.43      0.23      0.30        40\n",
      "         ssw       0.80      0.60      0.69        20\n",
      "         sun       0.71      0.25      0.37        20\n",
      "         suz       0.90      0.90      0.90        20\n",
      "         swa       0.32      0.40      0.36        20\n",
      "         swc       0.57      0.40      0.47        20\n",
      "         swe       1.00      0.75      0.86        20\n",
      "         swh       0.47      0.40      0.43        20\n",
      "         szl       0.94      0.85      0.89        20\n",
      "         tah       0.94      0.85      0.89        20\n",
      "         tam       1.00      0.95      0.97        20\n",
      "         tat       0.30      0.35      0.32        40\n",
      "         tbz       0.82      0.90      0.86        20\n",
      "         tca       1.00      1.00      1.00        20\n",
      "         tdt       1.00      1.00      1.00        20\n",
      "         teo       0.73      0.88      0.80        41\n",
      "         tgk       0.05      0.47      0.08        60\n",
      "         tgl       0.59      0.50      0.54        20\n",
      "         tha       1.00      0.60      0.75        20\n",
      "         tir       0.90      0.95      0.93        20\n",
      "         tlh       0.92      0.55      0.69        20\n",
      "         tls       0.90      0.90      0.90        20\n",
      "         toj       0.77      1.00      0.87        20\n",
      "         tok       0.95      1.00      0.98        20\n",
      "         ton       1.00      0.55      0.71        20\n",
      "         top       1.00      1.00      1.00        20\n",
      "         tpi       1.00      0.95      0.97        20\n",
      "         tsn       0.69      0.55      0.61        20\n",
      "         tso       0.81      0.85      0.83        20\n",
      "         tuc       0.87      1.00      0.93        20\n",
      "         tuk       0.49      0.57      0.53        40\n",
      "         tum       0.78      0.70      0.74        20\n",
      "         tur       0.78      0.35      0.48        20\n",
      "         tvl       0.82      0.90      0.86        20\n",
      "         twi       0.86      0.90      0.88        20\n",
      "         tyv       0.79      0.75      0.77        20\n",
      "         tzo       0.95      1.00      0.98        20\n",
      "         udm       0.91      0.50      0.65        20\n",
      "         uig       0.36      0.57      0.44        40\n",
      "         ukr       0.56      0.45      0.50        20\n",
      "         umb       0.84      0.80      0.82        20\n",
      "         urd       0.38      0.15      0.21        20\n",
      "         uzb       0.42      0.62      0.51        40\n",
      "         uzn       0.33      0.15      0.21        20\n",
      "         vec       0.47      0.40      0.43        20\n",
      "         ven       1.00      0.90      0.95        20\n",
      "         vep       0.76      0.80      0.78        20\n",
      "         vie       1.00      0.85      0.92        20\n",
      "         vls       0.62      0.50      0.56        20\n",
      "         vol       0.90      0.90      0.90        20\n",
      "         wal       1.00      0.85      0.92        20\n",
      "         war       0.83      0.50      0.62        20\n",
      "         wbm       0.98      0.98      0.98        41\n",
      "         wln       0.48      0.50      0.49        20\n",
      "         wol       0.55      0.55      0.55        20\n",
      "         wuu       0.00      0.00      0.00        20\n",
      "         xav       1.00      1.00      1.00        20\n",
      "         xho       0.70      0.35      0.47        20\n",
      "         xmf       1.00      0.65      0.79        20\n",
      "         yao       0.80      1.00      0.89        20\n",
      "         yap       1.00      0.95      0.97        20\n",
      "         yid       1.00      1.00      1.00        20\n",
      "         yom       0.94      0.75      0.83        20\n",
      "         yor       1.00      0.75      0.86        20\n",
      "         yue       0.00      0.00      0.00        20\n",
      "         zai       0.95      1.00      0.98        20\n",
      "         zea       0.70      0.70      0.70        20\n",
      "         zho       0.00      0.00      0.00        20\n",
      "         zlm       0.52      0.75      0.61        20\n",
      "         zsm       0.47      0.35      0.40        20\n",
      "         zul       0.29      0.10      0.15        20\n",
      "\n",
      "    accuracy                           0.68      8230\n",
      "   macro avg       0.75      0.69      0.70      8230\n",
      "weighted avg       0.73      0.68      0.69      8230\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TqdmTfidfVectorizer(max_features=20000, min_df=5, max_df=0.8)),\n",
    "    ('clf', LogisticRegression(solver='saga', max_iter=1000, verbose=1))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 5px solid green; padding-left: 10px;\">\n",
    "The overall accuracy reached with the baseline model is 73%, which is quite satisfactory.\n",
    "However, when looking at the detail class by class (cf. print above), the variance is quite large. We need a model more consistant.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2 - XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    padding: 15px;\n",
    "    border: 2px solid #bee5eb;\n",
    "    border-radius: 5px;\n",
    "    background-color: #d1ecf1;\n",
    "    color: #0c5460;\n",
    "    font-size: 14px;\n",
    "    margin-bottom: 10px;\n",
    "    line-height: 1.5;\n",
    "    max-width: 1125px;\">\n",
    "    <strong>ℹ️ Pipeline Description:</strong>\n",
    "    <p>\n",
    "    The pipeline consists in enconding the text in latent space using the Tfidf Vectorizer with 20k features, and then\n",
    "    classify the examples with an ensemblist method: XGBoost.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TFIDF Fit Transform: 100%|██████████| 32919/32919 [00:00<00:00, 1828893.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 23 epochs took 25 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TFIDF Transform: 100%|██████████| 8230/8230 [00:00<00:00, 2057771.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         abk       1.00      0.45      0.62        20\n",
      "         ace       1.00      0.90      0.95        20\n",
      "         ach       0.86      0.95      0.90        38\n",
      "         acm       0.00      0.00      0.00        20\n",
      "         acr       1.00      0.80      0.89        20\n",
      "         ada       1.00      0.90      0.95        21\n",
      "         afb       0.21      0.15      0.18        20\n",
      "         afr       0.71      0.85      0.77        20\n",
      "         ahk       0.95      1.00      0.98        20\n",
      "         ajp       0.16      0.20      0.18        20\n",
      "         aka       0.64      0.70      0.67        20\n",
      "         aln       0.55      0.55      0.55        20\n",
      "         als       0.47      0.35      0.40        20\n",
      "         alt       0.93      0.70      0.80        20\n",
      "         amh       1.00      0.60      0.75        20\n",
      "         aoj       1.00      1.00      1.00        20\n",
      "         apc       0.21      0.20      0.21        20\n",
      "         ara       0.32      0.35      0.33        20\n",
      "         arb       0.68      0.85      0.76        20\n",
      "         arg       0.35      0.60      0.44        20\n",
      "         arn       1.00      0.95      0.97        20\n",
      "         ary       0.25      0.25      0.25        20\n",
      "         arz       0.71      0.85      0.77        20\n",
      "         asm       1.00      0.85      0.92        20\n",
      "         ast       0.69      0.55      0.61        20\n",
      "         aym       0.75      0.30      0.43        20\n",
      "         ayr       0.71      0.60      0.65        20\n",
      "         azb       0.95      0.90      0.92        20\n",
      "         aze       0.24      0.33      0.28        40\n",
      "         azj       0.57      0.20      0.30        20\n",
      "         bak       0.40      0.40      0.40        40\n",
      "         bam       0.84      0.80      0.82        20\n",
      "         ban       0.43      0.45      0.44        20\n",
      "         bar       0.58      0.55      0.56        20\n",
      "         bcl       0.50      0.35      0.41        20\n",
      "         bel       0.55      0.30      0.39        20\n",
      "         bem       1.00      0.90      0.95        20\n",
      "         ber       0.52      0.55      0.54        20\n",
      "         bew       0.45      0.50      0.48        20\n",
      "         bih       0.56      0.45      0.50        20\n",
      "         bik       0.56      0.70      0.62        20\n",
      "         bis       0.82      0.90      0.86        20\n",
      "         bjn       0.70      0.80      0.74        20\n",
      "         bod       0.86      0.90      0.88        20\n",
      "         bos       0.40      0.20      0.27        20\n",
      "         bpy       1.00      1.00      1.00        20\n",
      "         bqc       1.00      0.90      0.95        20\n",
      "         bre       0.76      0.80      0.78        20\n",
      "         bsb       0.81      0.65      0.72        20\n",
      "         bul       0.42      0.75      0.54        20\n",
      "         bzj       1.00      1.00      1.00        20\n",
      "         cab       1.00      0.85      0.92        20\n",
      "         cak       0.71      0.75      0.73        20\n",
      "         cat       0.85      0.55      0.67        20\n",
      "         cbk       0.69      0.45      0.55        20\n",
      "         ceb       0.76      0.65      0.70        20\n",
      "         ces       0.45      0.25      0.32        20\n",
      "         che       0.95      1.00      0.98        20\n",
      "         chk       0.95      1.00      0.98        20\n",
      "         chv       0.56      0.50      0.53        20\n",
      "         cjk       1.00      0.80      0.89        20\n",
      "         ckb       0.58      0.35      0.44        20\n",
      "         cmn       0.00      0.00      0.00        20\n",
      "         cos       1.00      0.05      0.10        20\n",
      "         crh       0.60      0.80      0.69        40\n",
      "         crs       0.91      1.00      0.95        20\n",
      "         csb       0.86      0.60      0.71        20\n",
      "         csy       0.83      1.00      0.91        20\n",
      "         ctu       0.95      1.00      0.98        20\n",
      "         cuk       1.00      0.95      0.97        20\n",
      "         cym       1.00      0.85      0.92        20\n",
      "         dan       0.74      0.70      0.72        20\n",
      "         deu       0.76      0.95      0.84        20\n",
      "         diq       0.80      0.60      0.69        20\n",
      "         div       1.00      0.35      0.52        20\n",
      "         djk       0.95      1.00      0.98        20\n",
      "         dtp       0.56      0.90      0.69        20\n",
      "         dyu       0.60      0.75      0.67        20\n",
      "         dzo       0.93      0.70      0.80        20\n",
      "         ekk       0.46      0.30      0.36        20\n",
      "         ell       1.00      0.90      0.95        20\n",
      "         eml       0.80      0.80      0.80        20\n",
      "         eng       0.26      0.35      0.30        20\n",
      "         enm       0.45      0.91      0.60        23\n",
      "         epo       0.64      0.70      0.67        20\n",
      "         est       0.46      0.65      0.54        20\n",
      "         eus       0.91      0.50      0.65        20\n",
      "         ewe       0.79      0.95      0.86        20\n",
      "         ext       0.47      0.45      0.46        20\n",
      "         fao       0.85      0.85      0.85        20\n",
      "         fas       0.24      0.20      0.22        20\n",
      "         fij       0.95      1.00      0.98        20\n",
      "         fil       0.62      0.25      0.36        20\n",
      "         fin       0.80      0.40      0.53        20\n",
      "         fon       1.00      0.75      0.86        20\n",
      "         fra       0.88      0.70      0.78        20\n",
      "         frr       1.00      0.55      0.71        20\n",
      "         fry       0.83      0.75      0.79        20\n",
      "         ful       0.62      0.75      0.68        20\n",
      "         fur       0.77      0.85      0.81        20\n",
      "         gcf       0.71      0.85      0.77        20\n",
      "         gil       0.95      0.90      0.92        20\n",
      "         gla       0.78      0.70      0.74        20\n",
      "         gle       0.94      0.80      0.86        20\n",
      "         glg       0.35      0.35      0.35        20\n",
      "         glk       0.44      0.40      0.42        20\n",
      "         glv       0.82      0.90      0.86        20\n",
      "         gom       0.63      0.65      0.64        40\n",
      "         gor       1.00      0.55      0.71        20\n",
      "         grc       0.90      0.95      0.93        20\n",
      "         grn       0.60      0.60      0.60        20\n",
      "         gsw       0.82      0.70      0.76        20\n",
      "         guc       0.95      1.00      0.98        20\n",
      "         gug       0.75      0.90      0.82        20\n",
      "         guj       0.74      0.50      0.60        40\n",
      "         gym       1.00      1.00      1.00        20\n",
      "         hat       0.78      0.70      0.74        20\n",
      "         hau       0.33      0.33      0.33        40\n",
      "         haw       0.86      0.30      0.44        20\n",
      "         hbo       0.81      0.78      0.79        27\n",
      "         hbs       0.36      0.53      0.42        40\n",
      "         heb       0.75      0.45      0.56        20\n",
      "         hif       0.67      0.70      0.68        20\n",
      "         hil       0.92      1.00      0.96        22\n",
      "         hin       0.23      0.57      0.33        40\n",
      "         hmn       1.00      0.60      0.75        20\n",
      "         hmo       0.94      0.85      0.89        20\n",
      "         hne       0.55      0.60      0.57        20\n",
      "         hnj       1.00      1.00      1.00        26\n",
      "         hrv       0.17      0.15      0.16        20\n",
      "         hrx       0.74      0.85      0.79        20\n",
      "         hsb       0.78      0.35      0.48        20\n",
      "         hui       1.00      0.95      0.97        20\n",
      "         hun       0.95      0.90      0.92        20\n",
      "         hus       1.00      1.00      1.00        20\n",
      "         hye       0.88      0.75      0.81        20\n",
      "         hyw       0.92      0.88      0.90        25\n",
      "         iba       0.95      0.95      0.95        22\n",
      "         ibo       0.93      0.70      0.80        20\n",
      "         ido       0.65      0.75      0.70        20\n",
      "         ikk       1.00      1.00      1.00        20\n",
      "         iku       1.00      0.75      0.86        20\n",
      "         ile       0.72      0.90      0.80        20\n",
      "         ilo       0.83      0.95      0.88        20\n",
      "         ina       0.86      0.60      0.71        20\n",
      "         ind       0.43      0.50      0.47        20\n",
      "         isl       0.89      0.85      0.87        20\n",
      "         ita       0.47      0.45      0.46        20\n",
      "         ixl       1.00      1.00      1.00        20\n",
      "         jam       0.94      0.85      0.89        20\n",
      "         jav       0.79      0.75      0.77        20\n",
      "         jbo       0.81      0.85      0.83        20\n",
      "         jpn       0.00      0.00      0.00        20\n",
      "         kaa       0.69      0.62      0.66        40\n",
      "         kab       0.71      0.60      0.65        20\n",
      "         kac       0.91      1.00      0.95        20\n",
      "         kal       1.00      0.55      0.71        20\n",
      "         kam       0.69      0.90      0.78        20\n",
      "         kan       1.00      0.95      0.97        20\n",
      "         kat       0.47      0.53      0.49        40\n",
      "         kau       1.00      0.50      0.67        20\n",
      "         kaz       0.92      0.60      0.73        20\n",
      "         kbd       1.00      0.20      0.33        20\n",
      "         kbp       0.83      0.95      0.88        20\n",
      "         kea       0.64      0.90      0.75        20\n",
      "         kek       1.00      1.00      1.00        20\n",
      "         khm       1.00      0.90      0.95        20\n",
      "         kik       0.94      0.75      0.83        20\n",
      "         kin       0.37      0.35      0.36        20\n",
      "         kir       0.91      0.50      0.65        20\n",
      "         kjb       0.95      1.00      0.98        20\n",
      "         kjh       0.85      0.85      0.85        20\n",
      "         kmb       0.86      0.90      0.88        20\n",
      "         kmr       0.64      0.35      0.45        20\n",
      "         knv       0.87      1.00      0.93        20\n",
      "         kom       0.74      0.70      0.72        20\n",
      "         kon       0.79      0.95      0.86        20\n",
      "         kor       0.25      0.05      0.08        20\n",
      "         kos       1.00      1.00      1.00        20\n",
      "         kpg       1.00      1.00      1.00        20\n",
      "         krc       1.00      0.80      0.89        20\n",
      "         ksd       0.91      1.00      0.95        20\n",
      "         ksh       1.00      0.65      0.79        20\n",
      "         ksw       1.00      0.65      0.79        23\n",
      "         kur       0.44      0.53      0.48        40\n",
      "         lao       1.00      0.90      0.95        20\n",
      "         lat       0.81      0.85      0.83        20\n",
      "         lfn       0.74      0.85      0.79        20\n",
      "         lhu       1.00      0.95      0.97        20\n",
      "         lij       0.65      0.65      0.65        20\n",
      "         lim       0.69      0.55      0.61        20\n",
      "         lin       0.94      0.75      0.83        20\n",
      "         lit       0.78      0.70      0.74        20\n",
      "         lmo       0.52      0.55      0.54        20\n",
      "         ltz       0.38      0.15      0.21        20\n",
      "         lua       0.90      0.90      0.90        20\n",
      "         lue       0.91      0.97      0.94        30\n",
      "         lug       0.89      0.80      0.84        20\n",
      "         luo       1.00      0.90      0.95        20\n",
      "         lus       1.00      0.90      0.95        20\n",
      "         lvs       0.41      0.65      0.50        20\n",
      "         lzh       0.00      0.00      0.00        20\n",
      "         mad       0.39      0.45      0.42        20\n",
      "         mah       1.00      0.96      0.98        23\n",
      "         mai       0.93      0.65      0.76        20\n",
      "         mal       1.00      1.00      1.00        20\n",
      "         mam       0.90      0.95      0.93        20\n",
      "         mar       0.87      0.65      0.74        20\n",
      "         mau       1.00      1.00      1.00        20\n",
      "         mco       0.95      1.00      0.98        20\n",
      "         meu       0.69      0.90      0.78        20\n",
      "         mgh       0.72      0.90      0.80        20\n",
      "         mhr       0.89      0.85      0.87        20\n",
      "         min       1.00      0.90      0.95        20\n",
      "         mkd       0.79      0.75      0.77        20\n",
      "         mlg       0.56      0.45      0.50        20\n",
      "         mlt       0.75      0.60      0.67        20\n",
      "         mon       0.57      0.72      0.64        40\n",
      "         mos       0.90      0.95      0.93        20\n",
      "         mps       1.00      1.00      1.00        20\n",
      "         mri       0.53      0.45      0.49        20\n",
      "         msa       0.31      0.20      0.24        20\n",
      "         mwl       0.63      0.85      0.72        20\n",
      "         mya       1.00      0.95      0.97        20\n",
      "         myv       0.88      0.35      0.50        20\n",
      "         mzh       0.95      1.00      0.98        20\n",
      "         mzn       0.95      0.95      0.95        20\n",
      "         nap       0.62      0.40      0.48        20\n",
      "         naq       1.00      0.95      0.97        20\n",
      "         nav       1.00      1.00      1.00        20\n",
      "         nbl       0.50      0.40      0.44        20\n",
      "         nch       1.00      0.95      0.97        20\n",
      "         ncj       0.91      1.00      0.95        20\n",
      "         nde       0.75      0.60      0.67        20\n",
      "         ndo       1.00      0.90      0.95        20\n",
      "         nds       0.67      0.70      0.68        20\n",
      "         nep       0.75      0.30      0.43        20\n",
      "         new       0.83      0.75      0.79        20\n",
      "         ngl       0.41      0.59      0.48        22\n",
      "         ngu       1.00      1.00      1.00        20\n",
      "         niu       0.74      0.81      0.77        21\n",
      "         nld       0.65      0.65      0.65        20\n",
      "         nnb       0.82      0.45      0.58        20\n",
      "         nno       0.64      0.70      0.67        20\n",
      "         nob       0.24      0.25      0.24        20\n",
      "         nor       0.40      0.40      0.40        20\n",
      "         npi       0.65      0.85      0.74        20\n",
      "         nso       0.74      0.70      0.72        20\n",
      "         nya       0.92      0.55      0.69        20\n",
      "         nyu       0.90      0.95      0.93        20\n",
      "         oci       0.50      0.40      0.44        20\n",
      "         ori       0.44      0.35      0.39        20\n",
      "         orm       0.94      0.75      0.83        20\n",
      "         ory       0.50      0.55      0.52        20\n",
      "         oss       1.00      0.75      0.86        20\n",
      "         ote       0.83      1.00      0.91        20\n",
      "         pag       0.83      0.95      0.88        20\n",
      "         pam       0.57      0.20      0.30        20\n",
      "         pan       1.00      0.90      0.95        20\n",
      "         pap       1.00      0.90      0.95        20\n",
      "         pau       1.00      1.00      1.00        20\n",
      "         pcd       0.69      0.55      0.61        20\n",
      "         pcm       0.73      0.80      0.76        20\n",
      "         pes       0.47      0.45      0.46        20\n",
      "         pfl       0.68      0.65      0.67        20\n",
      "         pis       0.94      0.85      0.89        20\n",
      "         pls       0.87      1.00      0.93        20\n",
      "         plt       0.92      0.55      0.69        20\n",
      "         pms       1.00      0.95      0.97        20\n",
      "         pnb       0.95      0.90      0.92        20\n",
      "         poh       0.87      1.00      0.93        20\n",
      "         pol       0.77      0.50      0.61        20\n",
      "         pon       0.90      0.90      0.90        20\n",
      "         por       0.52      0.65      0.58        20\n",
      "         prs       0.50      0.65      0.57        20\n",
      "         pus       0.89      0.85      0.87        20\n",
      "         qub       0.82      0.84      0.83        32\n",
      "         quc       0.83      0.75      0.79        20\n",
      "         que       0.80      0.40      0.53        20\n",
      "         quh       0.73      0.80      0.76        20\n",
      "         quw       0.95      0.95      0.95        20\n",
      "         quy       0.94      0.75      0.83        20\n",
      "         quz       0.93      0.67      0.78        21\n",
      "         qvi       0.95      0.95      0.95        20\n",
      "         rap       0.73      0.40      0.52        20\n",
      "         rmy       0.71      0.50      0.59        20\n",
      "         roh       0.67      0.80      0.73        20\n",
      "         ron       0.75      0.60      0.67        20\n",
      "         rop       0.95      0.95      0.95        20\n",
      "         rue       0.88      0.35      0.50        20\n",
      "         rug       0.87      1.00      0.93        20\n",
      "         run       0.35      0.45      0.39        20\n",
      "         sag       0.91      1.00      0.95        20\n",
      "         sah       0.94      0.75      0.83        20\n",
      "         san       0.53      0.59      0.56        32\n",
      "         sat       1.00      0.95      0.97        20\n",
      "         scn       0.60      0.60      0.60        20\n",
      "         sco       0.59      0.65      0.62        20\n",
      "         seh       0.95      0.90      0.92        20\n",
      "         sgs       0.92      0.55      0.69        20\n",
      "         sin       1.00      0.85      0.92        20\n",
      "         slk       0.75      0.45      0.56        20\n",
      "         slv       0.35      0.35      0.35        20\n",
      "         sme       0.82      0.70      0.76        20\n",
      "         smo       0.67      0.60      0.63        20\n",
      "         sna       0.75      0.15      0.25        20\n",
      "         snd       0.95      0.95      0.95        20\n",
      "         som       0.69      0.45      0.55        40\n",
      "         sot       0.63      0.60      0.62        20\n",
      "         spa       0.12      0.15      0.13        20\n",
      "         sqi       0.50      0.50      0.50        20\n",
      "         srd       0.65      0.75      0.70        20\n",
      "         srm       1.00      1.00      1.00        20\n",
      "         srn       0.90      0.95      0.93        20\n",
      "         srp       0.45      0.23      0.30        40\n",
      "         ssw       0.80      0.60      0.69        20\n",
      "         sun       0.71      0.25      0.37        20\n",
      "         suz       0.90      0.90      0.90        20\n",
      "         swa       0.32      0.40      0.36        20\n",
      "         swc       0.57      0.40      0.47        20\n",
      "         swe       1.00      0.75      0.86        20\n",
      "         swh       0.47      0.40      0.43        20\n",
      "         szl       0.94      0.85      0.89        20\n",
      "         tah       0.94      0.85      0.89        20\n",
      "         tam       1.00      0.95      0.97        20\n",
      "         tat       0.30      0.35      0.32        40\n",
      "         tbz       0.82      0.90      0.86        20\n",
      "         tca       1.00      1.00      1.00        20\n",
      "         tdt       1.00      1.00      1.00        20\n",
      "         teo       0.73      0.88      0.80        41\n",
      "         tgk       0.05      0.47      0.08        60\n",
      "         tgl       0.59      0.50      0.54        20\n",
      "         tha       1.00      0.60      0.75        20\n",
      "         tir       0.90      0.95      0.93        20\n",
      "         tlh       0.92      0.55      0.69        20\n",
      "         tls       0.90      0.90      0.90        20\n",
      "         toj       0.77      1.00      0.87        20\n",
      "         tok       0.95      1.00      0.98        20\n",
      "         ton       1.00      0.55      0.71        20\n",
      "         top       1.00      1.00      1.00        20\n",
      "         tpi       1.00      0.95      0.97        20\n",
      "         tsn       0.69      0.55      0.61        20\n",
      "         tso       0.81      0.85      0.83        20\n",
      "         tuc       0.87      1.00      0.93        20\n",
      "         tuk       0.49      0.57      0.53        40\n",
      "         tum       0.78      0.70      0.74        20\n",
      "         tur       0.78      0.35      0.48        20\n",
      "         tvl       0.82      0.90      0.86        20\n",
      "         twi       0.86      0.90      0.88        20\n",
      "         tyv       0.79      0.75      0.77        20\n",
      "         tzo       0.95      1.00      0.98        20\n",
      "         udm       0.91      0.50      0.65        20\n",
      "         uig       0.36      0.57      0.44        40\n",
      "         ukr       0.56      0.45      0.50        20\n",
      "         umb       0.84      0.80      0.82        20\n",
      "         urd       0.38      0.15      0.21        20\n",
      "         uzb       0.42      0.62      0.51        40\n",
      "         uzn       0.33      0.15      0.21        20\n",
      "         vec       0.47      0.40      0.43        20\n",
      "         ven       1.00      0.90      0.95        20\n",
      "         vep       0.76      0.80      0.78        20\n",
      "         vie       1.00      0.85      0.92        20\n",
      "         vls       0.62      0.50      0.56        20\n",
      "         vol       0.90      0.90      0.90        20\n",
      "         wal       1.00      0.85      0.92        20\n",
      "         war       0.83      0.50      0.62        20\n",
      "         wbm       0.98      0.98      0.98        41\n",
      "         wln       0.48      0.50      0.49        20\n",
      "         wol       0.55      0.55      0.55        20\n",
      "         wuu       0.00      0.00      0.00        20\n",
      "         xav       1.00      1.00      1.00        20\n",
      "         xho       0.70      0.35      0.47        20\n",
      "         xmf       1.00      0.65      0.79        20\n",
      "         yao       0.80      1.00      0.89        20\n",
      "         yap       1.00      0.95      0.97        20\n",
      "         yid       1.00      1.00      1.00        20\n",
      "         yom       0.94      0.75      0.83        20\n",
      "         yor       1.00      0.75      0.86        20\n",
      "         yue       0.00      0.00      0.00        20\n",
      "         zai       0.95      1.00      0.98        20\n",
      "         zea       0.70      0.70      0.70        20\n",
      "         zho       0.00      0.00      0.00        20\n",
      "         zlm       0.52      0.75      0.61        20\n",
      "         zsm       0.47      0.35      0.40        20\n",
      "         zul       0.29      0.10      0.15        20\n",
      "\n",
      "    accuracy                           0.68      8230\n",
      "   macro avg       0.75      0.69      0.70      8230\n",
      "weighted avg       0.73      0.68      0.69      8230\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "pipeline_xgb = Pipeline([\n",
    "    ('tfidf', TqdmTfidfVectorizer(max_features=10000, min_df=5, max_df=0.8)),\n",
    "    ('clf', XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    ))\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 5px solid green; padding-left: 10px;\">\n",
    "Changing the classifier did not change the results, so the issue is probably from the tokenizer itself.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    padding: 15px;\n",
    "    border: 2px solid #bee5eb;\n",
    "    border-radius: 5px;\n",
    "    background-color: #d1ecf1;\n",
    "    color: #0c5460;\n",
    "    font-size: 14px;\n",
    "    margin-bottom: 10px;\n",
    "    line-height: 1.5;\n",
    "    max-width: 1125px;\">\n",
    "    <strong>ℹ️ Pipeline Description:</strong>\n",
    "    <p>\n",
    "    The pipeline consists in enconding the text in latent space using the Tfidf Vectorizer with 20k features, and then\n",
    "    classify the examples with a MLP classifier, hoping that it would allow the model to escape the local minimum that trapped\n",
    "    both the XGBoost and Logistic Regression.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TFIDF Fit Transform: 100%|██████████| 32919/32919 [00:00<00:00, 1676957.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 22 epochs took 21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TFIDF Transform: 100%|██████████| 8230/8230 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         abk       1.00      0.45      0.62        20\n",
      "         ace       1.00      0.90      0.95        20\n",
      "         ach       0.86      0.95      0.90        38\n",
      "         acm       0.00      0.00      0.00        20\n",
      "         acr       1.00      0.80      0.89        20\n",
      "         ada       1.00      0.90      0.95        21\n",
      "         afb       0.21      0.15      0.18        20\n",
      "         afr       0.71      0.85      0.77        20\n",
      "         ahk       0.95      1.00      0.98        20\n",
      "         ajp       0.16      0.20      0.18        20\n",
      "         aka       0.64      0.70      0.67        20\n",
      "         aln       0.55      0.55      0.55        20\n",
      "         als       0.47      0.35      0.40        20\n",
      "         alt       0.93      0.70      0.80        20\n",
      "         amh       1.00      0.60      0.75        20\n",
      "         aoj       1.00      1.00      1.00        20\n",
      "         apc       0.21      0.20      0.21        20\n",
      "         ara       0.32      0.35      0.33        20\n",
      "         arb       0.68      0.85      0.76        20\n",
      "         arg       0.35      0.60      0.44        20\n",
      "         arn       1.00      0.95      0.97        20\n",
      "         ary       0.25      0.25      0.25        20\n",
      "         arz       0.71      0.85      0.77        20\n",
      "         asm       1.00      0.85      0.92        20\n",
      "         ast       0.69      0.55      0.61        20\n",
      "         aym       0.75      0.30      0.43        20\n",
      "         ayr       0.71      0.60      0.65        20\n",
      "         azb       0.95      0.90      0.92        20\n",
      "         aze       0.24      0.33      0.28        40\n",
      "         azj       0.57      0.20      0.30        20\n",
      "         bak       0.40      0.40      0.40        40\n",
      "         bam       0.84      0.80      0.82        20\n",
      "         ban       0.43      0.45      0.44        20\n",
      "         bar       0.58      0.55      0.56        20\n",
      "         bcl       0.50      0.35      0.41        20\n",
      "         bel       0.55      0.30      0.39        20\n",
      "         bem       1.00      0.90      0.95        20\n",
      "         ber       0.52      0.55      0.54        20\n",
      "         bew       0.45      0.50      0.48        20\n",
      "         bih       0.56      0.45      0.50        20\n",
      "         bik       0.56      0.70      0.62        20\n",
      "         bis       0.82      0.90      0.86        20\n",
      "         bjn       0.70      0.80      0.74        20\n",
      "         bod       0.86      0.90      0.88        20\n",
      "         bos       0.40      0.20      0.27        20\n",
      "         bpy       1.00      1.00      1.00        20\n",
      "         bqc       1.00      0.90      0.95        20\n",
      "         bre       0.76      0.80      0.78        20\n",
      "         bsb       0.81      0.65      0.72        20\n",
      "         bul       0.42      0.75      0.54        20\n",
      "         bzj       1.00      1.00      1.00        20\n",
      "         cab       1.00      0.85      0.92        20\n",
      "         cak       0.71      0.75      0.73        20\n",
      "         cat       0.85      0.55      0.67        20\n",
      "         cbk       0.69      0.45      0.55        20\n",
      "         ceb       0.76      0.65      0.70        20\n",
      "         ces       0.45      0.25      0.32        20\n",
      "         che       0.95      1.00      0.98        20\n",
      "         chk       0.95      1.00      0.98        20\n",
      "         chv       0.56      0.50      0.53        20\n",
      "         cjk       1.00      0.80      0.89        20\n",
      "         ckb       0.58      0.35      0.44        20\n",
      "         cmn       0.00      0.00      0.00        20\n",
      "         cos       1.00      0.05      0.10        20\n",
      "         crh       0.60      0.80      0.69        40\n",
      "         crs       0.91      1.00      0.95        20\n",
      "         csb       0.86      0.60      0.71        20\n",
      "         csy       0.83      1.00      0.91        20\n",
      "         ctu       0.95      1.00      0.98        20\n",
      "         cuk       1.00      0.95      0.97        20\n",
      "         cym       1.00      0.85      0.92        20\n",
      "         dan       0.74      0.70      0.72        20\n",
      "         deu       0.76      0.95      0.84        20\n",
      "         diq       0.80      0.60      0.69        20\n",
      "         div       1.00      0.35      0.52        20\n",
      "         djk       0.95      1.00      0.98        20\n",
      "         dtp       0.56      0.90      0.69        20\n",
      "         dyu       0.60      0.75      0.67        20\n",
      "         dzo       0.93      0.70      0.80        20\n",
      "         ekk       0.46      0.30      0.36        20\n",
      "         ell       1.00      0.90      0.95        20\n",
      "         eml       0.80      0.80      0.80        20\n",
      "         eng       0.26      0.35      0.30        20\n",
      "         enm       0.45      0.91      0.60        23\n",
      "         epo       0.64      0.70      0.67        20\n",
      "         est       0.46      0.65      0.54        20\n",
      "         eus       0.91      0.50      0.65        20\n",
      "         ewe       0.79      0.95      0.86        20\n",
      "         ext       0.47      0.45      0.46        20\n",
      "         fao       0.85      0.85      0.85        20\n",
      "         fas       0.24      0.20      0.22        20\n",
      "         fij       0.95      1.00      0.98        20\n",
      "         fil       0.62      0.25      0.36        20\n",
      "         fin       0.80      0.40      0.53        20\n",
      "         fon       1.00      0.75      0.86        20\n",
      "         fra       0.88      0.70      0.78        20\n",
      "         frr       1.00      0.55      0.71        20\n",
      "         fry       0.83      0.75      0.79        20\n",
      "         ful       0.62      0.75      0.68        20\n",
      "         fur       0.77      0.85      0.81        20\n",
      "         gcf       0.71      0.85      0.77        20\n",
      "         gil       0.95      0.90      0.92        20\n",
      "         gla       0.78      0.70      0.74        20\n",
      "         gle       0.94      0.80      0.86        20\n",
      "         glg       0.35      0.35      0.35        20\n",
      "         glk       0.44      0.40      0.42        20\n",
      "         glv       0.82      0.90      0.86        20\n",
      "         gom       0.63      0.65      0.64        40\n",
      "         gor       1.00      0.55      0.71        20\n",
      "         grc       0.90      0.95      0.93        20\n",
      "         grn       0.60      0.60      0.60        20\n",
      "         gsw       0.82      0.70      0.76        20\n",
      "         guc       0.95      1.00      0.98        20\n",
      "         gug       0.75      0.90      0.82        20\n",
      "         guj       0.74      0.50      0.60        40\n",
      "         gym       1.00      1.00      1.00        20\n",
      "         hat       0.78      0.70      0.74        20\n",
      "         hau       0.33      0.33      0.33        40\n",
      "         haw       0.86      0.30      0.44        20\n",
      "         hbo       0.81      0.78      0.79        27\n",
      "         hbs       0.36      0.53      0.43        40\n",
      "         heb       0.75      0.45      0.56        20\n",
      "         hif       0.67      0.70      0.68        20\n",
      "         hil       0.92      1.00      0.96        22\n",
      "         hin       0.23      0.57      0.33        40\n",
      "         hmn       1.00      0.60      0.75        20\n",
      "         hmo       0.94      0.85      0.89        20\n",
      "         hne       0.55      0.60      0.57        20\n",
      "         hnj       1.00      1.00      1.00        26\n",
      "         hrv       0.17      0.15      0.16        20\n",
      "         hrx       0.74      0.85      0.79        20\n",
      "         hsb       0.78      0.35      0.48        20\n",
      "         hui       1.00      0.95      0.97        20\n",
      "         hun       0.95      0.90      0.92        20\n",
      "         hus       1.00      1.00      1.00        20\n",
      "         hye       0.88      0.75      0.81        20\n",
      "         hyw       0.92      0.88      0.90        25\n",
      "         iba       0.95      0.95      0.95        22\n",
      "         ibo       0.93      0.70      0.80        20\n",
      "         ido       0.65      0.75      0.70        20\n",
      "         ikk       1.00      1.00      1.00        20\n",
      "         iku       1.00      0.75      0.86        20\n",
      "         ile       0.72      0.90      0.80        20\n",
      "         ilo       0.83      0.95      0.88        20\n",
      "         ina       0.86      0.60      0.71        20\n",
      "         ind       0.43      0.50      0.47        20\n",
      "         isl       0.89      0.85      0.87        20\n",
      "         ita       0.47      0.45      0.46        20\n",
      "         ixl       1.00      1.00      1.00        20\n",
      "         jam       0.94      0.85      0.89        20\n",
      "         jav       0.79      0.75      0.77        20\n",
      "         jbo       0.81      0.85      0.83        20\n",
      "         jpn       0.00      0.00      0.00        20\n",
      "         kaa       0.69      0.62      0.66        40\n",
      "         kab       0.71      0.60      0.65        20\n",
      "         kac       0.91      1.00      0.95        20\n",
      "         kal       1.00      0.55      0.71        20\n",
      "         kam       0.69      0.90      0.78        20\n",
      "         kan       1.00      0.95      0.97        20\n",
      "         kat       0.47      0.53      0.49        40\n",
      "         kau       1.00      0.50      0.67        20\n",
      "         kaz       0.92      0.60      0.73        20\n",
      "         kbd       1.00      0.20      0.33        20\n",
      "         kbp       0.83      0.95      0.88        20\n",
      "         kea       0.64      0.90      0.75        20\n",
      "         kek       1.00      1.00      1.00        20\n",
      "         khm       1.00      0.90      0.95        20\n",
      "         kik       0.94      0.75      0.83        20\n",
      "         kin       0.37      0.35      0.36        20\n",
      "         kir       0.91      0.50      0.65        20\n",
      "         kjb       0.95      1.00      0.98        20\n",
      "         kjh       0.85      0.85      0.85        20\n",
      "         kmb       0.86      0.90      0.88        20\n",
      "         kmr       0.64      0.35      0.45        20\n",
      "         knv       0.87      1.00      0.93        20\n",
      "         kom       0.74      0.70      0.72        20\n",
      "         kon       0.79      0.95      0.86        20\n",
      "         kor       0.25      0.05      0.08        20\n",
      "         kos       1.00      1.00      1.00        20\n",
      "         kpg       1.00      1.00      1.00        20\n",
      "         krc       1.00      0.80      0.89        20\n",
      "         ksd       0.91      1.00      0.95        20\n",
      "         ksh       1.00      0.65      0.79        20\n",
      "         ksw       1.00      0.65      0.79        23\n",
      "         kur       0.44      0.53      0.48        40\n",
      "         lao       1.00      0.90      0.95        20\n",
      "         lat       0.81      0.85      0.83        20\n",
      "         lfn       0.74      0.85      0.79        20\n",
      "         lhu       1.00      0.95      0.97        20\n",
      "         lij       0.65      0.65      0.65        20\n",
      "         lim       0.69      0.55      0.61        20\n",
      "         lin       0.94      0.75      0.83        20\n",
      "         lit       0.78      0.70      0.74        20\n",
      "         lmo       0.52      0.55      0.54        20\n",
      "         ltz       0.38      0.15      0.21        20\n",
      "         lua       0.90      0.90      0.90        20\n",
      "         lue       0.91      0.97      0.94        30\n",
      "         lug       0.89      0.80      0.84        20\n",
      "         luo       1.00      0.90      0.95        20\n",
      "         lus       1.00      0.90      0.95        20\n",
      "         lvs       0.41      0.65      0.50        20\n",
      "         lzh       0.00      0.00      0.00        20\n",
      "         mad       0.39      0.45      0.42        20\n",
      "         mah       1.00      0.96      0.98        23\n",
      "         mai       0.93      0.65      0.76        20\n",
      "         mal       1.00      1.00      1.00        20\n",
      "         mam       0.90      0.95      0.93        20\n",
      "         mar       0.87      0.65      0.74        20\n",
      "         mau       1.00      1.00      1.00        20\n",
      "         mco       0.95      1.00      0.98        20\n",
      "         meu       0.69      0.90      0.78        20\n",
      "         mgh       0.72      0.90      0.80        20\n",
      "         mhr       0.89      0.85      0.87        20\n",
      "         min       1.00      0.90      0.95        20\n",
      "         mkd       0.79      0.75      0.77        20\n",
      "         mlg       0.56      0.45      0.50        20\n",
      "         mlt       0.75      0.60      0.67        20\n",
      "         mon       0.57      0.72      0.64        40\n",
      "         mos       0.90      0.95      0.93        20\n",
      "         mps       1.00      1.00      1.00        20\n",
      "         mri       0.53      0.45      0.49        20\n",
      "         msa       0.31      0.20      0.24        20\n",
      "         mwl       0.63      0.85      0.72        20\n",
      "         mya       1.00      0.95      0.97        20\n",
      "         myv       0.88      0.35      0.50        20\n",
      "         mzh       0.95      1.00      0.98        20\n",
      "         mzn       0.95      0.95      0.95        20\n",
      "         nap       0.62      0.40      0.48        20\n",
      "         naq       1.00      0.95      0.97        20\n",
      "         nav       1.00      1.00      1.00        20\n",
      "         nbl       0.50      0.40      0.44        20\n",
      "         nch       1.00      0.95      0.97        20\n",
      "         ncj       0.91      1.00      0.95        20\n",
      "         nde       0.75      0.60      0.67        20\n",
      "         ndo       1.00      0.90      0.95        20\n",
      "         nds       0.67      0.70      0.68        20\n",
      "         nep       0.75      0.30      0.43        20\n",
      "         new       0.83      0.75      0.79        20\n",
      "         ngl       0.41      0.59      0.48        22\n",
      "         ngu       1.00      1.00      1.00        20\n",
      "         niu       0.74      0.81      0.77        21\n",
      "         nld       0.65      0.65      0.65        20\n",
      "         nnb       0.82      0.45      0.58        20\n",
      "         nno       0.64      0.70      0.67        20\n",
      "         nob       0.24      0.25      0.24        20\n",
      "         nor       0.40      0.40      0.40        20\n",
      "         npi       0.65      0.85      0.74        20\n",
      "         nso       0.74      0.70      0.72        20\n",
      "         nya       0.92      0.55      0.69        20\n",
      "         nyu       0.90      0.95      0.93        20\n",
      "         oci       0.50      0.40      0.44        20\n",
      "         ori       0.44      0.35      0.39        20\n",
      "         orm       0.94      0.75      0.83        20\n",
      "         ory       0.50      0.55      0.52        20\n",
      "         oss       1.00      0.75      0.86        20\n",
      "         ote       0.83      1.00      0.91        20\n",
      "         pag       0.83      0.95      0.88        20\n",
      "         pam       0.57      0.20      0.30        20\n",
      "         pan       1.00      0.90      0.95        20\n",
      "         pap       1.00      0.90      0.95        20\n",
      "         pau       1.00      1.00      1.00        20\n",
      "         pcd       0.69      0.55      0.61        20\n",
      "         pcm       0.73      0.80      0.76        20\n",
      "         pes       0.47      0.45      0.46        20\n",
      "         pfl       0.68      0.65      0.67        20\n",
      "         pis       0.94      0.85      0.89        20\n",
      "         pls       0.87      1.00      0.93        20\n",
      "         plt       0.92      0.55      0.69        20\n",
      "         pms       1.00      0.95      0.97        20\n",
      "         pnb       0.95      0.90      0.92        20\n",
      "         poh       0.87      1.00      0.93        20\n",
      "         pol       0.77      0.50      0.61        20\n",
      "         pon       0.90      0.90      0.90        20\n",
      "         por       0.52      0.65      0.58        20\n",
      "         prs       0.50      0.65      0.57        20\n",
      "         pus       0.89      0.85      0.87        20\n",
      "         qub       0.82      0.84      0.83        32\n",
      "         quc       0.83      0.75      0.79        20\n",
      "         que       0.80      0.40      0.53        20\n",
      "         quh       0.73      0.80      0.76        20\n",
      "         quw       0.95      0.95      0.95        20\n",
      "         quy       0.94      0.75      0.83        20\n",
      "         quz       0.93      0.67      0.78        21\n",
      "         qvi       0.95      0.95      0.95        20\n",
      "         rap       0.73      0.40      0.52        20\n",
      "         rmy       0.71      0.50      0.59        20\n",
      "         roh       0.67      0.80      0.73        20\n",
      "         ron       0.75      0.60      0.67        20\n",
      "         rop       0.95      0.95      0.95        20\n",
      "         rue       0.88      0.35      0.50        20\n",
      "         rug       0.87      1.00      0.93        20\n",
      "         run       0.35      0.45      0.39        20\n",
      "         sag       0.91      1.00      0.95        20\n",
      "         sah       0.94      0.75      0.83        20\n",
      "         san       0.53      0.59      0.56        32\n",
      "         sat       1.00      0.95      0.97        20\n",
      "         scn       0.60      0.60      0.60        20\n",
      "         sco       0.59      0.65      0.62        20\n",
      "         seh       0.95      0.90      0.92        20\n",
      "         sgs       0.92      0.55      0.69        20\n",
      "         sin       1.00      0.85      0.92        20\n",
      "         slk       0.75      0.45      0.56        20\n",
      "         slv       0.35      0.35      0.35        20\n",
      "         sme       0.82      0.70      0.76        20\n",
      "         smo       0.67      0.60      0.63        20\n",
      "         sna       0.75      0.15      0.25        20\n",
      "         snd       0.95      0.95      0.95        20\n",
      "         som       0.69      0.45      0.55        40\n",
      "         sot       0.63      0.60      0.62        20\n",
      "         spa       0.12      0.15      0.13        20\n",
      "         sqi       0.50      0.50      0.50        20\n",
      "         srd       0.65      0.75      0.70        20\n",
      "         srm       1.00      1.00      1.00        20\n",
      "         srn       0.90      0.95      0.93        20\n",
      "         srp       0.43      0.23      0.30        40\n",
      "         ssw       0.80      0.60      0.69        20\n",
      "         sun       0.71      0.25      0.37        20\n",
      "         suz       0.90      0.90      0.90        20\n",
      "         swa       0.32      0.40      0.36        20\n",
      "         swc       0.57      0.40      0.47        20\n",
      "         swe       1.00      0.75      0.86        20\n",
      "         swh       0.47      0.40      0.43        20\n",
      "         szl       0.94      0.85      0.89        20\n",
      "         tah       0.94      0.85      0.89        20\n",
      "         tam       1.00      0.95      0.97        20\n",
      "         tat       0.30      0.35      0.32        40\n",
      "         tbz       0.82      0.90      0.86        20\n",
      "         tca       1.00      1.00      1.00        20\n",
      "         tdt       1.00      1.00      1.00        20\n",
      "         teo       0.73      0.88      0.80        41\n",
      "         tgk       0.05      0.47      0.08        60\n",
      "         tgl       0.59      0.50      0.54        20\n",
      "         tha       1.00      0.60      0.75        20\n",
      "         tir       0.90      0.95      0.93        20\n",
      "         tlh       0.92      0.55      0.69        20\n",
      "         tls       0.90      0.90      0.90        20\n",
      "         toj       0.77      1.00      0.87        20\n",
      "         tok       0.95      1.00      0.98        20\n",
      "         ton       1.00      0.55      0.71        20\n",
      "         top       1.00      1.00      1.00        20\n",
      "         tpi       1.00      0.95      0.97        20\n",
      "         tsn       0.69      0.55      0.61        20\n",
      "         tso       0.81      0.85      0.83        20\n",
      "         tuc       0.87      1.00      0.93        20\n",
      "         tuk       0.49      0.57      0.53        40\n",
      "         tum       0.78      0.70      0.74        20\n",
      "         tur       0.78      0.35      0.48        20\n",
      "         tvl       0.82      0.90      0.86        20\n",
      "         twi       0.86      0.90      0.88        20\n",
      "         tyv       0.79      0.75      0.77        20\n",
      "         tzo       0.95      1.00      0.98        20\n",
      "         udm       0.91      0.50      0.65        20\n",
      "         uig       0.36      0.57      0.44        40\n",
      "         ukr       0.56      0.45      0.50        20\n",
      "         umb       0.84      0.80      0.82        20\n",
      "         urd       0.38      0.15      0.21        20\n",
      "         uzb       0.42      0.62      0.51        40\n",
      "         uzn       0.33      0.15      0.21        20\n",
      "         vec       0.47      0.40      0.43        20\n",
      "         ven       1.00      0.90      0.95        20\n",
      "         vep       0.76      0.80      0.78        20\n",
      "         vie       1.00      0.85      0.92        20\n",
      "         vls       0.62      0.50      0.56        20\n",
      "         vol       0.90      0.90      0.90        20\n",
      "         wal       1.00      0.85      0.92        20\n",
      "         war       0.83      0.50      0.62        20\n",
      "         wbm       0.98      0.98      0.98        41\n",
      "         wln       0.48      0.50      0.49        20\n",
      "         wol       0.55      0.55      0.55        20\n",
      "         wuu       0.00      0.00      0.00        20\n",
      "         xav       1.00      1.00      1.00        20\n",
      "         xho       0.70      0.35      0.47        20\n",
      "         xmf       1.00      0.65      0.79        20\n",
      "         yao       0.80      1.00      0.89        20\n",
      "         yap       1.00      0.95      0.97        20\n",
      "         yid       1.00      1.00      1.00        20\n",
      "         yom       0.94      0.75      0.83        20\n",
      "         yor       1.00      0.75      0.86        20\n",
      "         yue       0.00      0.00      0.00        20\n",
      "         zai       0.95      1.00      0.98        20\n",
      "         zea       0.70      0.70      0.70        20\n",
      "         zho       0.00      0.00      0.00        20\n",
      "         zlm       0.52      0.75      0.61        20\n",
      "         zsm       0.47      0.35      0.40        20\n",
      "         zul       0.29      0.10      0.15        20\n",
      "\n",
      "    accuracy                           0.68      8230\n",
      "   macro avg       0.75      0.69      0.70      8230\n",
      "weighted avg       0.73      0.68      0.69      8230\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(df_train['Label'].unique())\n",
    "class TorchMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(TorchMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 128)\n",
    "        self.out = nn.Linear(128, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class TorchMLPClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim=10000, num_classes=num_labels, lr=0.01, epochs=10, batch_size=32, device=None):\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self._build_model()\n",
    "    def _build_model(self):\n",
    "        self.model_ = TorchMLP(self.input_dim, self.num_classes).to(self.device)\n",
    "    def fit(self, X, y):\n",
    "        if hasattr(X, \"todense\"):\n",
    "            X = X.todense()\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        y = np.array(y)\n",
    "        if self.num_classes == 2:\n",
    "            y_tensor = torch.from_numpy(y).long()\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            y_tensor = torch.from_numpy(y).long()\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        dataset = TensorDataset(torch.from_numpy(X), y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        optimizer = optim.Adam(self.model_.parameters(), lr=self.lr)\n",
    "        self.model_.train()\n",
    "        for _ in range(self.epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model_(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        return \n",
    "    \n",
    "    def predict(self, X):\n",
    "        if hasattr(X, \"todense\"):\n",
    "            X = X.todense()\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        self.model_.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.from_numpy(X).to(self.device)\n",
    "            outputs = self.model_(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "        return predicted.cpu().numpy()\n",
    "    def score(self, X, y):\n",
    "        return accuracy_score(y, self.predict(X))\n",
    "\n",
    "pipeline_torch = Pipeline([\n",
    "    ('tfidf', TqdmTfidfVectorizer(max_features=200000, min_df=5, max_df=0.8)),\n",
    "    ('to_dense', FunctionTransformer(lambda x: x.todense() if hasattr(x, \"todense\") else x)),\n",
    "    ('clf', TorchMLPClassifier(input_dim=200000, num_classes=num_labels, lr=0.01, epochs=100, batch_size=32))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like there might be a bit of an overfitting issue, however changing hyperparameters did not solve the problem.\n",
    "To increase performance, we decided to opt for a different tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TFIDF Transform: 100%|██████████| 38827/38827 [00:00<00:00, 1735313.61it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_test = df_test['Text']\n",
    "y_pred = pipeline.predict(X_test)\n",
    "df_test['Label'] = y_pred\n",
    "df_test.to_csv('results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    padding: 15px;\n",
    "    border: 2px solid #bee5eb;\n",
    "    border-radius: 5px;\n",
    "    background-color: #d1ecf1;\n",
    "    color: #0c5460;\n",
    "    font-size: 14px;\n",
    "    margin-bottom: 10px;\n",
    "    line-height: 1.5;\n",
    "    max-width: 1125px;\">\n",
    "    <strong>ℹ️ Pipeline Description:</strong>\n",
    "    <p>\n",
    "    The pipeline consists in enconding the text in latent space using the Bert Vectorizer, and then\n",
    "    classify the examples with the custom classifier: <b>BertForSequenceClassification</b>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 4.4059 | Train Accuracy: 0.1918\n",
      "Validation Loss: 3.0961 | Validation Accuracy: 0.3687\n",
      "Epoch 2/10\n",
      "Train Loss: 2.4749 | Train Accuracy: 0.4601\n",
      "Validation Loss: 1.9051 | Validation Accuracy: 0.5553\n",
      "Epoch 3/10\n",
      "Train Loss: 1.5376 | Train Accuracy: 0.6329\n",
      "Validation Loss: 1.3710 | Validation Accuracy: 0.6507\n",
      "Epoch 4/10\n",
      "Train Loss: 1.0250 | Train Accuracy: 0.7365\n",
      "Validation Loss: 1.0747 | Validation Accuracy: 0.7135\n",
      "Epoch 5/10\n",
      "Train Loss: 0.7253 | Train Accuracy: 0.8018\n",
      "Validation Loss: 0.9536 | Validation Accuracy: 0.7360\n",
      "Epoch 6/10\n",
      "Train Loss: 0.5377 | Train Accuracy: 0.8463\n",
      "Validation Loss: 0.9050 | Validation Accuracy: 0.7465\n",
      "Epoch 7/10\n",
      "Train Loss: 0.4141 | Train Accuracy: 0.8741\n",
      "Validation Loss: 0.9003 | Validation Accuracy: 0.7510\n",
      "Epoch 8/10\n",
      "Train Loss: 0.3213 | Train Accuracy: 0.9033\n",
      "Validation Loss: 0.8862 | Validation Accuracy: 0.7589\n",
      "Epoch 9/10\n",
      "Train Loss: 0.2601 | Train Accuracy: 0.9210\n",
      "Validation Loss: 0.9166 | Validation Accuracy: 0.7609\n",
      "Epoch 10/10\n",
      "Train Loss: 0.2087 | Train Accuracy: 0.9382\n",
      "Validation Loss: 0.9274 | Validation Accuracy: 0.7618\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_train is your dataset\n",
    "\n",
    "# Step 1: Preprocessing and Splitting the Data\n",
    "class ProportionalSplitter:\n",
    "    @staticmethod\n",
    "    def stratified_split(df, test_size=0.2):\n",
    "        train, test = train_test_split(\n",
    "            df, \n",
    "            test_size=test_size, \n",
    "            stratify=df['Label'], \n",
    "            random_state=42\n",
    "        )\n",
    "        return train, test\n",
    "\n",
    "data = df_train.copy()\n",
    "train_df, test_df = ProportionalSplitter.stratified_split(data)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['Label'] = label_encoder.fit_transform(train_df['Label'])\n",
    "test_df['Label'] = label_encoder.transform(test_df['Label'])\n",
    "\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "# Dataset Class to transform the dataframe in usable dataset.\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Loading Pretrained BERT Model and Tokenizer so that we can finetune it on our own data\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = TextDataset(\n",
    "    texts=train_df['Text'].tolist(), \n",
    "    labels=train_df['Label'].tolist(), \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "test_dataset = TextDataset(\n",
    "    texts=test_df['Text'].tolist(), \n",
    "    labels=test_df['Label'].tolist(), \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Training and evaluation loops\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(dataloader), accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(dataloader), accuracy\n",
    "\n",
    "# Training Setup - forced to use cuda, otherwise the training never converges (without gpu, this pipeline takes too long)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 5px solid green; padding-left: 10px;\">\n",
    "This method allows us to outperform the baseline model and achieve roughly 76% accuracy on the validation set. As seen on the train set, with proper regularization, the performance could probably be slightly increased</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as bert.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"bert.pth\")\n",
    "print(\"Model saved as bert.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving bert for future uses, which allows us to make predictions even if the kernel crashes, since the training took approximately <b>300 minutes</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\AppData\\Local\\Temp\\ipykernel_14300\\2285236433.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"bert.pth\"))\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"bert.pth\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Add predicted labels to df_test\n",
    "def predict_label(texts, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            encoding = tokenizer(\n",
    "                text,\n",
    "                max_length=128,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            pred = torch.argmax(logits, dim=1).item()\n",
    "            predictions.append(pred)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels added to df_test.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Usage</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55</td>\n",
       "      <td>Private</td>\n",
       "      <td>Ponovo dobija riječni oblik do Drežnice.</td>\n",
       "      <td>ngl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71</td>\n",
       "      <td>Private</td>\n",
       "      <td>Se formaron aproximadamente hace apenas unos 1...</td>\n",
       "      <td>cbk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>Private</td>\n",
       "      <td>Data juga harus terbebas dari kepentingan-kepe...</td>\n",
       "      <td>mad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>107</td>\n",
       "      <td>Private</td>\n",
       "      <td>ᐃᒃᓯᕙᐅᑕᖅ (ᑐᓵᔨᑎᒍᑦ): ᖁᔭᓐᓇᒦᒃ  ᒥᔅ ᐅᐃᓐᒥᐅᓪ. ᒥᔅᑕ ᐃᓄᒃ.</td>\n",
       "      <td>iku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>129</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bei Gefor rullt de Kéiseker sech an  riicht se...</td>\n",
       "      <td>ltz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID    Usage                                               Text Label\n",
       "0   55  Private           Ponovo dobija riječni oblik do Drežnice.   ngl\n",
       "1   71  Private  Se formaron aproximadamente hace apenas unos 1...   cbk\n",
       "2   67  Private  Data juga harus terbebas dari kepentingan-kepe...   mad\n",
       "3  107  Private      ᐃᒃᓯᕙᐅᑕᖅ (ᑐᓵᔨᑎᒍᑦ): ᖁᔭᓐᓇᒦᒃ  ᒥᔅ ᐅᐃᓐᒥᐅᓪ. ᒥᔅᑕ ᐃᓄᒃ.   iku\n",
       "4  129  Private  Bei Gefor rullt de Kéiseker sech an  riicht se...   ltz"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict and add to df_test\n",
    "df_test['Label'] = predict_label(df_test['Text'].tolist(), model, tokenizer, device)\n",
    "df_test['Label'] = label_encoder.inverse_transform(df_test['Label'])\n",
    "print(\"Predicted labels added to df_test.\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(\"Submission_louis_v2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

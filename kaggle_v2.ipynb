{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Data available, in different format so we need to repreprocess everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Usage</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136</td>\n",
       "      <td>Public</td>\n",
       "      <td>Finalment  Atena le recibe en l'acropoli d'Ate...</td>\n",
       "      <td>arg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>Public</td>\n",
       "      <td>Jane Laffort fille de Joseph Laffort et d' Ang...</td>\n",
       "      <td>lat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID   Usage                                               Text Label\n",
       "0  136  Public  Finalment  Atena le recibe en l'acropoli d'Ate...   arg\n",
       "1   62  Public  Jane Laffort fille de Joseph Laffort et d' Ang...   lat"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('train_submission.csv')\n",
    "df_train.dropna(subset=['Label'], inplace=True)\n",
    "labels_with_multiple_rows = df_train['Label'].value_counts()\n",
    "labels_to_keep = labels_with_multiple_rows[labels_with_multiple_rows > 1].index\n",
    "df_train = df_train[df_train['Label'].isin(labels_to_keep)]\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Usage</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55</td>\n",
       "      <td>Private</td>\n",
       "      <td>Ponovo dobija riječni oblik do Drežnice.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71</td>\n",
       "      <td>Private</td>\n",
       "      <td>Se formaron aproximadamente hace apenas unos 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    Usage                                               Text\n",
       "0  55  Private           Ponovo dobija riječni oblik do Drežnice.\n",
       "1  71  Private  Se formaron aproximadamente hace apenas unos 1..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('test_without_labels.csv')\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape =  (38750, 4)\n",
      "Test shape =  (38827, 3)\n",
      "List labels length =  385\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Shape = \",df_train.shape)\n",
    "print(\"Test shape = \",df_test.shape)\n",
    "print(\"List labels length = \", len(df_train['Label'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok - So we have 38K sentences in different languages, to classify in 390 categories. If the class is balanced, this would represent a 100:1 ratio, so ok to train without generating new sentences I assume. So first baseline algo will just train an NLP classifier on train dataset, and then use the test dataset to see how good it actually is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_excel(\"Viz.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Label  Count\n",
      "0     tgk    300\n",
      "1     uig    200\n",
      "2     crh    200\n",
      "3     kaa    200\n",
      "4     tat    200\n",
      "..    ...    ...\n",
      "380   wbm      3\n",
      "381   crs      2\n",
      "382   tvl      2\n",
      "383   pau      2\n",
      "384   gil      2\n",
      "\n",
      "[385 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_lab = pd.DataFrame(df_train)\n",
    "\n",
    "label_counts = df_lab[\"Label\"].value_counts().reset_index()\n",
    "label_counts.columns = [\"Label\", \"Count\"]\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 4.4616 | Train Accuracy: 0.1832\n",
      "Validation Loss: 3.1570 | Validation Accuracy: 0.3481\n",
      "Epoch 2/10\n",
      "Train Loss: 2.5570 | Train Accuracy: 0.4339\n",
      "Validation Loss: 2.0046 | Validation Accuracy: 0.5356\n",
      "Epoch 3/10\n",
      "Train Loss: 1.6385 | Train Accuracy: 0.6000\n",
      "Validation Loss: 1.4612 | Validation Accuracy: 0.6221\n",
      "Epoch 4/10\n",
      "Train Loss: 1.1258 | Train Accuracy: 0.7062\n",
      "Validation Loss: 1.1641 | Validation Accuracy: 0.6853\n",
      "Epoch 5/10\n",
      "Train Loss: 0.8052 | Train Accuracy: 0.7798\n",
      "Validation Loss: 1.0138 | Validation Accuracy: 0.7219\n",
      "Epoch 6/10\n",
      "Train Loss: 0.5959 | Train Accuracy: 0.8286\n",
      "Validation Loss: 0.9512 | Validation Accuracy: 0.7266\n",
      "Epoch 7/10\n",
      "Train Loss: 0.4559 | Train Accuracy: 0.8654\n",
      "Validation Loss: 0.9362 | Validation Accuracy: 0.7377\n",
      "Epoch 8/10\n",
      "Train Loss: 0.3531 | Train Accuracy: 0.8956\n",
      "Validation Loss: 0.9100 | Validation Accuracy: 0.7480\n",
      "Epoch 9/10\n",
      "Train Loss: 0.2818 | Train Accuracy: 0.9165\n",
      "Validation Loss: 0.9348 | Validation Accuracy: 0.7480\n",
      "Epoch 10/10\n",
      "Train Loss: 0.2307 | Train Accuracy: 0.9308\n",
      "Validation Loss: 0.9861 | Validation Accuracy: 0.7476\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_train is your dataset\n",
    "\n",
    "# Step 1: Preprocessing and Splitting the Data\n",
    "class ProportionalSplitter:\n",
    "    @staticmethod\n",
    "    def stratified_split(df, test_size=0.2):\n",
    "        train, test = train_test_split(\n",
    "            df, \n",
    "            test_size=test_size, \n",
    "            stratify=df['Label'], \n",
    "            random_state=42\n",
    "        )\n",
    "        return train, test\n",
    "\n",
    "data = df_train.copy()\n",
    "train_df, test_df = ProportionalSplitter.stratified_split(data)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['Label'] = label_encoder.fit_transform(train_df['Label'])\n",
    "test_df['Label'] = label_encoder.transform(test_df['Label'])\n",
    "\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "# Step 2: Define Dataset Class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Step 3: Load Pretrained BERT Model and Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = TextDataset(\n",
    "    texts=train_df['Text'].tolist(), \n",
    "    labels=train_df['Label'].tolist(), \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "test_dataset = TextDataset(\n",
    "    texts=test_df['Text'].tolist(), \n",
    "    labels=test_df['Label'].tolist(), \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "# Step 4: Define DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Step 5: Define Training and Evaluation Loops\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(dataloader), accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(dataloader), accuracy\n",
    "\n",
    "# Step 6: Training Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as bert.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"bert.pth\")\n",
    "print(\"Model saved as bert.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\AppData\\Local\\Temp\\ipykernel_12700\\2285236433.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"bert.pth\"))\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"bert.pth\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Add predicted labels to df_test\n",
    "def predict_label(texts, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            encoding = tokenizer(\n",
    "                text,\n",
    "                max_length=128,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            pred = torch.argmax(logits, dim=1).item()\n",
    "            predictions.append(pred)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels added to df_test.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Usage</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55</td>\n",
       "      <td>Private</td>\n",
       "      <td>Ponovo dobija riječni oblik do Drežnice.</td>\n",
       "      <td>hrv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71</td>\n",
       "      <td>Private</td>\n",
       "      <td>Se formaron aproximadamente hace apenas unos 1...</td>\n",
       "      <td>spa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>Private</td>\n",
       "      <td>Data juga harus terbebas dari kepentingan-kepe...</td>\n",
       "      <td>mad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>107</td>\n",
       "      <td>Private</td>\n",
       "      <td>ᐃᒃᓯᕙᐅᑕᖅ (ᑐᓵᔨᑎᒍᑦ): ᖁᔭᓐᓇᒦᒃ  ᒥᔅ ᐅᐃᓐᒥᐅᓪ. ᒥᔅᑕ ᐃᓄᒃ.</td>\n",
       "      <td>iku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>129</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bei Gefor rullt de Kéiseker sech an  riicht se...</td>\n",
       "      <td>ltz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID    Usage                                               Text Label\n",
       "0   55  Private           Ponovo dobija riječni oblik do Drežnice.   hrv\n",
       "1   71  Private  Se formaron aproximadamente hace apenas unos 1...   spa\n",
       "2   67  Private  Data juga harus terbebas dari kepentingan-kepe...   mad\n",
       "3  107  Private      ᐃᒃᓯᕙᐅᑕᖅ (ᑐᓵᔨᑎᒍᑦ): ᖁᔭᓐᓇᒦᒃ  ᒥᔅ ᐅᐃᓐᒥᐅᓪ. ᒥᔅᑕ ᐃᓄᒃ.   iku\n",
       "4  129  Private  Bei Gefor rullt de Kéiseker sech an  riicht se...   ltz"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict and add to df_test\n",
    "df_test['Label'] = predict_label(df_test['Text'].tolist(), model, tokenizer, device)\n",
    "df_test['Label'] = label_encoder.inverse_transform(df_test['Label'])\n",
    "print(\"Predicted labels added to df_test.\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(\"Submission_louis.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Data available, in different format so we need to repreprocess everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    padding: 15px;\n",
    "    border: 2px solid #bee5eb;\n",
    "    border-radius: 5px;\n",
    "    background-color: #d1ecf1;\n",
    "    color: #0c5460;\n",
    "    font-size: 14px;\n",
    "    margin-bottom: 10px;\n",
    "    line-height: 1.5;\n",
    "    max-width: 1125px;\">\n",
    "    <strong>ℹ️ Data Processing:</strong>\n",
    "    <p>\n",
    "    Although the initial dataset is of sufficient quality, augmenting the dataset did lead to some <b>type inconsistencies</b>, hence the .astype() functions.\n",
    "    Additionally, I erased the 'blanks' row, since I did not want to train a model to predict nothing, which would only decrease accuracy.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Finalment  Atena le recibe en l'acropoli d'Ate...</td>\n",
       "      <td>arg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jane Laffort fille de Joseph Laffort et d' Ang...</td>\n",
       "      <td>lat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Label\n",
       "0  Finalment  Atena le recibe en l'acropoli d'Ate...   arg\n",
       "1  Jane Laffort fille de Joseph Laffort et d' Ang...   lat"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train = pd.read_csv('Lexical_juggling_train.csv')\n",
    "# df_train = pd.read_csv('train_submission.csv')\n",
    "df_train = pd.read_excel('train_augmented.xlsx', sheet_name='Data')\n",
    "df_train.dropna(subset=['Label'], inplace=True)\n",
    "labels_with_multiple_rows = df_train['Label'].value_counts()\n",
    "labels_to_keep = labels_with_multiple_rows[labels_with_multiple_rows > 1].index\n",
    "df_train = df_train[df_train['Label'].isin(labels_to_keep)]\n",
    "df_train['Text'] = df_train['Text'].astype(str)\n",
    "df_train['Label'] = df_train['Label'].astype(str)\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Usage</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Private</td>\n",
       "      <td>Hüttwilen el xe on comune del Canton Turgovia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Private</td>\n",
       "      <td>La leĝo zorgas pri kompenso de nur la plej gra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Usage                                               Text\n",
       "0  Private  Hüttwilen el xe on comune del Canton Turgovia ...\n",
       "1  Private  La leĝo zorgas pri kompenso de nur la plej gra..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('test_without_labels.csv')\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape =  (41149, 2)\n",
      "Test shape =  (190567, 2)\n",
      "List labels length =  385\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Shape = \",df_train.shape)\n",
    "print(\"Test shape = \",df_test.shape)\n",
    "print(\"List labels length = \", len(df_train['Label'].unique()))\n",
    "# print(df_train['Label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "label_counts = df_train['Label'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = sns.color_palette(\"viridis\", 50)\n",
    "n, bins, patches = plt.hist(label_counts, bins=50, color=colors[-1])\n",
    "\n",
    "for i, patch in enumerate(patches):\n",
    "    patch.set_facecolor(colors[i % len(colors)])\n",
    "\n",
    "for i, patch in enumerate(patches):\n",
    "    patch.set_facecolor(colors[i % len(colors)])\n",
    "\n",
    "    if n[i] > 0:\n",
    "        plt.text(patch.get_x() + patch.get_width() / 2, n[i], f\"{int(n[i])}\", \n",
    "                 ha='center', va='bottom', fontsize=10, fontweight='bold', color='black')\n",
    "\n",
    "\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title(\"Number of Observations per Label\", fontsize=14, fontname='serif')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 5px solid green; padding-left: 10px;\">As such, we have <b>38K sentences</b> in different languages, to classify in 390 categories. We assume that each label refers to a language, though the exact semantic is not relevant for the classification. The classes were initially unbalanced. To solve the class imbalance, we decided to proceed with <b>data augmentation</b> (oversampling to be accurate) and generate \"train_augmented.xlsx\" through a mix of gpt2 (initially), and then queries with chatgpt-o1-mini for quality's sake.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Label  Count\n",
      "0     tgk    300\n",
      "1     teo    205\n",
      "2     wbm    203\n",
      "3     hin    200\n",
      "4     tat    200\n",
      "..    ...    ...\n",
      "380   xho    100\n",
      "381   yao    100\n",
      "382   hus    100\n",
      "383   kau    100\n",
      "384   ceb     99\n",
      "\n",
      "[385 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_lab = pd.DataFrame(df_train)\n",
    "\n",
    "label_counts = df_lab[\"Label\"].value_counts().reset_index()\n",
    "label_counts.columns = [\"Label\", \"Count\"]\n",
    "# Simple print to make sure that each label possesses at least 100 observations (ceb was missed by one but don't mind it, I forgot one example)\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 -  Character + Word N-Gram TF-IDF + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class TqdmTfidfVectorizer(TfidfVectorizer):\n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        raw_documents = list(tqdm(raw_documents, desc=\"TFIDF Fit Transform\"))\n",
    "        return super().fit_transform(raw_documents, y)\n",
    "    def transform(self, raw_documents):\n",
    "        raw_documents = list(tqdm(raw_documents, desc=\"TFIDF Transform\"))\n",
    "        return super().transform(raw_documents)\n",
    "\n",
    "# Defining'Text' as X since this will be the data available and 'Label' as Y as we need to classify, hence it is our 'Prediction'\n",
    "X = df_train['Text']\n",
    "y = df_train['Label']\n",
    "\n",
    "# Split in 80/20, sampled randomly while keeping class balance in both train and test sets, using the argument 'stratify' based on labels.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.01, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 22 epochs took 451 seconds\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(analyzer='char_wb', ngram_range=(3, 6), max_features=200000)),\n",
    "    ('clf', LogisticRegression(solver='saga', max_iter=50, verbose=2))\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "# y_pred = pipeline.predict(X_test)\n",
    "y_pred = pipeline.predict(df_test['Text'])\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    padding: 15px;\n",
    "    border: 2px solid #bee5eb;\n",
    "    border-radius: 5px;\n",
    "    background-color: #d1ecf1;\n",
    "    color: #0c5460;\n",
    "    font-size: 14px;\n",
    "    margin-bottom: 10px;\n",
    "    line-height: 1.5;\n",
    "    max-width: 1125px;\">\n",
    "    <strong>ℹ️ Pipeline Description:</strong>\n",
    "    <p>\n",
    "    The baseline pipeline consists in enconding the text in latent space using the Tfidf Vectorizer with 20k features, and then\n",
    "    classify the examples with a logistic regression with 'saga' solver.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TFIDF Fit Transform: 100%|██████████| 32919/32919 [00:00<00:00, 650364.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 19 epochs took 26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TFIDF Transform: 100%|██████████| 8230/8230 [00:00<00:00, 3925303.83it/s]\n",
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         abk       1.00      0.45      0.62        20\n",
      "         ace       1.00      0.90      0.95        20\n",
      "         ach       0.86      0.95      0.90        38\n",
      "         acm       0.00      0.00      0.00        20\n",
      "         acr       1.00      0.80      0.89        20\n",
      "         ada       1.00      0.90      0.95        21\n",
      "         afb       0.21      0.15      0.18        20\n",
      "         afr       0.71      0.85      0.77        20\n",
      "         ahk       0.95      1.00      0.98        20\n",
      "         ajp       0.16      0.20      0.18        20\n",
      "         aka       0.64      0.70      0.67        20\n",
      "         aln       0.55      0.55      0.55        20\n",
      "         als       0.47      0.35      0.40        20\n",
      "         alt       0.93      0.70      0.80        20\n",
      "         amh       1.00      0.60      0.75        20\n",
      "         aoj       1.00      1.00      1.00        20\n",
      "         apc       0.21      0.20      0.21        20\n",
      "         ara       0.32      0.35      0.33        20\n",
      "         arb       0.68      0.85      0.76        20\n",
      "         arg       0.35      0.60      0.44        20\n",
      "         arn       1.00      0.95      0.97        20\n",
      "         ary       0.25      0.25      0.25        20\n",
      "         arz       0.71      0.85      0.77        20\n",
      "         asm       1.00      0.85      0.92        20\n",
      "         ast       0.69      0.55      0.61        20\n",
      "         aym       0.75      0.30      0.43        20\n",
      "         ayr       0.71      0.60      0.65        20\n",
      "         azb       0.95      0.90      0.92        20\n",
      "         aze       0.24      0.33      0.28        40\n",
      "         azj       0.57      0.20      0.30        20\n",
      "         bak       0.40      0.40      0.40        40\n",
      "         bam       0.84      0.80      0.82        20\n",
      "         ban       0.43      0.45      0.44        20\n",
      "         bar       0.58      0.55      0.56        20\n",
      "         bcl       0.50      0.35      0.41        20\n",
      "         bel       0.55      0.30      0.39        20\n",
      "         bem       1.00      0.90      0.95        20\n",
      "         ber       0.52      0.55      0.54        20\n",
      "         bew       0.45      0.50      0.48        20\n",
      "         bih       0.56      0.45      0.50        20\n",
      "         bik       0.56      0.70      0.62        20\n",
      "         bis       0.82      0.90      0.86        20\n",
      "         bjn       0.70      0.80      0.74        20\n",
      "         bod       0.86      0.90      0.88        20\n",
      "         bos       0.40      0.20      0.27        20\n",
      "         bpy       1.00      1.00      1.00        20\n",
      "         bqc       1.00      0.90      0.95        20\n",
      "         bre       0.76      0.80      0.78        20\n",
      "         bsb       0.81      0.65      0.72        20\n",
      "         bul       0.42      0.75      0.54        20\n",
      "         bzj       1.00      1.00      1.00        20\n",
      "         cab       1.00      0.85      0.92        20\n",
      "         cak       0.71      0.75      0.73        20\n",
      "         cat       0.85      0.55      0.67        20\n",
      "         cbk       0.69      0.45      0.55        20\n",
      "         ceb       0.76      0.65      0.70        20\n",
      "         ces       0.45      0.25      0.32        20\n",
      "         che       0.95      1.00      0.98        20\n",
      "         chk       0.95      1.00      0.98        20\n",
      "         chv       0.56      0.50      0.53        20\n",
      "         cjk       1.00      0.80      0.89        20\n",
      "         ckb       0.58      0.35      0.44        20\n",
      "         cmn       0.00      0.00      0.00        20\n",
      "         cos       1.00      0.05      0.10        20\n",
      "         crh       0.60      0.80      0.69        40\n",
      "         crs       0.91      1.00      0.95        20\n",
      "         csb       0.86      0.60      0.71        20\n",
      "         csy       0.83      1.00      0.91        20\n",
      "         ctu       0.95      1.00      0.98        20\n",
      "         cuk       1.00      0.95      0.97        20\n",
      "         cym       1.00      0.85      0.92        20\n",
      "         dan       0.74      0.70      0.72        20\n",
      "         deu       0.76      0.95      0.84        20\n",
      "         diq       0.80      0.60      0.69        20\n",
      "         div       1.00      0.35      0.52        20\n",
      "         djk       0.95      1.00      0.98        20\n",
      "         dtp       0.56      0.90      0.69        20\n",
      "         dyu       0.60      0.75      0.67        20\n",
      "         dzo       0.93      0.70      0.80        20\n",
      "         ekk       0.46      0.30      0.36        20\n",
      "         ell       1.00      0.90      0.95        20\n",
      "         eml       0.80      0.80      0.80        20\n",
      "         eng       0.26      0.35      0.30        20\n",
      "         enm       0.45      0.91      0.60        23\n",
      "         epo       0.64      0.70      0.67        20\n",
      "         est       0.46      0.65      0.54        20\n",
      "         eus       0.91      0.50      0.65        20\n",
      "         ewe       0.79      0.95      0.86        20\n",
      "         ext       0.47      0.45      0.46        20\n",
      "         fao       0.85      0.85      0.85        20\n",
      "         fas       0.24      0.20      0.22        20\n",
      "         fij       0.95      1.00      0.98        20\n",
      "         fil       0.62      0.25      0.36        20\n",
      "         fin       0.80      0.40      0.53        20\n",
      "         fon       1.00      0.75      0.86        20\n",
      "         fra       0.88      0.70      0.78        20\n",
      "         frr       1.00      0.55      0.71        20\n",
      "         fry       0.83      0.75      0.79        20\n",
      "         ful       0.62      0.75      0.68        20\n",
      "         fur       0.77      0.85      0.81        20\n",
      "         gcf       0.71      0.85      0.77        20\n",
      "         gil       0.95      0.90      0.92        20\n",
      "         gla       0.78      0.70      0.74        20\n",
      "         gle       0.94      0.80      0.86        20\n",
      "         glg       0.35      0.35      0.35        20\n",
      "         glk       0.44      0.40      0.42        20\n",
      "         glv       0.82      0.90      0.86        20\n",
      "         gom       0.63      0.65      0.64        40\n",
      "         gor       1.00      0.55      0.71        20\n",
      "         grc       0.90      0.95      0.93        20\n",
      "         grn       0.60      0.60      0.60        20\n",
      "         gsw       0.82      0.70      0.76        20\n",
      "         guc       0.95      1.00      0.98        20\n",
      "         gug       0.75      0.90      0.82        20\n",
      "         guj       0.74      0.50      0.60        40\n",
      "         gym       1.00      1.00      1.00        20\n",
      "         hat       0.78      0.70      0.74        20\n",
      "         hau       0.33      0.33      0.33        40\n",
      "         haw       0.86      0.30      0.44        20\n",
      "         hbo       0.81      0.78      0.79        27\n",
      "         hbs       0.36      0.53      0.43        40\n",
      "         heb       0.75      0.45      0.56        20\n",
      "         hif       0.67      0.70      0.68        20\n",
      "         hil       0.92      1.00      0.96        22\n",
      "         hin       0.23      0.57      0.33        40\n",
      "         hmn       1.00      0.60      0.75        20\n",
      "         hmo       0.94      0.85      0.89        20\n",
      "         hne       0.55      0.60      0.57        20\n",
      "         hnj       1.00      1.00      1.00        26\n",
      "         hrv       0.17      0.15      0.16        20\n",
      "         hrx       0.74      0.85      0.79        20\n",
      "         hsb       0.78      0.35      0.48        20\n",
      "         hui       1.00      0.95      0.97        20\n",
      "         hun       0.95      0.90      0.92        20\n",
      "         hus       1.00      1.00      1.00        20\n",
      "         hye       0.88      0.75      0.81        20\n",
      "         hyw       0.92      0.88      0.90        25\n",
      "         iba       0.95      0.95      0.95        22\n",
      "         ibo       0.93      0.70      0.80        20\n",
      "         ido       0.65      0.75      0.70        20\n",
      "         ikk       1.00      1.00      1.00        20\n",
      "         iku       1.00      0.75      0.86        20\n",
      "         ile       0.72      0.90      0.80        20\n",
      "         ilo       0.83      0.95      0.88        20\n",
      "         ina       0.86      0.60      0.71        20\n",
      "         ind       0.43      0.50      0.47        20\n",
      "         isl       0.89      0.85      0.87        20\n",
      "         ita       0.47      0.45      0.46        20\n",
      "         ixl       1.00      1.00      1.00        20\n",
      "         jam       0.94      0.85      0.89        20\n",
      "         jav       0.79      0.75      0.77        20\n",
      "         jbo       0.81      0.85      0.83        20\n",
      "         jpn       0.00      0.00      0.00        20\n",
      "         kaa       0.69      0.62      0.66        40\n",
      "         kab       0.71      0.60      0.65        20\n",
      "         kac       0.91      1.00      0.95        20\n",
      "         kal       1.00      0.55      0.71        20\n",
      "         kam       0.69      0.90      0.78        20\n",
      "         kan       1.00      0.95      0.97        20\n",
      "         kat       0.47      0.53      0.49        40\n",
      "         kau       1.00      0.50      0.67        20\n",
      "         kaz       0.92      0.60      0.73        20\n",
      "         kbd       1.00      0.20      0.33        20\n",
      "         kbp       0.83      0.95      0.88        20\n",
      "         kea       0.64      0.90      0.75        20\n",
      "         kek       1.00      1.00      1.00        20\n",
      "         khm       1.00      0.90      0.95        20\n",
      "         kik       0.94      0.75      0.83        20\n",
      "         kin       0.37      0.35      0.36        20\n",
      "         kir       0.91      0.50      0.65        20\n",
      "         kjb       0.95      1.00      0.98        20\n",
      "         kjh       0.85      0.85      0.85        20\n",
      "         kmb       0.86      0.90      0.88        20\n",
      "         kmr       0.64      0.35      0.45        20\n",
      "         knv       0.87      1.00      0.93        20\n",
      "         kom       0.74      0.70      0.72        20\n",
      "         kon       0.79      0.95      0.86        20\n",
      "         kor       0.25      0.05      0.08        20\n",
      "         kos       1.00      1.00      1.00        20\n",
      "         kpg       1.00      1.00      1.00        20\n",
      "         krc       1.00      0.80      0.89        20\n",
      "         ksd       0.91      1.00      0.95        20\n",
      "         ksh       1.00      0.65      0.79        20\n",
      "         ksw       1.00      0.65      0.79        23\n",
      "         kur       0.44      0.53      0.48        40\n",
      "         lao       1.00      0.90      0.95        20\n",
      "         lat       0.81      0.85      0.83        20\n",
      "         lfn       0.74      0.85      0.79        20\n",
      "         lhu       1.00      0.95      0.97        20\n",
      "         lij       0.65      0.65      0.65        20\n",
      "         lim       0.69      0.55      0.61        20\n",
      "         lin       0.94      0.75      0.83        20\n",
      "         lit       0.78      0.70      0.74        20\n",
      "         lmo       0.52      0.55      0.54        20\n",
      "         ltz       0.38      0.15      0.21        20\n",
      "         lua       0.90      0.90      0.90        20\n",
      "         lue       0.91      0.97      0.94        30\n",
      "         lug       0.89      0.80      0.84        20\n",
      "         luo       1.00      0.90      0.95        20\n",
      "         lus       1.00      0.90      0.95        20\n",
      "         lvs       0.41      0.65      0.50        20\n",
      "         lzh       0.00      0.00      0.00        20\n",
      "         mad       0.39      0.45      0.42        20\n",
      "         mah       1.00      0.96      0.98        23\n",
      "         mai       0.93      0.65      0.76        20\n",
      "         mal       1.00      1.00      1.00        20\n",
      "         mam       0.90      0.95      0.93        20\n",
      "         mar       0.87      0.65      0.74        20\n",
      "         mau       1.00      1.00      1.00        20\n",
      "         mco       0.95      1.00      0.98        20\n",
      "         meu       0.69      0.90      0.78        20\n",
      "         mgh       0.72      0.90      0.80        20\n",
      "         mhr       0.89      0.85      0.87        20\n",
      "         min       1.00      0.90      0.95        20\n",
      "         mkd       0.79      0.75      0.77        20\n",
      "         mlg       0.56      0.45      0.50        20\n",
      "         mlt       0.75      0.60      0.67        20\n",
      "         mon       0.57      0.72      0.64        40\n",
      "         mos       0.90      0.95      0.93        20\n",
      "         mps       1.00      1.00      1.00        20\n",
      "         mri       0.53      0.45      0.49        20\n",
      "         msa       0.31      0.20      0.24        20\n",
      "         mwl       0.63      0.85      0.72        20\n",
      "         mya       1.00      0.95      0.97        20\n",
      "         myv       0.88      0.35      0.50        20\n",
      "         mzh       0.95      1.00      0.98        20\n",
      "         mzn       0.95      0.95      0.95        20\n",
      "         nap       0.62      0.40      0.48        20\n",
      "         naq       1.00      0.95      0.97        20\n",
      "         nav       1.00      1.00      1.00        20\n",
      "         nbl       0.50      0.40      0.44        20\n",
      "         nch       1.00      0.95      0.97        20\n",
      "         ncj       0.91      1.00      0.95        20\n",
      "         nde       0.75      0.60      0.67        20\n",
      "         ndo       1.00      0.90      0.95        20\n",
      "         nds       0.67      0.70      0.68        20\n",
      "         nep       0.75      0.30      0.43        20\n",
      "         new       0.83      0.75      0.79        20\n",
      "         ngl       0.41      0.59      0.48        22\n",
      "         ngu       1.00      1.00      1.00        20\n",
      "         niu       0.74      0.81      0.77        21\n",
      "         nld       0.65      0.65      0.65        20\n",
      "         nnb       0.82      0.45      0.58        20\n",
      "         nno       0.64      0.70      0.67        20\n",
      "         nob       0.24      0.25      0.24        20\n",
      "         nor       0.40      0.40      0.40        20\n",
      "         npi       0.65      0.85      0.74        20\n",
      "         nso       0.74      0.70      0.72        20\n",
      "         nya       0.92      0.55      0.69        20\n",
      "         nyu       0.90      0.95      0.93        20\n",
      "         oci       0.50      0.40      0.44        20\n",
      "         ori       0.44      0.35      0.39        20\n",
      "         orm       0.94      0.75      0.83        20\n",
      "         ory       0.50      0.55      0.52        20\n",
      "         oss       1.00      0.75      0.86        20\n",
      "         ote       0.83      1.00      0.91        20\n",
      "         pag       0.83      0.95      0.88        20\n",
      "         pam       0.57      0.20      0.30        20\n",
      "         pan       1.00      0.90      0.95        20\n",
      "         pap       1.00      0.90      0.95        20\n",
      "         pau       1.00      1.00      1.00        20\n",
      "         pcd       0.69      0.55      0.61        20\n",
      "         pcm       0.73      0.80      0.76        20\n",
      "         pes       0.47      0.45      0.46        20\n",
      "         pfl       0.68      0.65      0.67        20\n",
      "         pis       0.94      0.85      0.89        20\n",
      "         pls       0.87      1.00      0.93        20\n",
      "         plt       0.92      0.55      0.69        20\n",
      "         pms       1.00      0.95      0.97        20\n",
      "         pnb       0.95      0.90      0.92        20\n",
      "         poh       0.87      1.00      0.93        20\n",
      "         pol       0.77      0.50      0.61        20\n",
      "         pon       0.90      0.90      0.90        20\n",
      "         por       0.52      0.65      0.58        20\n",
      "         prs       0.50      0.65      0.57        20\n",
      "         pus       0.89      0.85      0.87        20\n",
      "         qub       0.82      0.84      0.83        32\n",
      "         quc       0.83      0.75      0.79        20\n",
      "         que       0.80      0.40      0.53        20\n",
      "         quh       0.73      0.80      0.76        20\n",
      "         quw       0.95      0.95      0.95        20\n",
      "         quy       0.94      0.75      0.83        20\n",
      "         quz       0.93      0.67      0.78        21\n",
      "         qvi       0.95      0.95      0.95        20\n",
      "         rap       0.73      0.40      0.52        20\n",
      "         rmy       0.71      0.50      0.59        20\n",
      "         roh       0.67      0.80      0.73        20\n",
      "         ron       0.75      0.60      0.67        20\n",
      "         rop       0.95      0.95      0.95        20\n",
      "         rue       0.88      0.35      0.50        20\n",
      "         rug       0.87      1.00      0.93        20\n",
      "         run       0.35      0.45      0.39        20\n",
      "         sag       0.91      1.00      0.95        20\n",
      "         sah       0.94      0.75      0.83        20\n",
      "         san       0.53      0.59      0.56        32\n",
      "         sat       1.00      0.95      0.97        20\n",
      "         scn       0.60      0.60      0.60        20\n",
      "         sco       0.59      0.65      0.62        20\n",
      "         seh       0.95      0.90      0.92        20\n",
      "         sgs       0.92      0.55      0.69        20\n",
      "         sin       1.00      0.85      0.92        20\n",
      "         slk       0.75      0.45      0.56        20\n",
      "         slv       0.35      0.35      0.35        20\n",
      "         sme       0.82      0.70      0.76        20\n",
      "         smo       0.67      0.60      0.63        20\n",
      "         sna       0.75      0.15      0.25        20\n",
      "         snd       0.95      0.95      0.95        20\n",
      "         som       0.69      0.45      0.55        40\n",
      "         sot       0.63      0.60      0.62        20\n",
      "         spa       0.12      0.15      0.13        20\n",
      "         sqi       0.50      0.50      0.50        20\n",
      "         srd       0.65      0.75      0.70        20\n",
      "         srm       1.00      1.00      1.00        20\n",
      "         srn       0.90      0.95      0.93        20\n",
      "         srp       0.43      0.23      0.30        40\n",
      "         ssw       0.80      0.60      0.69        20\n",
      "         sun       0.71      0.25      0.37        20\n",
      "         suz       0.90      0.90      0.90        20\n",
      "         swa       0.32      0.40      0.36        20\n",
      "         swc       0.57      0.40      0.47        20\n",
      "         swe       1.00      0.75      0.86        20\n",
      "         swh       0.47      0.40      0.43        20\n",
      "         szl       0.94      0.85      0.89        20\n",
      "         tah       0.94      0.85      0.89        20\n",
      "         tam       1.00      0.95      0.97        20\n",
      "         tat       0.30      0.35      0.32        40\n",
      "         tbz       0.82      0.90      0.86        20\n",
      "         tca       1.00      1.00      1.00        20\n",
      "         tdt       1.00      1.00      1.00        20\n",
      "         teo       0.73      0.88      0.80        41\n",
      "         tgk       0.05      0.47      0.08        60\n",
      "         tgl       0.59      0.50      0.54        20\n",
      "         tha       1.00      0.60      0.75        20\n",
      "         tir       0.90      0.95      0.93        20\n",
      "         tlh       0.92      0.55      0.69        20\n",
      "         tls       0.90      0.90      0.90        20\n",
      "         toj       0.77      1.00      0.87        20\n",
      "         tok       0.95      1.00      0.98        20\n",
      "         ton       1.00      0.55      0.71        20\n",
      "         top       1.00      1.00      1.00        20\n",
      "         tpi       1.00      0.95      0.97        20\n",
      "         tsn       0.69      0.55      0.61        20\n",
      "         tso       0.81      0.85      0.83        20\n",
      "         tuc       0.87      1.00      0.93        20\n",
      "         tuk       0.49      0.57      0.53        40\n",
      "         tum       0.78      0.70      0.74        20\n",
      "         tur       0.78      0.35      0.48        20\n",
      "         tvl       0.82      0.90      0.86        20\n",
      "         twi       0.86      0.90      0.88        20\n",
      "         tyv       0.79      0.75      0.77        20\n",
      "         tzo       0.95      1.00      0.98        20\n",
      "         udm       0.91      0.50      0.65        20\n",
      "         uig       0.36      0.57      0.44        40\n",
      "         ukr       0.56      0.45      0.50        20\n",
      "         umb       0.84      0.80      0.82        20\n",
      "         urd       0.38      0.15      0.21        20\n",
      "         uzb       0.42      0.62      0.51        40\n",
      "         uzn       0.33      0.15      0.21        20\n",
      "         vec       0.47      0.40      0.43        20\n",
      "         ven       1.00      0.90      0.95        20\n",
      "         vep       0.76      0.80      0.78        20\n",
      "         vie       1.00      0.85      0.92        20\n",
      "         vls       0.62      0.50      0.56        20\n",
      "         vol       0.90      0.90      0.90        20\n",
      "         wal       1.00      0.85      0.92        20\n",
      "         war       0.83      0.50      0.62        20\n",
      "         wbm       0.98      0.98      0.98        41\n",
      "         wln       0.48      0.50      0.49        20\n",
      "         wol       0.55      0.55      0.55        20\n",
      "         wuu       0.00      0.00      0.00        20\n",
      "         xav       1.00      1.00      1.00        20\n",
      "         xho       0.70      0.35      0.47        20\n",
      "         xmf       1.00      0.65      0.79        20\n",
      "         yao       0.80      1.00      0.89        20\n",
      "         yap       1.00      0.95      0.97        20\n",
      "         yid       1.00      1.00      1.00        20\n",
      "         yom       0.94      0.75      0.83        20\n",
      "         yor       1.00      0.75      0.86        20\n",
      "         yue       0.00      0.00      0.00        20\n",
      "         zai       0.95      1.00      0.98        20\n",
      "         zea       0.70      0.70      0.70        20\n",
      "         zho       0.00      0.00      0.00        20\n",
      "         zlm       0.52      0.75      0.61        20\n",
      "         zsm       0.47      0.35      0.40        20\n",
      "         zul       0.29      0.10      0.15        20\n",
      "\n",
      "    accuracy                           0.68      8230\n",
      "   macro avg       0.75      0.69      0.70      8230\n",
      "weighted avg       0.73      0.68      0.69      8230\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TqdmTfidfVectorizer(max_features=20000, min_df=5, max_df=0.8)),\n",
    "    ('clf', LogisticRegression(solver='saga', max_iter=1000, verbose=1))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "# y_pred = pipeline.predict(df_test['Text'])\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190567, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>epo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>urd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>quy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>chk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID Label\n",
       "0   1   ven\n",
       "1   2   epo\n",
       "2   3   urd\n",
       "3   4   quy\n",
       "4   5   chk"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(y_pred, columns=['Label'])\n",
    "df['ID'] = df.index + 1\n",
    "df = df[['ID', 'Label']]\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 5px solid green; padding-left: 10px;\">\n",
    "The overall accuracy reached with the baseline model is 73%, which is quite satisfactory.\n",
    "However, when looking at the detail class by class (cf. print above), the variance is quite large. We need a model more consistant.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2 - XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    padding: 15px;\n",
    "    border: 2px solid #bee5eb;\n",
    "    border-radius: 5px;\n",
    "    background-color: #d1ecf1;\n",
    "    color: #0c5460;\n",
    "    font-size: 14px;\n",
    "    margin-bottom: 10px;\n",
    "    line-height: 1.5;\n",
    "    max-width: 1125px;\">\n",
    "    <strong>ℹ️ Pipeline Description:</strong>\n",
    "    <p>\n",
    "    The pipeline consists in enconding the text in latent space using the Tfidf Vectorizer with 20k features, and then\n",
    "    classify the examples with an ensemblist method: XGBoost.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TFIDF Fit Transform: 100%|██████████| 32919/32919 [00:00<00:00, 1828893.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 23 epochs took 25 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TFIDF Transform: 100%|██████████| 8230/8230 [00:00<00:00, 2057771.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         abk       1.00      0.45      0.62        20\n",
      "         ace       1.00      0.90      0.95        20\n",
      "         ach       0.86      0.95      0.90        38\n",
      "         acm       0.00      0.00      0.00        20\n",
      "         acr       1.00      0.80      0.89        20\n",
      "         ada       1.00      0.90      0.95        21\n",
      "         afb       0.21      0.15      0.18        20\n",
      "         afr       0.71      0.85      0.77        20\n",
      "         ahk       0.95      1.00      0.98        20\n",
      "         ajp       0.16      0.20      0.18        20\n",
      "         aka       0.64      0.70      0.67        20\n",
      "         aln       0.55      0.55      0.55        20\n",
      "         als       0.47      0.35      0.40        20\n",
      "         alt       0.93      0.70      0.80        20\n",
      "         amh       1.00      0.60      0.75        20\n",
      "         aoj       1.00      1.00      1.00        20\n",
      "         apc       0.21      0.20      0.21        20\n",
      "         ara       0.32      0.35      0.33        20\n",
      "         arb       0.68      0.85      0.76        20\n",
      "         arg       0.35      0.60      0.44        20\n",
      "         arn       1.00      0.95      0.97        20\n",
      "         ary       0.25      0.25      0.25        20\n",
      "         arz       0.71      0.85      0.77        20\n",
      "         asm       1.00      0.85      0.92        20\n",
      "         ast       0.69      0.55      0.61        20\n",
      "         aym       0.75      0.30      0.43        20\n",
      "         ayr       0.71      0.60      0.65        20\n",
      "         azb       0.95      0.90      0.92        20\n",
      "         aze       0.24      0.33      0.28        40\n",
      "         azj       0.57      0.20      0.30        20\n",
      "         bak       0.40      0.40      0.40        40\n",
      "         bam       0.84      0.80      0.82        20\n",
      "         ban       0.43      0.45      0.44        20\n",
      "         bar       0.58      0.55      0.56        20\n",
      "         bcl       0.50      0.35      0.41        20\n",
      "         bel       0.55      0.30      0.39        20\n",
      "         bem       1.00      0.90      0.95        20\n",
      "         ber       0.52      0.55      0.54        20\n",
      "         bew       0.45      0.50      0.48        20\n",
      "         bih       0.56      0.45      0.50        20\n",
      "         bik       0.56      0.70      0.62        20\n",
      "         bis       0.82      0.90      0.86        20\n",
      "         bjn       0.70      0.80      0.74        20\n",
      "         bod       0.86      0.90      0.88        20\n",
      "         bos       0.40      0.20      0.27        20\n",
      "         bpy       1.00      1.00      1.00        20\n",
      "         bqc       1.00      0.90      0.95        20\n",
      "         bre       0.76      0.80      0.78        20\n",
      "         bsb       0.81      0.65      0.72        20\n",
      "         bul       0.42      0.75      0.54        20\n",
      "         bzj       1.00      1.00      1.00        20\n",
      "         cab       1.00      0.85      0.92        20\n",
      "         cak       0.71      0.75      0.73        20\n",
      "         cat       0.85      0.55      0.67        20\n",
      "         cbk       0.69      0.45      0.55        20\n",
      "         ceb       0.76      0.65      0.70        20\n",
      "         ces       0.45      0.25      0.32        20\n",
      "         che       0.95      1.00      0.98        20\n",
      "         chk       0.95      1.00      0.98        20\n",
      "         chv       0.56      0.50      0.53        20\n",
      "         cjk       1.00      0.80      0.89        20\n",
      "         ckb       0.58      0.35      0.44        20\n",
      "         cmn       0.00      0.00      0.00        20\n",
      "         cos       1.00      0.05      0.10        20\n",
      "         crh       0.60      0.80      0.69        40\n",
      "         crs       0.91      1.00      0.95        20\n",
      "         csb       0.86      0.60      0.71        20\n",
      "         csy       0.83      1.00      0.91        20\n",
      "         ctu       0.95      1.00      0.98        20\n",
      "         cuk       1.00      0.95      0.97        20\n",
      "         cym       1.00      0.85      0.92        20\n",
      "         dan       0.74      0.70      0.72        20\n",
      "         deu       0.76      0.95      0.84        20\n",
      "         diq       0.80      0.60      0.69        20\n",
      "         div       1.00      0.35      0.52        20\n",
      "         djk       0.95      1.00      0.98        20\n",
      "         dtp       0.56      0.90      0.69        20\n",
      "         dyu       0.60      0.75      0.67        20\n",
      "         dzo       0.93      0.70      0.80        20\n",
      "         ekk       0.46      0.30      0.36        20\n",
      "         ell       1.00      0.90      0.95        20\n",
      "         eml       0.80      0.80      0.80        20\n",
      "         eng       0.26      0.35      0.30        20\n",
      "         enm       0.45      0.91      0.60        23\n",
      "         epo       0.64      0.70      0.67        20\n",
      "         est       0.46      0.65      0.54        20\n",
      "         eus       0.91      0.50      0.65        20\n",
      "         ewe       0.79      0.95      0.86        20\n",
      "         ext       0.47      0.45      0.46        20\n",
      "         fao       0.85      0.85      0.85        20\n",
      "         fas       0.24      0.20      0.22        20\n",
      "         fij       0.95      1.00      0.98        20\n",
      "         fil       0.62      0.25      0.36        20\n",
      "         fin       0.80      0.40      0.53        20\n",
      "         fon       1.00      0.75      0.86        20\n",
      "         fra       0.88      0.70      0.78        20\n",
      "         frr       1.00      0.55      0.71        20\n",
      "         fry       0.83      0.75      0.79        20\n",
      "         ful       0.62      0.75      0.68        20\n",
      "         fur       0.77      0.85      0.81        20\n",
      "         gcf       0.71      0.85      0.77        20\n",
      "         gil       0.95      0.90      0.92        20\n",
      "         gla       0.78      0.70      0.74        20\n",
      "         gle       0.94      0.80      0.86        20\n",
      "         glg       0.35      0.35      0.35        20\n",
      "         glk       0.44      0.40      0.42        20\n",
      "         glv       0.82      0.90      0.86        20\n",
      "         gom       0.63      0.65      0.64        40\n",
      "         gor       1.00      0.55      0.71        20\n",
      "         grc       0.90      0.95      0.93        20\n",
      "         grn       0.60      0.60      0.60        20\n",
      "         gsw       0.82      0.70      0.76        20\n",
      "         guc       0.95      1.00      0.98        20\n",
      "         gug       0.75      0.90      0.82        20\n",
      "         guj       0.74      0.50      0.60        40\n",
      "         gym       1.00      1.00      1.00        20\n",
      "         hat       0.78      0.70      0.74        20\n",
      "         hau       0.33      0.33      0.33        40\n",
      "         haw       0.86      0.30      0.44        20\n",
      "         hbo       0.81      0.78      0.79        27\n",
      "         hbs       0.36      0.53      0.42        40\n",
      "         heb       0.75      0.45      0.56        20\n",
      "         hif       0.67      0.70      0.68        20\n",
      "         hil       0.92      1.00      0.96        22\n",
      "         hin       0.23      0.57      0.33        40\n",
      "         hmn       1.00      0.60      0.75        20\n",
      "         hmo       0.94      0.85      0.89        20\n",
      "         hne       0.55      0.60      0.57        20\n",
      "         hnj       1.00      1.00      1.00        26\n",
      "         hrv       0.17      0.15      0.16        20\n",
      "         hrx       0.74      0.85      0.79        20\n",
      "         hsb       0.78      0.35      0.48        20\n",
      "         hui       1.00      0.95      0.97        20\n",
      "         hun       0.95      0.90      0.92        20\n",
      "         hus       1.00      1.00      1.00        20\n",
      "         hye       0.88      0.75      0.81        20\n",
      "         hyw       0.92      0.88      0.90        25\n",
      "         iba       0.95      0.95      0.95        22\n",
      "         ibo       0.93      0.70      0.80        20\n",
      "         ido       0.65      0.75      0.70        20\n",
      "         ikk       1.00      1.00      1.00        20\n",
      "         iku       1.00      0.75      0.86        20\n",
      "         ile       0.72      0.90      0.80        20\n",
      "         ilo       0.83      0.95      0.88        20\n",
      "         ina       0.86      0.60      0.71        20\n",
      "         ind       0.43      0.50      0.47        20\n",
      "         isl       0.89      0.85      0.87        20\n",
      "         ita       0.47      0.45      0.46        20\n",
      "         ixl       1.00      1.00      1.00        20\n",
      "         jam       0.94      0.85      0.89        20\n",
      "         jav       0.79      0.75      0.77        20\n",
      "         jbo       0.81      0.85      0.83        20\n",
      "         jpn       0.00      0.00      0.00        20\n",
      "         kaa       0.69      0.62      0.66        40\n",
      "         kab       0.71      0.60      0.65        20\n",
      "         kac       0.91      1.00      0.95        20\n",
      "         kal       1.00      0.55      0.71        20\n",
      "         kam       0.69      0.90      0.78        20\n",
      "         kan       1.00      0.95      0.97        20\n",
      "         kat       0.47      0.53      0.49        40\n",
      "         kau       1.00      0.50      0.67        20\n",
      "         kaz       0.92      0.60      0.73        20\n",
      "         kbd       1.00      0.20      0.33        20\n",
      "         kbp       0.83      0.95      0.88        20\n",
      "         kea       0.64      0.90      0.75        20\n",
      "         kek       1.00      1.00      1.00        20\n",
      "         khm       1.00      0.90      0.95        20\n",
      "         kik       0.94      0.75      0.83        20\n",
      "         kin       0.37      0.35      0.36        20\n",
      "         kir       0.91      0.50      0.65        20\n",
      "         kjb       0.95      1.00      0.98        20\n",
      "         kjh       0.85      0.85      0.85        20\n",
      "         kmb       0.86      0.90      0.88        20\n",
      "         kmr       0.64      0.35      0.45        20\n",
      "         knv       0.87      1.00      0.93        20\n",
      "         kom       0.74      0.70      0.72        20\n",
      "         kon       0.79      0.95      0.86        20\n",
      "         kor       0.25      0.05      0.08        20\n",
      "         kos       1.00      1.00      1.00        20\n",
      "         kpg       1.00      1.00      1.00        20\n",
      "         krc       1.00      0.80      0.89        20\n",
      "         ksd       0.91      1.00      0.95        20\n",
      "         ksh       1.00      0.65      0.79        20\n",
      "         ksw       1.00      0.65      0.79        23\n",
      "         kur       0.44      0.53      0.48        40\n",
      "         lao       1.00      0.90      0.95        20\n",
      "         lat       0.81      0.85      0.83        20\n",
      "         lfn       0.74      0.85      0.79        20\n",
      "         lhu       1.00      0.95      0.97        20\n",
      "         lij       0.65      0.65      0.65        20\n",
      "         lim       0.69      0.55      0.61        20\n",
      "         lin       0.94      0.75      0.83        20\n",
      "         lit       0.78      0.70      0.74        20\n",
      "         lmo       0.52      0.55      0.54        20\n",
      "         ltz       0.38      0.15      0.21        20\n",
      "         lua       0.90      0.90      0.90        20\n",
      "         lue       0.91      0.97      0.94        30\n",
      "         lug       0.89      0.80      0.84        20\n",
      "         luo       1.00      0.90      0.95        20\n",
      "         lus       1.00      0.90      0.95        20\n",
      "         lvs       0.41      0.65      0.50        20\n",
      "         lzh       0.00      0.00      0.00        20\n",
      "         mad       0.39      0.45      0.42        20\n",
      "         mah       1.00      0.96      0.98        23\n",
      "         mai       0.93      0.65      0.76        20\n",
      "         mal       1.00      1.00      1.00        20\n",
      "         mam       0.90      0.95      0.93        20\n",
      "         mar       0.87      0.65      0.74        20\n",
      "         mau       1.00      1.00      1.00        20\n",
      "         mco       0.95      1.00      0.98        20\n",
      "         meu       0.69      0.90      0.78        20\n",
      "         mgh       0.72      0.90      0.80        20\n",
      "         mhr       0.89      0.85      0.87        20\n",
      "         min       1.00      0.90      0.95        20\n",
      "         mkd       0.79      0.75      0.77        20\n",
      "         mlg       0.56      0.45      0.50        20\n",
      "         mlt       0.75      0.60      0.67        20\n",
      "         mon       0.57      0.72      0.64        40\n",
      "         mos       0.90      0.95      0.93        20\n",
      "         mps       1.00      1.00      1.00        20\n",
      "         mri       0.53      0.45      0.49        20\n",
      "         msa       0.31      0.20      0.24        20\n",
      "         mwl       0.63      0.85      0.72        20\n",
      "         mya       1.00      0.95      0.97        20\n",
      "         myv       0.88      0.35      0.50        20\n",
      "         mzh       0.95      1.00      0.98        20\n",
      "         mzn       0.95      0.95      0.95        20\n",
      "         nap       0.62      0.40      0.48        20\n",
      "         naq       1.00      0.95      0.97        20\n",
      "         nav       1.00      1.00      1.00        20\n",
      "         nbl       0.50      0.40      0.44        20\n",
      "         nch       1.00      0.95      0.97        20\n",
      "         ncj       0.91      1.00      0.95        20\n",
      "         nde       0.75      0.60      0.67        20\n",
      "         ndo       1.00      0.90      0.95        20\n",
      "         nds       0.67      0.70      0.68        20\n",
      "         nep       0.75      0.30      0.43        20\n",
      "         new       0.83      0.75      0.79        20\n",
      "         ngl       0.41      0.59      0.48        22\n",
      "         ngu       1.00      1.00      1.00        20\n",
      "         niu       0.74      0.81      0.77        21\n",
      "         nld       0.65      0.65      0.65        20\n",
      "         nnb       0.82      0.45      0.58        20\n",
      "         nno       0.64      0.70      0.67        20\n",
      "         nob       0.24      0.25      0.24        20\n",
      "         nor       0.40      0.40      0.40        20\n",
      "         npi       0.65      0.85      0.74        20\n",
      "         nso       0.74      0.70      0.72        20\n",
      "         nya       0.92      0.55      0.69        20\n",
      "         nyu       0.90      0.95      0.93        20\n",
      "         oci       0.50      0.40      0.44        20\n",
      "         ori       0.44      0.35      0.39        20\n",
      "         orm       0.94      0.75      0.83        20\n",
      "         ory       0.50      0.55      0.52        20\n",
      "         oss       1.00      0.75      0.86        20\n",
      "         ote       0.83      1.00      0.91        20\n",
      "         pag       0.83      0.95      0.88        20\n",
      "         pam       0.57      0.20      0.30        20\n",
      "         pan       1.00      0.90      0.95        20\n",
      "         pap       1.00      0.90      0.95        20\n",
      "         pau       1.00      1.00      1.00        20\n",
      "         pcd       0.69      0.55      0.61        20\n",
      "         pcm       0.73      0.80      0.76        20\n",
      "         pes       0.47      0.45      0.46        20\n",
      "         pfl       0.68      0.65      0.67        20\n",
      "         pis       0.94      0.85      0.89        20\n",
      "         pls       0.87      1.00      0.93        20\n",
      "         plt       0.92      0.55      0.69        20\n",
      "         pms       1.00      0.95      0.97        20\n",
      "         pnb       0.95      0.90      0.92        20\n",
      "         poh       0.87      1.00      0.93        20\n",
      "         pol       0.77      0.50      0.61        20\n",
      "         pon       0.90      0.90      0.90        20\n",
      "         por       0.52      0.65      0.58        20\n",
      "         prs       0.50      0.65      0.57        20\n",
      "         pus       0.89      0.85      0.87        20\n",
      "         qub       0.82      0.84      0.83        32\n",
      "         quc       0.83      0.75      0.79        20\n",
      "         que       0.80      0.40      0.53        20\n",
      "         quh       0.73      0.80      0.76        20\n",
      "         quw       0.95      0.95      0.95        20\n",
      "         quy       0.94      0.75      0.83        20\n",
      "         quz       0.93      0.67      0.78        21\n",
      "         qvi       0.95      0.95      0.95        20\n",
      "         rap       0.73      0.40      0.52        20\n",
      "         rmy       0.71      0.50      0.59        20\n",
      "         roh       0.67      0.80      0.73        20\n",
      "         ron       0.75      0.60      0.67        20\n",
      "         rop       0.95      0.95      0.95        20\n",
      "         rue       0.88      0.35      0.50        20\n",
      "         rug       0.87      1.00      0.93        20\n",
      "         run       0.35      0.45      0.39        20\n",
      "         sag       0.91      1.00      0.95        20\n",
      "         sah       0.94      0.75      0.83        20\n",
      "         san       0.53      0.59      0.56        32\n",
      "         sat       1.00      0.95      0.97        20\n",
      "         scn       0.60      0.60      0.60        20\n",
      "         sco       0.59      0.65      0.62        20\n",
      "         seh       0.95      0.90      0.92        20\n",
      "         sgs       0.92      0.55      0.69        20\n",
      "         sin       1.00      0.85      0.92        20\n",
      "         slk       0.75      0.45      0.56        20\n",
      "         slv       0.35      0.35      0.35        20\n",
      "         sme       0.82      0.70      0.76        20\n",
      "         smo       0.67      0.60      0.63        20\n",
      "         sna       0.75      0.15      0.25        20\n",
      "         snd       0.95      0.95      0.95        20\n",
      "         som       0.69      0.45      0.55        40\n",
      "         sot       0.63      0.60      0.62        20\n",
      "         spa       0.12      0.15      0.13        20\n",
      "         sqi       0.50      0.50      0.50        20\n",
      "         srd       0.65      0.75      0.70        20\n",
      "         srm       1.00      1.00      1.00        20\n",
      "         srn       0.90      0.95      0.93        20\n",
      "         srp       0.45      0.23      0.30        40\n",
      "         ssw       0.80      0.60      0.69        20\n",
      "         sun       0.71      0.25      0.37        20\n",
      "         suz       0.90      0.90      0.90        20\n",
      "         swa       0.32      0.40      0.36        20\n",
      "         swc       0.57      0.40      0.47        20\n",
      "         swe       1.00      0.75      0.86        20\n",
      "         swh       0.47      0.40      0.43        20\n",
      "         szl       0.94      0.85      0.89        20\n",
      "         tah       0.94      0.85      0.89        20\n",
      "         tam       1.00      0.95      0.97        20\n",
      "         tat       0.30      0.35      0.32        40\n",
      "         tbz       0.82      0.90      0.86        20\n",
      "         tca       1.00      1.00      1.00        20\n",
      "         tdt       1.00      1.00      1.00        20\n",
      "         teo       0.73      0.88      0.80        41\n",
      "         tgk       0.05      0.47      0.08        60\n",
      "         tgl       0.59      0.50      0.54        20\n",
      "         tha       1.00      0.60      0.75        20\n",
      "         tir       0.90      0.95      0.93        20\n",
      "         tlh       0.92      0.55      0.69        20\n",
      "         tls       0.90      0.90      0.90        20\n",
      "         toj       0.77      1.00      0.87        20\n",
      "         tok       0.95      1.00      0.98        20\n",
      "         ton       1.00      0.55      0.71        20\n",
      "         top       1.00      1.00      1.00        20\n",
      "         tpi       1.00      0.95      0.97        20\n",
      "         tsn       0.69      0.55      0.61        20\n",
      "         tso       0.81      0.85      0.83        20\n",
      "         tuc       0.87      1.00      0.93        20\n",
      "         tuk       0.49      0.57      0.53        40\n",
      "         tum       0.78      0.70      0.74        20\n",
      "         tur       0.78      0.35      0.48        20\n",
      "         tvl       0.82      0.90      0.86        20\n",
      "         twi       0.86      0.90      0.88        20\n",
      "         tyv       0.79      0.75      0.77        20\n",
      "         tzo       0.95      1.00      0.98        20\n",
      "         udm       0.91      0.50      0.65        20\n",
      "         uig       0.36      0.57      0.44        40\n",
      "         ukr       0.56      0.45      0.50        20\n",
      "         umb       0.84      0.80      0.82        20\n",
      "         urd       0.38      0.15      0.21        20\n",
      "         uzb       0.42      0.62      0.51        40\n",
      "         uzn       0.33      0.15      0.21        20\n",
      "         vec       0.47      0.40      0.43        20\n",
      "         ven       1.00      0.90      0.95        20\n",
      "         vep       0.76      0.80      0.78        20\n",
      "         vie       1.00      0.85      0.92        20\n",
      "         vls       0.62      0.50      0.56        20\n",
      "         vol       0.90      0.90      0.90        20\n",
      "         wal       1.00      0.85      0.92        20\n",
      "         war       0.83      0.50      0.62        20\n",
      "         wbm       0.98      0.98      0.98        41\n",
      "         wln       0.48      0.50      0.49        20\n",
      "         wol       0.55      0.55      0.55        20\n",
      "         wuu       0.00      0.00      0.00        20\n",
      "         xav       1.00      1.00      1.00        20\n",
      "         xho       0.70      0.35      0.47        20\n",
      "         xmf       1.00      0.65      0.79        20\n",
      "         yao       0.80      1.00      0.89        20\n",
      "         yap       1.00      0.95      0.97        20\n",
      "         yid       1.00      1.00      1.00        20\n",
      "         yom       0.94      0.75      0.83        20\n",
      "         yor       1.00      0.75      0.86        20\n",
      "         yue       0.00      0.00      0.00        20\n",
      "         zai       0.95      1.00      0.98        20\n",
      "         zea       0.70      0.70      0.70        20\n",
      "         zho       0.00      0.00      0.00        20\n",
      "         zlm       0.52      0.75      0.61        20\n",
      "         zsm       0.47      0.35      0.40        20\n",
      "         zul       0.29      0.10      0.15        20\n",
      "\n",
      "    accuracy                           0.68      8230\n",
      "   macro avg       0.75      0.69      0.70      8230\n",
      "weighted avg       0.73      0.68      0.69      8230\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "pipeline_xgb = Pipeline([\n",
    "    ('tfidf', TqdmTfidfVectorizer(max_features=10000, min_df=5, max_df=0.8)),\n",
    "    ('clf', XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    ))\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 5px solid green; padding-left: 10px;\">\n",
    "Changing the classifier did not change the results, so the issue is probably from the tokenizer itself.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    padding: 15px;\n",
    "    border: 2px solid #bee5eb;\n",
    "    border-radius: 5px;\n",
    "    background-color: #d1ecf1;\n",
    "    color: #0c5460;\n",
    "    font-size: 14px;\n",
    "    margin-bottom: 10px;\n",
    "    line-height: 1.5;\n",
    "    max-width: 1125px;\">\n",
    "    <strong>ℹ️ Pipeline Description:</strong>\n",
    "    <p>\n",
    "    The pipeline consists in enconding the text in latent space using the Tfidf Vectorizer with 20k features, and then\n",
    "    classify the examples with a MLP classifier, hoping that it would allow the model to escape the local minimum that trapped\n",
    "    both the XGBoost and Logistic Regression.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TFIDF Fit Transform: 100%|██████████| 32919/32919 [00:00<00:00, 1676957.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 22 epochs took 21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TFIDF Transform: 100%|██████████| 8230/8230 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         abk       1.00      0.45      0.62        20\n",
      "         ace       1.00      0.90      0.95        20\n",
      "         ach       0.86      0.95      0.90        38\n",
      "         acm       0.00      0.00      0.00        20\n",
      "         acr       1.00      0.80      0.89        20\n",
      "         ada       1.00      0.90      0.95        21\n",
      "         afb       0.21      0.15      0.18        20\n",
      "         afr       0.71      0.85      0.77        20\n",
      "         ahk       0.95      1.00      0.98        20\n",
      "         ajp       0.16      0.20      0.18        20\n",
      "         aka       0.64      0.70      0.67        20\n",
      "         aln       0.55      0.55      0.55        20\n",
      "         als       0.47      0.35      0.40        20\n",
      "         alt       0.93      0.70      0.80        20\n",
      "         amh       1.00      0.60      0.75        20\n",
      "         aoj       1.00      1.00      1.00        20\n",
      "         apc       0.21      0.20      0.21        20\n",
      "         ara       0.32      0.35      0.33        20\n",
      "         arb       0.68      0.85      0.76        20\n",
      "         arg       0.35      0.60      0.44        20\n",
      "         arn       1.00      0.95      0.97        20\n",
      "         ary       0.25      0.25      0.25        20\n",
      "         arz       0.71      0.85      0.77        20\n",
      "         asm       1.00      0.85      0.92        20\n",
      "         ast       0.69      0.55      0.61        20\n",
      "         aym       0.75      0.30      0.43        20\n",
      "         ayr       0.71      0.60      0.65        20\n",
      "         azb       0.95      0.90      0.92        20\n",
      "         aze       0.24      0.33      0.28        40\n",
      "         azj       0.57      0.20      0.30        20\n",
      "         bak       0.40      0.40      0.40        40\n",
      "         bam       0.84      0.80      0.82        20\n",
      "         ban       0.43      0.45      0.44        20\n",
      "         bar       0.58      0.55      0.56        20\n",
      "         bcl       0.50      0.35      0.41        20\n",
      "         bel       0.55      0.30      0.39        20\n",
      "         bem       1.00      0.90      0.95        20\n",
      "         ber       0.52      0.55      0.54        20\n",
      "         bew       0.45      0.50      0.48        20\n",
      "         bih       0.56      0.45      0.50        20\n",
      "         bik       0.56      0.70      0.62        20\n",
      "         bis       0.82      0.90      0.86        20\n",
      "         bjn       0.70      0.80      0.74        20\n",
      "         bod       0.86      0.90      0.88        20\n",
      "         bos       0.40      0.20      0.27        20\n",
      "         bpy       1.00      1.00      1.00        20\n",
      "         bqc       1.00      0.90      0.95        20\n",
      "         bre       0.76      0.80      0.78        20\n",
      "         bsb       0.81      0.65      0.72        20\n",
      "         bul       0.42      0.75      0.54        20\n",
      "         bzj       1.00      1.00      1.00        20\n",
      "         cab       1.00      0.85      0.92        20\n",
      "         cak       0.71      0.75      0.73        20\n",
      "         cat       0.85      0.55      0.67        20\n",
      "         cbk       0.69      0.45      0.55        20\n",
      "         ceb       0.76      0.65      0.70        20\n",
      "         ces       0.45      0.25      0.32        20\n",
      "         che       0.95      1.00      0.98        20\n",
      "         chk       0.95      1.00      0.98        20\n",
      "         chv       0.56      0.50      0.53        20\n",
      "         cjk       1.00      0.80      0.89        20\n",
      "         ckb       0.58      0.35      0.44        20\n",
      "         cmn       0.00      0.00      0.00        20\n",
      "         cos       1.00      0.05      0.10        20\n",
      "         crh       0.60      0.80      0.69        40\n",
      "         crs       0.91      1.00      0.95        20\n",
      "         csb       0.86      0.60      0.71        20\n",
      "         csy       0.83      1.00      0.91        20\n",
      "         ctu       0.95      1.00      0.98        20\n",
      "         cuk       1.00      0.95      0.97        20\n",
      "         cym       1.00      0.85      0.92        20\n",
      "         dan       0.74      0.70      0.72        20\n",
      "         deu       0.76      0.95      0.84        20\n",
      "         diq       0.80      0.60      0.69        20\n",
      "         div       1.00      0.35      0.52        20\n",
      "         djk       0.95      1.00      0.98        20\n",
      "         dtp       0.56      0.90      0.69        20\n",
      "         dyu       0.60      0.75      0.67        20\n",
      "         dzo       0.93      0.70      0.80        20\n",
      "         ekk       0.46      0.30      0.36        20\n",
      "         ell       1.00      0.90      0.95        20\n",
      "         eml       0.80      0.80      0.80        20\n",
      "         eng       0.26      0.35      0.30        20\n",
      "         enm       0.45      0.91      0.60        23\n",
      "         epo       0.64      0.70      0.67        20\n",
      "         est       0.46      0.65      0.54        20\n",
      "         eus       0.91      0.50      0.65        20\n",
      "         ewe       0.79      0.95      0.86        20\n",
      "         ext       0.47      0.45      0.46        20\n",
      "         fao       0.85      0.85      0.85        20\n",
      "         fas       0.24      0.20      0.22        20\n",
      "         fij       0.95      1.00      0.98        20\n",
      "         fil       0.62      0.25      0.36        20\n",
      "         fin       0.80      0.40      0.53        20\n",
      "         fon       1.00      0.75      0.86        20\n",
      "         fra       0.88      0.70      0.78        20\n",
      "         frr       1.00      0.55      0.71        20\n",
      "         fry       0.83      0.75      0.79        20\n",
      "         ful       0.62      0.75      0.68        20\n",
      "         fur       0.77      0.85      0.81        20\n",
      "         gcf       0.71      0.85      0.77        20\n",
      "         gil       0.95      0.90      0.92        20\n",
      "         gla       0.78      0.70      0.74        20\n",
      "         gle       0.94      0.80      0.86        20\n",
      "         glg       0.35      0.35      0.35        20\n",
      "         glk       0.44      0.40      0.42        20\n",
      "         glv       0.82      0.90      0.86        20\n",
      "         gom       0.63      0.65      0.64        40\n",
      "         gor       1.00      0.55      0.71        20\n",
      "         grc       0.90      0.95      0.93        20\n",
      "         grn       0.60      0.60      0.60        20\n",
      "         gsw       0.82      0.70      0.76        20\n",
      "         guc       0.95      1.00      0.98        20\n",
      "         gug       0.75      0.90      0.82        20\n",
      "         guj       0.74      0.50      0.60        40\n",
      "         gym       1.00      1.00      1.00        20\n",
      "         hat       0.78      0.70      0.74        20\n",
      "         hau       0.33      0.33      0.33        40\n",
      "         haw       0.86      0.30      0.44        20\n",
      "         hbo       0.81      0.78      0.79        27\n",
      "         hbs       0.36      0.53      0.43        40\n",
      "         heb       0.75      0.45      0.56        20\n",
      "         hif       0.67      0.70      0.68        20\n",
      "         hil       0.92      1.00      0.96        22\n",
      "         hin       0.23      0.57      0.33        40\n",
      "         hmn       1.00      0.60      0.75        20\n",
      "         hmo       0.94      0.85      0.89        20\n",
      "         hne       0.55      0.60      0.57        20\n",
      "         hnj       1.00      1.00      1.00        26\n",
      "         hrv       0.17      0.15      0.16        20\n",
      "         hrx       0.74      0.85      0.79        20\n",
      "         hsb       0.78      0.35      0.48        20\n",
      "         hui       1.00      0.95      0.97        20\n",
      "         hun       0.95      0.90      0.92        20\n",
      "         hus       1.00      1.00      1.00        20\n",
      "         hye       0.88      0.75      0.81        20\n",
      "         hyw       0.92      0.88      0.90        25\n",
      "         iba       0.95      0.95      0.95        22\n",
      "         ibo       0.93      0.70      0.80        20\n",
      "         ido       0.65      0.75      0.70        20\n",
      "         ikk       1.00      1.00      1.00        20\n",
      "         iku       1.00      0.75      0.86        20\n",
      "         ile       0.72      0.90      0.80        20\n",
      "         ilo       0.83      0.95      0.88        20\n",
      "         ina       0.86      0.60      0.71        20\n",
      "         ind       0.43      0.50      0.47        20\n",
      "         isl       0.89      0.85      0.87        20\n",
      "         ita       0.47      0.45      0.46        20\n",
      "         ixl       1.00      1.00      1.00        20\n",
      "         jam       0.94      0.85      0.89        20\n",
      "         jav       0.79      0.75      0.77        20\n",
      "         jbo       0.81      0.85      0.83        20\n",
      "         jpn       0.00      0.00      0.00        20\n",
      "         kaa       0.69      0.62      0.66        40\n",
      "         kab       0.71      0.60      0.65        20\n",
      "         kac       0.91      1.00      0.95        20\n",
      "         kal       1.00      0.55      0.71        20\n",
      "         kam       0.69      0.90      0.78        20\n",
      "         kan       1.00      0.95      0.97        20\n",
      "         kat       0.47      0.53      0.49        40\n",
      "         kau       1.00      0.50      0.67        20\n",
      "         kaz       0.92      0.60      0.73        20\n",
      "         kbd       1.00      0.20      0.33        20\n",
      "         kbp       0.83      0.95      0.88        20\n",
      "         kea       0.64      0.90      0.75        20\n",
      "         kek       1.00      1.00      1.00        20\n",
      "         khm       1.00      0.90      0.95        20\n",
      "         kik       0.94      0.75      0.83        20\n",
      "         kin       0.37      0.35      0.36        20\n",
      "         kir       0.91      0.50      0.65        20\n",
      "         kjb       0.95      1.00      0.98        20\n",
      "         kjh       0.85      0.85      0.85        20\n",
      "         kmb       0.86      0.90      0.88        20\n",
      "         kmr       0.64      0.35      0.45        20\n",
      "         knv       0.87      1.00      0.93        20\n",
      "         kom       0.74      0.70      0.72        20\n",
      "         kon       0.79      0.95      0.86        20\n",
      "         kor       0.25      0.05      0.08        20\n",
      "         kos       1.00      1.00      1.00        20\n",
      "         kpg       1.00      1.00      1.00        20\n",
      "         krc       1.00      0.80      0.89        20\n",
      "         ksd       0.91      1.00      0.95        20\n",
      "         ksh       1.00      0.65      0.79        20\n",
      "         ksw       1.00      0.65      0.79        23\n",
      "         kur       0.44      0.53      0.48        40\n",
      "         lao       1.00      0.90      0.95        20\n",
      "         lat       0.81      0.85      0.83        20\n",
      "         lfn       0.74      0.85      0.79        20\n",
      "         lhu       1.00      0.95      0.97        20\n",
      "         lij       0.65      0.65      0.65        20\n",
      "         lim       0.69      0.55      0.61        20\n",
      "         lin       0.94      0.75      0.83        20\n",
      "         lit       0.78      0.70      0.74        20\n",
      "         lmo       0.52      0.55      0.54        20\n",
      "         ltz       0.38      0.15      0.21        20\n",
      "         lua       0.90      0.90      0.90        20\n",
      "         lue       0.91      0.97      0.94        30\n",
      "         lug       0.89      0.80      0.84        20\n",
      "         luo       1.00      0.90      0.95        20\n",
      "         lus       1.00      0.90      0.95        20\n",
      "         lvs       0.41      0.65      0.50        20\n",
      "         lzh       0.00      0.00      0.00        20\n",
      "         mad       0.39      0.45      0.42        20\n",
      "         mah       1.00      0.96      0.98        23\n",
      "         mai       0.93      0.65      0.76        20\n",
      "         mal       1.00      1.00      1.00        20\n",
      "         mam       0.90      0.95      0.93        20\n",
      "         mar       0.87      0.65      0.74        20\n",
      "         mau       1.00      1.00      1.00        20\n",
      "         mco       0.95      1.00      0.98        20\n",
      "         meu       0.69      0.90      0.78        20\n",
      "         mgh       0.72      0.90      0.80        20\n",
      "         mhr       0.89      0.85      0.87        20\n",
      "         min       1.00      0.90      0.95        20\n",
      "         mkd       0.79      0.75      0.77        20\n",
      "         mlg       0.56      0.45      0.50        20\n",
      "         mlt       0.75      0.60      0.67        20\n",
      "         mon       0.57      0.72      0.64        40\n",
      "         mos       0.90      0.95      0.93        20\n",
      "         mps       1.00      1.00      1.00        20\n",
      "         mri       0.53      0.45      0.49        20\n",
      "         msa       0.31      0.20      0.24        20\n",
      "         mwl       0.63      0.85      0.72        20\n",
      "         mya       1.00      0.95      0.97        20\n",
      "         myv       0.88      0.35      0.50        20\n",
      "         mzh       0.95      1.00      0.98        20\n",
      "         mzn       0.95      0.95      0.95        20\n",
      "         nap       0.62      0.40      0.48        20\n",
      "         naq       1.00      0.95      0.97        20\n",
      "         nav       1.00      1.00      1.00        20\n",
      "         nbl       0.50      0.40      0.44        20\n",
      "         nch       1.00      0.95      0.97        20\n",
      "         ncj       0.91      1.00      0.95        20\n",
      "         nde       0.75      0.60      0.67        20\n",
      "         ndo       1.00      0.90      0.95        20\n",
      "         nds       0.67      0.70      0.68        20\n",
      "         nep       0.75      0.30      0.43        20\n",
      "         new       0.83      0.75      0.79        20\n",
      "         ngl       0.41      0.59      0.48        22\n",
      "         ngu       1.00      1.00      1.00        20\n",
      "         niu       0.74      0.81      0.77        21\n",
      "         nld       0.65      0.65      0.65        20\n",
      "         nnb       0.82      0.45      0.58        20\n",
      "         nno       0.64      0.70      0.67        20\n",
      "         nob       0.24      0.25      0.24        20\n",
      "         nor       0.40      0.40      0.40        20\n",
      "         npi       0.65      0.85      0.74        20\n",
      "         nso       0.74      0.70      0.72        20\n",
      "         nya       0.92      0.55      0.69        20\n",
      "         nyu       0.90      0.95      0.93        20\n",
      "         oci       0.50      0.40      0.44        20\n",
      "         ori       0.44      0.35      0.39        20\n",
      "         orm       0.94      0.75      0.83        20\n",
      "         ory       0.50      0.55      0.52        20\n",
      "         oss       1.00      0.75      0.86        20\n",
      "         ote       0.83      1.00      0.91        20\n",
      "         pag       0.83      0.95      0.88        20\n",
      "         pam       0.57      0.20      0.30        20\n",
      "         pan       1.00      0.90      0.95        20\n",
      "         pap       1.00      0.90      0.95        20\n",
      "         pau       1.00      1.00      1.00        20\n",
      "         pcd       0.69      0.55      0.61        20\n",
      "         pcm       0.73      0.80      0.76        20\n",
      "         pes       0.47      0.45      0.46        20\n",
      "         pfl       0.68      0.65      0.67        20\n",
      "         pis       0.94      0.85      0.89        20\n",
      "         pls       0.87      1.00      0.93        20\n",
      "         plt       0.92      0.55      0.69        20\n",
      "         pms       1.00      0.95      0.97        20\n",
      "         pnb       0.95      0.90      0.92        20\n",
      "         poh       0.87      1.00      0.93        20\n",
      "         pol       0.77      0.50      0.61        20\n",
      "         pon       0.90      0.90      0.90        20\n",
      "         por       0.52      0.65      0.58        20\n",
      "         prs       0.50      0.65      0.57        20\n",
      "         pus       0.89      0.85      0.87        20\n",
      "         qub       0.82      0.84      0.83        32\n",
      "         quc       0.83      0.75      0.79        20\n",
      "         que       0.80      0.40      0.53        20\n",
      "         quh       0.73      0.80      0.76        20\n",
      "         quw       0.95      0.95      0.95        20\n",
      "         quy       0.94      0.75      0.83        20\n",
      "         quz       0.93      0.67      0.78        21\n",
      "         qvi       0.95      0.95      0.95        20\n",
      "         rap       0.73      0.40      0.52        20\n",
      "         rmy       0.71      0.50      0.59        20\n",
      "         roh       0.67      0.80      0.73        20\n",
      "         ron       0.75      0.60      0.67        20\n",
      "         rop       0.95      0.95      0.95        20\n",
      "         rue       0.88      0.35      0.50        20\n",
      "         rug       0.87      1.00      0.93        20\n",
      "         run       0.35      0.45      0.39        20\n",
      "         sag       0.91      1.00      0.95        20\n",
      "         sah       0.94      0.75      0.83        20\n",
      "         san       0.53      0.59      0.56        32\n",
      "         sat       1.00      0.95      0.97        20\n",
      "         scn       0.60      0.60      0.60        20\n",
      "         sco       0.59      0.65      0.62        20\n",
      "         seh       0.95      0.90      0.92        20\n",
      "         sgs       0.92      0.55      0.69        20\n",
      "         sin       1.00      0.85      0.92        20\n",
      "         slk       0.75      0.45      0.56        20\n",
      "         slv       0.35      0.35      0.35        20\n",
      "         sme       0.82      0.70      0.76        20\n",
      "         smo       0.67      0.60      0.63        20\n",
      "         sna       0.75      0.15      0.25        20\n",
      "         snd       0.95      0.95      0.95        20\n",
      "         som       0.69      0.45      0.55        40\n",
      "         sot       0.63      0.60      0.62        20\n",
      "         spa       0.12      0.15      0.13        20\n",
      "         sqi       0.50      0.50      0.50        20\n",
      "         srd       0.65      0.75      0.70        20\n",
      "         srm       1.00      1.00      1.00        20\n",
      "         srn       0.90      0.95      0.93        20\n",
      "         srp       0.43      0.23      0.30        40\n",
      "         ssw       0.80      0.60      0.69        20\n",
      "         sun       0.71      0.25      0.37        20\n",
      "         suz       0.90      0.90      0.90        20\n",
      "         swa       0.32      0.40      0.36        20\n",
      "         swc       0.57      0.40      0.47        20\n",
      "         swe       1.00      0.75      0.86        20\n",
      "         swh       0.47      0.40      0.43        20\n",
      "         szl       0.94      0.85      0.89        20\n",
      "         tah       0.94      0.85      0.89        20\n",
      "         tam       1.00      0.95      0.97        20\n",
      "         tat       0.30      0.35      0.32        40\n",
      "         tbz       0.82      0.90      0.86        20\n",
      "         tca       1.00      1.00      1.00        20\n",
      "         tdt       1.00      1.00      1.00        20\n",
      "         teo       0.73      0.88      0.80        41\n",
      "         tgk       0.05      0.47      0.08        60\n",
      "         tgl       0.59      0.50      0.54        20\n",
      "         tha       1.00      0.60      0.75        20\n",
      "         tir       0.90      0.95      0.93        20\n",
      "         tlh       0.92      0.55      0.69        20\n",
      "         tls       0.90      0.90      0.90        20\n",
      "         toj       0.77      1.00      0.87        20\n",
      "         tok       0.95      1.00      0.98        20\n",
      "         ton       1.00      0.55      0.71        20\n",
      "         top       1.00      1.00      1.00        20\n",
      "         tpi       1.00      0.95      0.97        20\n",
      "         tsn       0.69      0.55      0.61        20\n",
      "         tso       0.81      0.85      0.83        20\n",
      "         tuc       0.87      1.00      0.93        20\n",
      "         tuk       0.49      0.57      0.53        40\n",
      "         tum       0.78      0.70      0.74        20\n",
      "         tur       0.78      0.35      0.48        20\n",
      "         tvl       0.82      0.90      0.86        20\n",
      "         twi       0.86      0.90      0.88        20\n",
      "         tyv       0.79      0.75      0.77        20\n",
      "         tzo       0.95      1.00      0.98        20\n",
      "         udm       0.91      0.50      0.65        20\n",
      "         uig       0.36      0.57      0.44        40\n",
      "         ukr       0.56      0.45      0.50        20\n",
      "         umb       0.84      0.80      0.82        20\n",
      "         urd       0.38      0.15      0.21        20\n",
      "         uzb       0.42      0.62      0.51        40\n",
      "         uzn       0.33      0.15      0.21        20\n",
      "         vec       0.47      0.40      0.43        20\n",
      "         ven       1.00      0.90      0.95        20\n",
      "         vep       0.76      0.80      0.78        20\n",
      "         vie       1.00      0.85      0.92        20\n",
      "         vls       0.62      0.50      0.56        20\n",
      "         vol       0.90      0.90      0.90        20\n",
      "         wal       1.00      0.85      0.92        20\n",
      "         war       0.83      0.50      0.62        20\n",
      "         wbm       0.98      0.98      0.98        41\n",
      "         wln       0.48      0.50      0.49        20\n",
      "         wol       0.55      0.55      0.55        20\n",
      "         wuu       0.00      0.00      0.00        20\n",
      "         xav       1.00      1.00      1.00        20\n",
      "         xho       0.70      0.35      0.47        20\n",
      "         xmf       1.00      0.65      0.79        20\n",
      "         yao       0.80      1.00      0.89        20\n",
      "         yap       1.00      0.95      0.97        20\n",
      "         yid       1.00      1.00      1.00        20\n",
      "         yom       0.94      0.75      0.83        20\n",
      "         yor       1.00      0.75      0.86        20\n",
      "         yue       0.00      0.00      0.00        20\n",
      "         zai       0.95      1.00      0.98        20\n",
      "         zea       0.70      0.70      0.70        20\n",
      "         zho       0.00      0.00      0.00        20\n",
      "         zlm       0.52      0.75      0.61        20\n",
      "         zsm       0.47      0.35      0.40        20\n",
      "         zul       0.29      0.10      0.15        20\n",
      "\n",
      "    accuracy                           0.68      8230\n",
      "   macro avg       0.75      0.69      0.70      8230\n",
      "weighted avg       0.73      0.68      0.69      8230\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(df_train['Label'].unique())\n",
    "class TorchMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(TorchMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 128)\n",
    "        self.out = nn.Linear(128, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class TorchMLPClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim=10000, num_classes=num_labels, lr=0.01, epochs=10, batch_size=32, device=None):\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self._build_model()\n",
    "    def _build_model(self):\n",
    "        self.model_ = TorchMLP(self.input_dim, self.num_classes).to(self.device)\n",
    "    def fit(self, X, y):\n",
    "        if hasattr(X, \"todense\"):\n",
    "            X = X.todense()\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        y = np.array(y)\n",
    "        if self.num_classes == 2:\n",
    "            y_tensor = torch.from_numpy(y).long()\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            y_tensor = torch.from_numpy(y).long()\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        dataset = TensorDataset(torch.from_numpy(X), y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        optimizer = optim.Adam(self.model_.parameters(), lr=self.lr)\n",
    "        self.model_.train()\n",
    "        for _ in range(self.epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model_(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        return \n",
    "    \n",
    "    def predict(self, X):\n",
    "        if hasattr(X, \"todense\"):\n",
    "            X = X.todense()\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        self.model_.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.from_numpy(X).to(self.device)\n",
    "            outputs = self.model_(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "        return predicted.cpu().numpy()\n",
    "    def score(self, X, y):\n",
    "        return accuracy_score(y, self.predict(X))\n",
    "\n",
    "pipeline_torch = Pipeline([\n",
    "    ('tfidf', TqdmTfidfVectorizer(max_features=200000, min_df=5, max_df=0.8)),\n",
    "    ('to_dense', FunctionTransformer(lambda x: x.todense() if hasattr(x, \"todense\") else x)),\n",
    "    ('clf', TorchMLPClassifier(input_dim=200000, num_classes=num_labels, lr=0.01, epochs=100, batch_size=32))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like there might be a bit of an overfitting issue, however changing hyperparameters did not solve the problem.\n",
    "To increase performance, we decided to opt for a different tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TFIDF Transform: 100%|██████████| 38827/38827 [00:00<00:00, 1735313.61it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_test = df_test['Text']\n",
    "y_pred = pipeline.predict(X_test)\n",
    "df_test['Label'] = y_pred\n",
    "df_test.to_csv('results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    padding: 15px;\n",
    "    border: 2px solid #bee5eb;\n",
    "    border-radius: 5px;\n",
    "    background-color: #d1ecf1;\n",
    "    color: #0c5460;\n",
    "    font-size: 14px;\n",
    "    margin-bottom: 10px;\n",
    "    line-height: 1.5;\n",
    "    max-width: 1125px;\">\n",
    "    <strong>ℹ️ Pipeline Description:</strong>\n",
    "    <p>\n",
    "    The pipeline consists in enconding the text in latent space using the Bert Vectorizer, and then\n",
    "    classify the examples with the custom classifier: <b>BertForSequenceClassification</b>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_train is your dataset\n",
    "\n",
    "# Step 1: Preprocessing and Splitting the Data\n",
    "class ProportionalSplitter:\n",
    "    @staticmethod\n",
    "    def stratified_split(df, test_size=0.2):\n",
    "        train, test = train_test_split(\n",
    "            df, \n",
    "            test_size=test_size, \n",
    "            stratify=df['Label'], \n",
    "            random_state=42\n",
    "        )\n",
    "        return train, test\n",
    "\n",
    "data = df_train.copy()\n",
    "train_df, test_df = ProportionalSplitter.stratified_split(data)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['Label'] = label_encoder.fit_transform(train_df['Label'])\n",
    "test_df['Label'] = label_encoder.transform(test_df['Label'])\n",
    "\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "# Dataset Class to transform the dataframe in usable dataset.\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Loading Pretrained BERT Model and Tokenizer so that we can finetune it on our own data\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = TextDataset(\n",
    "    texts=train_df['Text'].tolist(), \n",
    "    labels=train_df['Label'].tolist(), \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "test_dataset = TextDataset(\n",
    "    texts=test_df['Text'].tolist(), \n",
    "    labels=test_df['Label'].tolist(), \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Training and evaluation loops\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(dataloader), accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(dataloader), accuracy\n",
    "\n",
    "# Training Setup - forced to use cuda, otherwise the training never converges (without gpu, this pipeline takes too long)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 5px solid green; padding-left: 10px;\">\n",
    "This method allows us to outperform the baseline model and achieve roughly 76% accuracy on the validation set. As seen on the train set, with proper regularization, the performance could probably be slightly increased</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as bert.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"bert.pth\")\n",
    "print(\"Model saved as bert.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving bert for future uses, which allows us to make predictions even if the kernel crashes, since the training took approximately <b>300 minutes</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\AppData\\Local\\Temp\\ipykernel_19712\\2422753304.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"bert.pth\"))\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"bert.pth\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Add predicted labels to df_test\n",
    "def predict_label(texts, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, text in enumerate(texts):\n",
    "            encoding = tokenizer(\n",
    "                text,\n",
    "                max_length=128,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            pred = torch.argmax(logits, dim=1).item()\n",
    "            predictions.append(pred)\n",
    "            if (i + 1) % 5000 == 0:\n",
    "                print(f\"{i + 1} labels predicted...\")\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 labels predicted...\n",
      "10000 labels predicted...\n",
      "15000 labels predicted...\n",
      "20000 labels predicted...\n",
      "25000 labels predicted...\n",
      "30000 labels predicted...\n",
      "35000 labels predicted...\n",
      "40000 labels predicted...\n",
      "45000 labels predicted...\n",
      "50000 labels predicted...\n",
      "55000 labels predicted...\n",
      "60000 labels predicted...\n",
      "65000 labels predicted...\n",
      "70000 labels predicted...\n",
      "75000 labels predicted...\n",
      "80000 labels predicted...\n",
      "85000 labels predicted...\n",
      "90000 labels predicted...\n",
      "95000 labels predicted...\n",
      "100000 labels predicted...\n",
      "105000 labels predicted...\n",
      "110000 labels predicted...\n",
      "115000 labels predicted...\n",
      "120000 labels predicted...\n",
      "125000 labels predicted...\n",
      "130000 labels predicted...\n",
      "135000 labels predicted...\n",
      "140000 labels predicted...\n",
      "145000 labels predicted...\n",
      "150000 labels predicted...\n",
      "155000 labels predicted...\n",
      "160000 labels predicted...\n",
      "165000 labels predicted...\n",
      "170000 labels predicted...\n",
      "175000 labels predicted...\n",
      "180000 labels predicted...\n",
      "185000 labels predicted...\n",
      "190000 labels predicted...\n",
      "Predicted labels added to df_test.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Usage</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Private</td>\n",
       "      <td>Hüttwilen el xe on comune del Canton Turgovia ...</td>\n",
       "      <td>ven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Private</td>\n",
       "      <td>La leĝo zorgas pri kompenso de nur la plej gra...</td>\n",
       "      <td>epo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Private</td>\n",
       "      <td>پک اپ پر اپنے ڈرائیور سے پہلے پہنچیں</td>\n",
       "      <td>urd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Private</td>\n",
       "      <td>Mukmu  Ch'itana mukmu icha Butun nisqaqa nisqa...</td>\n",
       "      <td>quy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Private</td>\n",
       "      <td>Iwe   lon ena fansoun   lupwen ra aleani än Mo...</td>\n",
       "      <td>chk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Usage                                               Text Label\n",
       "0  Private  Hüttwilen el xe on comune del Canton Turgovia ...   ven\n",
       "1  Private  La leĝo zorgas pri kompenso de nur la plej gra...   epo\n",
       "2  Private               پک اپ پر اپنے ڈرائیور سے پہلے پہنچیں   urd\n",
       "3  Private  Mukmu  Ch'itana mukmu icha Butun nisqaqa nisqa...   quy\n",
       "4  Private  Iwe   lon ena fansoun   lupwen ra aleani än Mo...   chk"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict and add to df_test\n",
    "df_test['Label'] = predict_label(df_test['Text'].tolist(), model, tokenizer, device)\n",
    "df_test['Label'] = label_encoder.inverse_transform(df_test['Label'])\n",
    "print(\"Predicted labels added to df_test.\")\n",
    "df_test['ID'] = df_test.index\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Hüttwilen el xe on comune del Canton Turgovia ...</td>\n",
       "      <td>ven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>La leĝo zorgas pri kompenso de nur la plej gra...</td>\n",
       "      <td>epo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>پک اپ پر اپنے ڈرائیور سے پہلے پہنچیں</td>\n",
       "      <td>urd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Mukmu  Ch'itana mukmu icha Butun nisqaqa nisqa...</td>\n",
       "      <td>quy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Iwe   lon ena fansoun   lupwen ra aleani än Mo...</td>\n",
       "      <td>chk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                               Text Label\n",
       "0   1  Hüttwilen el xe on comune del Canton Turgovia ...   ven\n",
       "1   2  La leĝo zorgas pri kompenso de nur la plej gra...   epo\n",
       "2   3               پک اپ پر اپنے ڈرائیور سے پہلے پہنچیں   urd\n",
       "3   4  Mukmu  Ch'itana mukmu icha Butun nisqaqa nisqa...   quy\n",
       "4   5  Iwe   lon ena fansoun   lupwen ra aleani än Mo...   chk"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['ID'] = df_test.index + 1\n",
    "df_test = df_test[['ID', 'Text', 'Label']]\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(\"Submission_louis_v3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>epo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>urd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>quy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>chk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID Label\n",
       "0   1   ven\n",
       "1   2   epo\n",
       "2   3   urd\n",
       "3   4   quy\n",
       "4   5   chk"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Submission_louis_v2.csv')\n",
    "df = df[['ID', 'Label']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Test.csv', index=False, encoding='utf-8-sig', quoting=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190567, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>epo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>urd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>quy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>chk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID Label\n",
       "0   1   ven\n",
       "1   2   epo\n",
       "2   3   urd\n",
       "3   4   quy\n",
       "4   5   chk"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Test.csv')\n",
    "# df['Label'] = df['Label'].replace('\"','')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Data available, in different format so we need to repreprocess everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Finalment  Atena le recibe en l'acropoli d'Ate...</td>\n",
       "      <td>arg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jane Laffort fille de Joseph Laffort et d' Ang...</td>\n",
       "      <td>lat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Label\n",
       "0  Finalment  Atena le recibe en l'acropoli d'Ate...   arg\n",
       "1  Jane Laffort fille de Joseph Laffort et d' Ang...   lat"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train = pd.read_csv('Lexical_juggling_train.csv')\n",
    "# df_train = pd.read_csv('train_submission.csv')\n",
    "df_train = pd.read_excel('train_augmented.xlsx', sheet_name='Data')\n",
    "df_train.dropna(subset=['Label'], inplace=True)\n",
    "labels_with_multiple_rows = df_train['Label'].value_counts()\n",
    "labels_to_keep = labels_with_multiple_rows[labels_with_multiple_rows > 1].index\n",
    "df_train = df_train[df_train['Label'].isin(labels_to_keep)]\n",
    "df_train['Text'] = df_train['Text'].astype(str)\n",
    "df_train['Label'] = df_train['Label'].astype(str)\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Usage</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55</td>\n",
       "      <td>Private</td>\n",
       "      <td>Ponovo dobija riječni oblik do Drežnice.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71</td>\n",
       "      <td>Private</td>\n",
       "      <td>Se formaron aproximadamente hace apenas unos 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    Usage                                               Text\n",
       "0  55  Private           Ponovo dobija riječni oblik do Drežnice.\n",
       "1  71  Private  Se formaron aproximadamente hace apenas unos 1..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('test_without_labels.csv')\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape =  (41149, 2)\n",
      "Test shape =  (38827, 3)\n",
      "List labels length =  385\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Shape = \",df_train.shape)\n",
    "print(\"Test shape = \",df_test.shape)\n",
    "print(\"List labels length = \", len(df_train['Label'].unique()))\n",
    "# print(df_train['Label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok - So we have 38K sentences in different languages, to classify in 390 categories. If the class is balanced, this would represent a 100:1 ratio, so ok to train without generating new sentences I assume. So first baseline algo will just train an NLP classifier on train dataset, and then use the test dataset to see how good it actually is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Label  Count\n",
      "0     tgk    300\n",
      "1     teo    205\n",
      "2     wbm    203\n",
      "3     hin    200\n",
      "4     tat    200\n",
      "..    ...    ...\n",
      "380   xho    100\n",
      "381   yao    100\n",
      "382   hus    100\n",
      "383   kau    100\n",
      "384   ceb     99\n",
      "\n",
      "[385 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_lab = pd.DataFrame(df_train)\n",
    "\n",
    "label_counts = df_lab[\"Label\"].value_counts().reset_index()\n",
    "label_counts.columns = [\"Label\", \"Count\"]\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TFIDF Fit Transform: 100%|██████████| 32919/32919 [00:00<00:00, 1709810.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 20 epochs took 18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TFIDF Transform: 100%|██████████| 8230/8230 [00:00<00:00, 525509.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         abk       1.00      0.20      0.33        20\n",
      "         ace       1.00      0.90      0.95        20\n",
      "         ach       0.84      0.97      0.90        38\n",
      "         acm       0.25      0.10      0.14        20\n",
      "         acr       1.00      0.85      0.92        20\n",
      "         ada       0.95      0.90      0.93        21\n",
      "         afb       0.18      0.10      0.13        20\n",
      "         afr       0.68      0.85      0.76        20\n",
      "         ahk       0.95      1.00      0.98        20\n",
      "         ajp       0.19      0.25      0.22        20\n",
      "         aka       0.64      0.70      0.67        20\n",
      "         aln       0.50      0.55      0.52        20\n",
      "         als       0.33      0.25      0.29        20\n",
      "         alt       1.00      0.70      0.82        20\n",
      "         amh       1.00      0.50      0.67        20\n",
      "         aoj       1.00      1.00      1.00        20\n",
      "         apc       0.24      0.25      0.24        20\n",
      "         ara       0.30      0.35      0.33        20\n",
      "         arb       0.68      0.85      0.76        20\n",
      "         arg       0.32      0.55      0.41        20\n",
      "         arn       1.00      0.95      0.97        20\n",
      "         ary       0.22      0.20      0.21        20\n",
      "         arz       0.70      0.80      0.74        20\n",
      "         asm       1.00      0.90      0.95        20\n",
      "         ast       0.61      0.55      0.58        20\n",
      "         aym       0.75      0.30      0.43        20\n",
      "         ayr       0.75      0.60      0.67        20\n",
      "         azb       0.89      0.85      0.87        20\n",
      "         aze       0.15      0.20      0.17        40\n",
      "         azj       0.33      0.20      0.25        20\n",
      "         bak       0.40      0.40      0.40        40\n",
      "         bam       0.73      0.80      0.76        20\n",
      "         ban       0.47      0.45      0.46        20\n",
      "         bar       0.55      0.55      0.55        20\n",
      "         bcl       0.43      0.30      0.35        20\n",
      "         bel       0.60      0.30      0.40        20\n",
      "         bem       0.95      0.90      0.92        20\n",
      "         ber       0.37      0.50      0.43        20\n",
      "         bew       0.44      0.35      0.39        20\n",
      "         bih       0.59      0.50      0.54        20\n",
      "         bik       0.50      0.65      0.57        20\n",
      "         bis       0.82      0.90      0.86        20\n",
      "         bjn       0.81      0.85      0.83        20\n",
      "         bod       0.86      0.90      0.88        20\n",
      "         bos       0.33      0.15      0.21        20\n",
      "         bpy       1.00      1.00      1.00        20\n",
      "         bqc       1.00      0.95      0.97        20\n",
      "         bre       0.80      0.80      0.80        20\n",
      "         bsb       0.76      0.65      0.70        20\n",
      "         bul       0.41      0.75      0.53        20\n",
      "         bzj       1.00      1.00      1.00        20\n",
      "         cab       1.00      0.85      0.92        20\n",
      "         cak       0.75      0.75      0.75        20\n",
      "         cat       0.80      0.60      0.69        20\n",
      "         cbk       0.69      0.45      0.55        20\n",
      "         ceb       0.72      0.65      0.68        20\n",
      "         ces       0.40      0.20      0.27        20\n",
      "         che       0.95      0.90      0.92        20\n",
      "         chk       0.95      1.00      0.98        20\n",
      "         chv       0.47      0.40      0.43        20\n",
      "         cjk       1.00      0.80      0.89        20\n",
      "         ckb       0.47      0.35      0.40        20\n",
      "         cmn       0.00      0.00      0.00        20\n",
      "         cos       0.75      0.15      0.25        20\n",
      "         crh       0.51      0.78      0.61        40\n",
      "         crs       0.87      1.00      0.93        20\n",
      "         csb       0.63      0.60      0.62        20\n",
      "         csy       0.79      0.95      0.86        20\n",
      "         ctu       0.91      1.00      0.95        20\n",
      "         cuk       1.00      0.95      0.97        20\n",
      "         cym       1.00      0.90      0.95        20\n",
      "         dan       0.67      0.60      0.63        20\n",
      "         deu       0.70      0.80      0.74        20\n",
      "         diq       0.67      0.50      0.57        20\n",
      "         div       1.00      0.35      0.52        20\n",
      "         djk       0.95      1.00      0.98        20\n",
      "         dtp       0.53      0.85      0.65        20\n",
      "         dyu       0.56      0.70      0.62        20\n",
      "         dzo       0.93      0.70      0.80        20\n",
      "         ekk       0.46      0.30      0.36        20\n",
      "         ell       0.94      0.85      0.89        20\n",
      "         eml       0.80      0.80      0.80        20\n",
      "         eng       0.21      0.30      0.25        20\n",
      "         enm       0.45      0.91      0.60        23\n",
      "         epo       0.59      0.65      0.62        20\n",
      "         est       0.44      0.60      0.51        20\n",
      "         eus       0.90      0.45      0.60        20\n",
      "         ewe       0.77      1.00      0.87        20\n",
      "         ext       0.53      0.45      0.49        20\n",
      "         fao       0.84      0.80      0.82        20\n",
      "         fas       0.25      0.20      0.22        20\n",
      "         fij       0.95      1.00      0.98        20\n",
      "         fil       0.50      0.20      0.29        20\n",
      "         fin       0.88      0.35      0.50        20\n",
      "         fon       1.00      0.70      0.82        20\n",
      "         fra       0.82      0.70      0.76        20\n",
      "         frr       1.00      0.45      0.62        20\n",
      "         fry       0.83      0.75      0.79        20\n",
      "         ful       0.62      0.75      0.68        20\n",
      "         fur       0.74      0.85      0.79        20\n",
      "         gcf       0.70      0.80      0.74        20\n",
      "         gil       0.90      0.90      0.90        20\n",
      "         gla       0.76      0.65      0.70        20\n",
      "         gle       0.89      0.80      0.84        20\n",
      "         glg       0.42      0.40      0.41        20\n",
      "         glk       0.42      0.40      0.41        20\n",
      "         glv       0.82      0.90      0.86        20\n",
      "         gom       0.62      0.60      0.61        40\n",
      "         gor       0.91      0.50      0.65        20\n",
      "         grc       0.83      0.75      0.79        20\n",
      "         grn       0.70      0.70      0.70        20\n",
      "         gsw       0.82      0.70      0.76        20\n",
      "         guc       0.95      1.00      0.98        20\n",
      "         gug       0.83      0.95      0.88        20\n",
      "         guj       0.72      0.45      0.55        40\n",
      "         gym       1.00      1.00      1.00        20\n",
      "         hat       0.71      0.75      0.73        20\n",
      "         hau       0.33      0.30      0.32        40\n",
      "         haw       0.67      0.30      0.41        20\n",
      "         hbo       0.84      0.78      0.81        27\n",
      "         hbs       0.39      0.50      0.44        40\n",
      "         heb       0.80      0.40      0.53        20\n",
      "         hif       0.64      0.70      0.67        20\n",
      "         hil       0.91      0.95      0.93        22\n",
      "         hin       0.24      0.57      0.34        40\n",
      "         hmn       1.00      0.60      0.75        20\n",
      "         hmo       0.94      0.85      0.89        20\n",
      "         hne       0.60      0.60      0.60        20\n",
      "         hnj       1.00      1.00      1.00        26\n",
      "         hrv       0.24      0.25      0.24        20\n",
      "         hrx       0.80      0.80      0.80        20\n",
      "         hsb       0.64      0.35      0.45        20\n",
      "         hui       1.00      0.95      0.97        20\n",
      "         hun       0.95      0.90      0.92        20\n",
      "         hus       1.00      1.00      1.00        20\n",
      "         hye       1.00      0.55      0.71        20\n",
      "         hyw       0.85      0.88      0.86        25\n",
      "         iba       0.95      0.95      0.95        22\n",
      "         ibo       0.93      0.70      0.80        20\n",
      "         ido       0.62      0.75      0.68        20\n",
      "         ikk       1.00      0.90      0.95        20\n",
      "         iku       1.00      0.50      0.67        20\n",
      "         ile       0.75      0.90      0.82        20\n",
      "         ilo       0.83      0.95      0.88        20\n",
      "         ina       0.87      0.65      0.74        20\n",
      "         ind       0.32      0.30      0.31        20\n",
      "         isl       0.85      0.85      0.85        20\n",
      "         ita       0.47      0.45      0.46        20\n",
      "         ixl       1.00      1.00      1.00        20\n",
      "         jam       0.94      0.85      0.89        20\n",
      "         jav       0.79      0.75      0.77        20\n",
      "         jbo       0.85      0.85      0.85        20\n",
      "         jpn       0.00      0.00      0.00        20\n",
      "         kaa       0.75      0.60      0.67        40\n",
      "         kab       0.64      0.35      0.45        20\n",
      "         kac       0.91      1.00      0.95        20\n",
      "         kal       1.00      0.45      0.62        20\n",
      "         kam       0.69      0.90      0.78        20\n",
      "         kan       1.00      0.80      0.89        20\n",
      "         kat       0.45      0.53      0.48        40\n",
      "         kau       0.91      0.50      0.65        20\n",
      "         kaz       0.85      0.55      0.67        20\n",
      "         kbd       0.67      0.10      0.17        20\n",
      "         kbp       0.83      0.95      0.88        20\n",
      "         kea       0.56      0.90      0.69        20\n",
      "         kek       1.00      1.00      1.00        20\n",
      "         khm       1.00      0.90      0.95        20\n",
      "         kik       0.94      0.75      0.83        20\n",
      "         kin       0.39      0.35      0.37        20\n",
      "         kir       0.82      0.45      0.58        20\n",
      "         kjb       0.91      1.00      0.95        20\n",
      "         kjh       0.92      0.60      0.73        20\n",
      "         kmb       0.86      0.90      0.88        20\n",
      "         kmr       0.70      0.35      0.47        20\n",
      "         knv       0.83      0.95      0.88        20\n",
      "         kom       0.72      0.65      0.68        20\n",
      "         kon       0.76      0.95      0.84        20\n",
      "         kor       0.00      0.00      0.00        20\n",
      "         kos       1.00      1.00      1.00        20\n",
      "         kpg       1.00      1.00      1.00        20\n",
      "         krc       1.00      0.60      0.75        20\n",
      "         ksd       0.95      1.00      0.98        20\n",
      "         ksh       0.93      0.65      0.76        20\n",
      "         ksw       1.00      0.65      0.79        23\n",
      "         kur       0.43      0.47      0.45        40\n",
      "         lao       1.00      0.90      0.95        20\n",
      "         lat       0.85      0.85      0.85        20\n",
      "         lfn       0.74      0.85      0.79        20\n",
      "         lhu       1.00      0.95      0.97        20\n",
      "         lij       0.65      0.65      0.65        20\n",
      "         lim       0.67      0.50      0.57        20\n",
      "         lin       0.94      0.75      0.83        20\n",
      "         lit       0.70      0.70      0.70        20\n",
      "         lmo       0.55      0.55      0.55        20\n",
      "         ltz       0.33      0.15      0.21        20\n",
      "         lua       0.86      0.90      0.88        20\n",
      "         lue       0.91      0.97      0.94        30\n",
      "         lug       0.88      0.75      0.81        20\n",
      "         luo       1.00      0.85      0.92        20\n",
      "         lus       1.00      0.90      0.95        20\n",
      "         lvs       0.39      0.65      0.49        20\n",
      "         lzh       0.00      0.00      0.00        20\n",
      "         mad       0.32      0.45      0.38        20\n",
      "         mah       1.00      0.96      0.98        23\n",
      "         mai       0.93      0.65      0.76        20\n",
      "         mal       1.00      0.95      0.97        20\n",
      "         mam       0.90      0.95      0.93        20\n",
      "         mar       0.87      0.65      0.74        20\n",
      "         mau       1.00      1.00      1.00        20\n",
      "         mco       0.95      1.00      0.98        20\n",
      "         meu       0.67      0.90      0.77        20\n",
      "         mgh       0.67      0.90      0.77        20\n",
      "         mhr       0.89      0.85      0.87        20\n",
      "         min       1.00      0.95      0.97        20\n",
      "         mkd       0.68      0.75      0.71        20\n",
      "         mlg       0.56      0.45      0.50        20\n",
      "         mlt       0.73      0.55      0.63        20\n",
      "         mon       0.56      0.68      0.61        40\n",
      "         mos       0.90      0.90      0.90        20\n",
      "         mps       1.00      1.00      1.00        20\n",
      "         mri       0.57      0.40      0.47        20\n",
      "         msa       0.33      0.15      0.21        20\n",
      "         mwl       0.59      0.85      0.69        20\n",
      "         mya       1.00      0.95      0.97        20\n",
      "         myv       0.78      0.35      0.48        20\n",
      "         mzh       0.95      1.00      0.98        20\n",
      "         mzn       0.95      0.90      0.92        20\n",
      "         nap       0.67      0.40      0.50        20\n",
      "         naq       1.00      0.95      0.97        20\n",
      "         nav       1.00      1.00      1.00        20\n",
      "         nbl       0.44      0.35      0.39        20\n",
      "         nch       1.00      0.95      0.97        20\n",
      "         ncj       0.87      1.00      0.93        20\n",
      "         nde       0.67      0.50      0.57        20\n",
      "         ndo       0.94      0.85      0.89        20\n",
      "         nds       0.59      0.65      0.62        20\n",
      "         nep       0.70      0.35      0.47        20\n",
      "         new       0.83      0.75      0.79        20\n",
      "         ngl       0.31      0.45      0.37        22\n",
      "         ngu       1.00      1.00      1.00        20\n",
      "         niu       0.75      0.86      0.80        21\n",
      "         nld       0.59      0.65      0.62        20\n",
      "         nnb       0.75      0.30      0.43        20\n",
      "         nno       0.62      0.65      0.63        20\n",
      "         nob       0.25      0.30      0.27        20\n",
      "         nor       0.44      0.40      0.42        20\n",
      "         npi       0.65      0.75      0.70        20\n",
      "         nso       0.67      0.70      0.68        20\n",
      "         nya       0.86      0.60      0.71        20\n",
      "         nyu       0.83      0.95      0.88        20\n",
      "         oci       0.44      0.40      0.42        20\n",
      "         ori       0.42      0.40      0.41        20\n",
      "         orm       0.93      0.70      0.80        20\n",
      "         ory       0.47      0.45      0.46        20\n",
      "         oss       1.00      0.70      0.82        20\n",
      "         ote       0.83      1.00      0.91        20\n",
      "         pag       0.90      0.95      0.93        20\n",
      "         pam       0.57      0.20      0.30        20\n",
      "         pan       1.00      0.85      0.92        20\n",
      "         pap       0.94      0.85      0.89        20\n",
      "         pau       1.00      1.00      1.00        20\n",
      "         pcd       0.73      0.55      0.63        20\n",
      "         pcm       0.76      0.80      0.78        20\n",
      "         pes       0.44      0.40      0.42        20\n",
      "         pfl       0.65      0.65      0.65        20\n",
      "         pis       0.94      0.85      0.89        20\n",
      "         pls       0.87      1.00      0.93        20\n",
      "         plt       0.86      0.60      0.71        20\n",
      "         pms       1.00      0.95      0.97        20\n",
      "         pnb       0.95      0.90      0.92        20\n",
      "         poh       0.87      1.00      0.93        20\n",
      "         pol       0.83      0.50      0.62        20\n",
      "         pon       0.94      0.85      0.89        20\n",
      "         por       0.54      0.65      0.59        20\n",
      "         prs       0.50      0.65      0.57        20\n",
      "         pus       0.89      0.85      0.87        20\n",
      "         qub       0.79      0.81      0.80        32\n",
      "         quc       0.88      0.75      0.81        20\n",
      "         que       0.83      0.50      0.62        20\n",
      "         quh       0.71      0.75      0.73        20\n",
      "         quw       0.90      0.95      0.93        20\n",
      "         quy       0.88      0.70      0.78        20\n",
      "         quz       1.00      0.62      0.76        21\n",
      "         qvi       0.95      0.90      0.92        20\n",
      "         rap       0.64      0.35      0.45        20\n",
      "         rmy       0.71      0.50      0.59        20\n",
      "         roh       0.59      0.80      0.68        20\n",
      "         ron       0.76      0.65      0.70        20\n",
      "         rop       0.95      0.95      0.95        20\n",
      "         rue       0.75      0.15      0.25        20\n",
      "         rug       0.87      1.00      0.93        20\n",
      "         run       0.38      0.50      0.43        20\n",
      "         sag       0.91      1.00      0.95        20\n",
      "         sah       0.88      0.70      0.78        20\n",
      "         san       0.50      0.56      0.53        32\n",
      "         sat       1.00      0.95      0.97        20\n",
      "         scn       0.63      0.60      0.62        20\n",
      "         sco       0.55      0.60      0.57        20\n",
      "         seh       1.00      0.80      0.89        20\n",
      "         sgs       1.00      0.55      0.71        20\n",
      "         sin       1.00      0.80      0.89        20\n",
      "         slk       0.56      0.25      0.34        20\n",
      "         slv       0.28      0.35      0.31        20\n",
      "         sme       0.81      0.65      0.72        20\n",
      "         smo       0.67      0.60      0.63        20\n",
      "         sna       0.75      0.15      0.25        20\n",
      "         snd       0.95      0.90      0.92        20\n",
      "         som       0.71      0.42      0.53        40\n",
      "         sot       0.63      0.60      0.62        20\n",
      "         spa       0.14      0.15      0.14        20\n",
      "         sqi       0.42      0.40      0.41        20\n",
      "         srd       0.62      0.75      0.68        20\n",
      "         srm       1.00      1.00      1.00        20\n",
      "         srn       0.90      0.95      0.93        20\n",
      "         srp       0.43      0.25      0.32        40\n",
      "         ssw       0.71      0.50      0.59        20\n",
      "         sun       0.83      0.25      0.38        20\n",
      "         suz       0.86      0.90      0.88        20\n",
      "         swa       0.33      0.40      0.36        20\n",
      "         swc       0.62      0.40      0.48        20\n",
      "         swe       0.79      0.75      0.77        20\n",
      "         swh       0.61      0.55      0.58        20\n",
      "         szl       0.89      0.85      0.87        20\n",
      "         tah       1.00      0.85      0.92        20\n",
      "         tam       1.00      0.95      0.97        20\n",
      "         tat       0.25      0.28      0.26        40\n",
      "         tbz       0.84      0.80      0.82        20\n",
      "         tca       1.00      1.00      1.00        20\n",
      "         tdt       1.00      1.00      1.00        20\n",
      "         teo       0.75      0.88      0.81        41\n",
      "         tgk       0.04      0.43      0.07        60\n",
      "         tgl       0.59      0.50      0.54        20\n",
      "         tha       1.00      0.40      0.57        20\n",
      "         tir       0.90      0.95      0.93        20\n",
      "         tlh       0.82      0.45      0.58        20\n",
      "         tls       0.90      0.90      0.90        20\n",
      "         toj       0.74      1.00      0.85        20\n",
      "         tok       0.95      1.00      0.98        20\n",
      "         ton       1.00      0.55      0.71        20\n",
      "         top       1.00      1.00      1.00        20\n",
      "         tpi       1.00      0.95      0.97        20\n",
      "         tsn       0.60      0.45      0.51        20\n",
      "         tso       0.77      0.85      0.81        20\n",
      "         tuc       1.00      1.00      1.00        20\n",
      "         tuk       0.40      0.53      0.46        40\n",
      "         tum       0.79      0.75      0.77        20\n",
      "         tur       0.78      0.35      0.48        20\n",
      "         tvl       0.75      0.90      0.82        20\n",
      "         twi       0.86      0.90      0.88        20\n",
      "         tyv       0.72      0.65      0.68        20\n",
      "         tzo       0.95      1.00      0.98        20\n",
      "         udm       0.90      0.45      0.60        20\n",
      "         uig       0.37      0.55      0.44        40\n",
      "         ukr       0.54      0.35      0.42        20\n",
      "         umb       0.75      0.75      0.75        20\n",
      "         urd       0.33      0.10      0.15        20\n",
      "         uzb       0.44      0.65      0.53        40\n",
      "         uzn       0.29      0.10      0.15        20\n",
      "         vec       0.47      0.35      0.40        20\n",
      "         ven       0.95      0.90      0.92        20\n",
      "         vep       0.71      0.75      0.73        20\n",
      "         vie       1.00      0.75      0.86        20\n",
      "         vls       0.62      0.40      0.48        20\n",
      "         vol       0.82      0.90      0.86        20\n",
      "         wal       1.00      0.75      0.86        20\n",
      "         war       0.77      0.50      0.61        20\n",
      "         wbm       0.98      0.98      0.98        41\n",
      "         wln       0.45      0.45      0.45        20\n",
      "         wol       0.55      0.55      0.55        20\n",
      "         wuu       0.00      0.00      0.00        20\n",
      "         xav       1.00      1.00      1.00        20\n",
      "         xho       0.78      0.35      0.48        20\n",
      "         xmf       1.00      0.65      0.79        20\n",
      "         yao       0.80      1.00      0.89        20\n",
      "         yap       1.00      0.95      0.97        20\n",
      "         yid       0.95      1.00      0.98        20\n",
      "         yom       0.94      0.75      0.83        20\n",
      "         yor       0.93      0.70      0.80        20\n",
      "         yue       0.00      0.00      0.00        20\n",
      "         zai       0.95      0.95      0.95        20\n",
      "         zea       0.68      0.75      0.71        20\n",
      "         zho       0.00      0.00      0.00        20\n",
      "         zlm       0.53      0.80      0.64        20\n",
      "         zsm       0.47      0.45      0.46        20\n",
      "         zul       0.33      0.10      0.15        20\n",
      "\n",
      "    accuracy                           0.66      8230\n",
      "   macro avg       0.73      0.67      0.68      8230\n",
      "weighted avg       0.72      0.66      0.67      8230\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Louis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class TqdmTfidfVectorizer(TfidfVectorizer):\n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        raw_documents = list(tqdm(raw_documents, desc=\"TFIDF Fit Transform\"))\n",
    "        return super().fit_transform(raw_documents, y)\n",
    "    def transform(self, raw_documents):\n",
    "        raw_documents = list(tqdm(raw_documents, desc=\"TFIDF Transform\"))\n",
    "        return super().transform(raw_documents)\n",
    "\n",
    "# Assume df_train is your DataFrame with columns 'Text' and 'Label'\n",
    "X = df_train['Text']\n",
    "y = df_train['Label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TqdmTfidfVectorizer(max_features=10000, min_df=5, max_df=0.8)),\n",
    "    ('clf', LogisticRegression(solver='saga', max_iter=100, verbose=1))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 4.5153 | Train Accuracy: 0.1794\n",
      "Validation Loss: 3.1784 | Validation Accuracy: 0.3425\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_train is your dataset\n",
    "\n",
    "# Step 1: Preprocessing and Splitting the Data\n",
    "class ProportionalSplitter:\n",
    "    @staticmethod\n",
    "    def stratified_split(df, test_size=0.2):\n",
    "        train, test = train_test_split(\n",
    "            df, \n",
    "            test_size=test_size, \n",
    "            stratify=df['Label'], \n",
    "            random_state=42\n",
    "        )\n",
    "        return train, test\n",
    "\n",
    "data = df_train.copy()\n",
    "train_df, test_df = ProportionalSplitter.stratified_split(data)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['Label'] = label_encoder.fit_transform(train_df['Label'])\n",
    "test_df['Label'] = label_encoder.transform(test_df['Label'])\n",
    "\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "# Step 2: Define Dataset Class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Step 3: Load Pretrained BERT Model and Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = TextDataset(\n",
    "    texts=train_df['Text'].tolist(), \n",
    "    labels=train_df['Label'].tolist(), \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "test_dataset = TextDataset(\n",
    "    texts=test_df['Text'].tolist(), \n",
    "    labels=test_df['Label'].tolist(), \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "# Step 4: Define DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Step 5: Define Training and Evaluation Loops\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(dataloader), accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(dataloader), accuracy\n",
    "\n",
    "# Step 6: Training Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                           | 3/2058 [00:29<5:41:20,  9.97s/it, accuracy=0, loss=5.96]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 108\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 108\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, device)\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[49], line 70\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     69\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 70\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     72\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = df_train.copy()\n",
    "\n",
    "def stratified_split(df, test_size=0.2):\n",
    "    return train_test_split(df, test_size=test_size, stratify=df['Label'], random_state=42)\n",
    "\n",
    "train_df, test_df = stratified_split(data)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['Label'] = label_encoder.fit_transform(train_df['Label'])\n",
    "test_df['Label'] = label_encoder.transform(test_df['Label'])\n",
    "\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "train_dataset = TextDataset(train_df['Text'].tolist(), train_df['Label'].tolist(), tokenizer, 128)\n",
    "test_dataset = TextDataset(test_df['Text'].tolist(), test_df['Label'].tolist(), tokenizer, 128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loop = tqdm(dataloader, desc=\"Training\", leave=True, position=0, ncols=100)\n",
    "    \n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        loop.set_postfix(loss=loss.item(), accuracy=correct/total)\n",
    "    \n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loop = tqdm(dataloader, desc=\"Evaluating\", leave=True, position=0, ncols=100)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loop:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            loop.set_postfix(loss=loss.item(), accuracy=correct/total)\n",
    "    \n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(model, test_loader, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as bert.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"bert.pth\")\n",
    "print(\"Model saved as bert.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\AppData\\Local\\Temp\\ipykernel_12700\\2285236433.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"bert.pth\"))\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"bert.pth\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Add predicted labels to df_test\n",
    "def predict_label(texts, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            encoding = tokenizer(\n",
    "                text,\n",
    "                max_length=128,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            pred = torch.argmax(logits, dim=1).item()\n",
    "            predictions.append(pred)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels added to df_test.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Usage</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55</td>\n",
       "      <td>Private</td>\n",
       "      <td>Ponovo dobija riječni oblik do Drežnice.</td>\n",
       "      <td>hrv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71</td>\n",
       "      <td>Private</td>\n",
       "      <td>Se formaron aproximadamente hace apenas unos 1...</td>\n",
       "      <td>spa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>Private</td>\n",
       "      <td>Data juga harus terbebas dari kepentingan-kepe...</td>\n",
       "      <td>mad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>107</td>\n",
       "      <td>Private</td>\n",
       "      <td>ᐃᒃᓯᕙᐅᑕᖅ (ᑐᓵᔨᑎᒍᑦ): ᖁᔭᓐᓇᒦᒃ  ᒥᔅ ᐅᐃᓐᒥᐅᓪ. ᒥᔅᑕ ᐃᓄᒃ.</td>\n",
       "      <td>iku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>129</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bei Gefor rullt de Kéiseker sech an  riicht se...</td>\n",
       "      <td>ltz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID    Usage                                               Text Label\n",
       "0   55  Private           Ponovo dobija riječni oblik do Drežnice.   hrv\n",
       "1   71  Private  Se formaron aproximadamente hace apenas unos 1...   spa\n",
       "2   67  Private  Data juga harus terbebas dari kepentingan-kepe...   mad\n",
       "3  107  Private      ᐃᒃᓯᕙᐅᑕᖅ (ᑐᓵᔨᑎᒍᑦ): ᖁᔭᓐᓇᒦᒃ  ᒥᔅ ᐅᐃᓐᒥᐅᓪ. ᒥᔅᑕ ᐃᓄᒃ.   iku\n",
       "4  129  Private  Bei Gefor rullt de Kéiseker sech an  riicht se...   ltz"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict and add to df_test\n",
    "df_test['Label'] = predict_label(df_test['Text'].tolist(), model, tokenizer, device)\n",
    "df_test['Label'] = label_encoder.inverse_transform(df_test['Label'])\n",
    "print(\"Predicted labels added to df_test.\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(\"Submission_louis_v2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
